{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 1: Verificar versión de Python**\n",
    "# Se valida que la versión de Python sea la requerida.\n",
    "#\n",
    "# - **Líneas Clave:**\n",
    "#   - Importación de librerías para verificar la versión (`sys`, `os`).\n",
    "#   - Configuración de logging para advertencias y mensajes informativos.\n",
    "#   - Comparación de la versión actual con la requerida.\n",
    "# - **Oportunidad de Mejora:** Agregar soporte para versiones cercanas si no es crítica la compatibilidad exacta.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Desactivar advertencias de paralelización en tokenizadores\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "# Versión requerida\n",
    "REQUIRED_VERSION = (3, 10, 12)\n",
    "current_version = sys.version_info\n",
    "\n",
    "# Validar compatibilidad de versión\n",
    "if (current_version.major, current_version.minor, current_version.micro) != REQUIRED_VERSION:\n",
    "    logging.warning(f\"\"\"\n",
    "    **********************************************\n",
    "    ** Advertencia: Versión de Python no compatible **\n",
    "    **********************************************\n",
    "    Este chatbot está optimizado para Python {REQUIRED_VERSION[0]}.{REQUIRED_VERSION[1]}.{REQUIRED_VERSION[2]}.\n",
    "    La versión actual es Python {current_version.major}.{current_version.minor}.{current_version.micro}.\n",
    "    Algunas funcionalidades pueden no funcionar correctamente.\n",
    "    **********************************************\n",
    "    \"\"\")\n",
    "else:\n",
    "    logging.info(\"\"\"\n",
    "    **********************************************\n",
    "    ** Versión de Python compatible **\n",
    "    **********************************************\n",
    "    Python 3.10.12 detectado correctamente.\n",
    "    Todas las funcionalidades deberían operar sin problemas.\n",
    "    **********************************************\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 2: Instalación de Paquetes Necesarios**\n",
    "# Se listan las bibliotecas requeridas para el funcionamiento del chatbot.\n",
    "#\n",
    "# - **Líneas Clave:**\n",
    "#   - Uso de `requirements.txt` para instalar dependencias.\n",
    "#   - Listado de las principales bibliotecas usadas.\n",
    "# - **Oportunidad de Mejora:** Implementar una verificación automática de instalación.\n",
    "\n",
    "# %%\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 3: Importar Librerías y Configurar Logging**\n",
    "# Importa todas las librerías necesarias y configura un sistema de logs.\n",
    "#\n",
    "# - **Líneas Clave:**\n",
    "#   - Importación de librerías estándar (`os`, `json`, `logging`).\n",
    "#   - Importación de librerías específicas (`hnswlib`, `sentence_transformers`).\n",
    "#   - Configuración del sistema de logs.\n",
    "#   - Carga de variables de entorno desde un archivo `.env`.\n",
    "# - **Oportunidad de Mejora:** Separar configuraciones sensibles como la clave de API en una función de inicialización.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import hnswlib\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "# Configuración de logs\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Mensaje de confirmación de importación\n",
    "logging.info(\"Librerías importadas correctamente.\")\n",
    "\n",
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "logging.info(\"Variables de entorno cargadas desde el archivo .env.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 4: Cargar Documentos**\n",
    "# Carga documentos desde archivos o directorios.\n",
    "#\n",
    "# - **Líneas Clave:**\n",
    "#   - `load_documents`: Carga archivos de tipo `.txt`, `.json`, y `.pdf`.\n",
    "#   - `extract_content`: Procesa el contenido de los documentos dependiendo de su tipo.\n",
    "# - **Oportunidad de Mejora:** Mejorar la validación y manejo de excepciones para formatos no soportados.\n",
    "\n",
    "# %%\n",
    "def load_documents(source, is_directory=False):\n",
    "    \"\"\"\n",
    "    Carga documentos desde un archivo o directorio.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source):\n",
    "        logging.error(f\"La fuente '{source}' no existe.\")\n",
    "        raise FileNotFoundError(f\"La fuente '{source}' no se encontró.\")\n",
    "\n",
    "    loaded_files = []\n",
    "    if is_directory:\n",
    "        logging.info(f\"Iniciando carga desde el directorio: {source}.\")\n",
    "        for filename in os.listdir(source):\n",
    "            filepath = os.path.join(source, filename)\n",
    "            if os.path.isfile(filepath) and filepath.endswith(('.txt', '.json', '.pdf')):\n",
    "                content = extract_content(filepath)\n",
    "                if content:\n",
    "                    loaded_files.append({\"filename\": filename, \"content\": content})\n",
    "                    logging.info(f\"Archivo '{filename}' cargado correctamente.\")\n",
    "    else:\n",
    "        logging.info(f\"Iniciando carga del archivo: {source}.\")\n",
    "        content = extract_content(source)\n",
    "        if content:\n",
    "            loaded_files.append({\"filename\": os.path.basename(source), \"content\": content})\n",
    "            logging.info(f\"Archivo '{os.path.basename(source)}' cargado correctamente.\")\n",
    "\n",
    "    logging.info(f\"{len(loaded_files)} documentos cargados.\")\n",
    "    return loaded_files\n",
    "\n",
    "def extract_content(filepath):\n",
    "    \"\"\"\n",
    "    Extrae el contenido del archivo según su tipo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.txt'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            units = content.split(\"\\n-----\\n\")\n",
    "            return units\n",
    "        elif filepath.endswith('.json'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            return data\n",
    "        elif filepath.endswith('.pdf'):\n",
    "            reader = PdfReader(filepath)\n",
    "            return ''.join(page.extract_text() or '' for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al extraer contenido de '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuración de ruta y carga de documentos\n",
    "ruta_fuente = 'data'\n",
    "documentos = load_documents(ruta_fuente, is_directory=True)\n",
    "logging.info(f\"Se cargaron {len(documentos)} documentos exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 5: Configurar la Clave API de Gemini**\n",
    "# - Obtiene la clave API desde las variables de entorno.\n",
    "# - Crea una instancia del modelo Gemini para uso posterior.\n",
    "# OPORTUNIDAD DE MEJORA: Manejo más robusto ante una clave inválida o expiración.\n",
    "\n",
    "# %%\n",
    "gemini_llm = None\n",
    "\n",
    "def configure_gemini():\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        logging.error(\"La clave API de Gemini no está configurada.\")\n",
    "        raise EnvironmentError(\"Configura GEMINI_API_KEY en tu archivo .env.\")\n",
    "    gemini = Gemini(api_key=api_key)\n",
    "    logging.info(\"Gemini configurado correctamente.\")\n",
    "    return gemini\n",
    "\n",
    "gemini_llm = configure_gemini()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 6: Configurar el Modelo de Embeddings**\n",
    "# - Se utiliza SentenceTransformer para generar embeddings de texto.\n",
    "# - doc_enfermedad(pregunta): Determina el índice del documento más relevante \n",
    "#   según la similitud con el nombre del archivo.\n",
    "#\n",
    "# OPORTUNIDAD DE MEJORA:\n",
    "# - En lugar de basarse en el nombre del archivo, usar embeddings del contenido \n",
    "#   para mayor precisión.\n",
    "\n",
    "# %%\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "def doc_enfermedad(pregunta):\n",
    "    if not documentos:\n",
    "        logging.warning(\"No se encontraron documentos. Índice por defecto: 0.\")\n",
    "        return 0\n",
    "    preg_embedding = model.encode(pregunta)\n",
    "    archivos = [doc['filename'] for doc in documentos]\n",
    "    doc_filenames_embeddings = [model.encode(name) for name in archivos]\n",
    "    similarities = [util.cos_sim(preg_embedding, emb).item() for emb in doc_filenames_embeddings]\n",
    "    max_index = similarities.index(max(similarities))\n",
    "    return max_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 7: Crear Clases para Documentos e Índices**\n",
    "# - Clase Document: Representa un documento con su texto y metadatos.\n",
    "# - Clase HNSWIndex: Crea un índice para recuperación rápida de información mediante embeddings.\n",
    "#\n",
    "# OPORTUNIDAD DE MEJORA:\n",
    "# - Incluir más metadatos o estructuras de datos más complejas.\n",
    "\n",
    "# %%\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Título: {self.metadata.get('Title', 'N/A')}\\n\"\n",
    "            f\"Resumen: {self.metadata.get('Summary', 'N/A')}\\n\"\n",
    "            f\"Tipo de Estudio: {self.metadata.get('StudyType', 'N/A')}\\n\"\n",
    "            f\"Paises donde se desarrolla el estudio: {self.metadata.get('Countries', 'N/A')}\\n\"\n",
    "            f\"Fase en que se encuentra el estudio: {self.metadata.get('Phases', 'N/A')}\\n\"\n",
    "            f\"Identificación en ClinicaTrial: {self.metadata.get('IDestudio', 'N/A')}.\\n\\n\"\n",
    "        )\n",
    "\n",
    "class HNSWIndex:\n",
    "    def __init__(self, embeddings, metadata=None, space='cosine', ef_construction=200, M=16):\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = hnswlib.Index(space=space, dim=self.dimension)\n",
    "        self.index.init_index(max_elements=embeddings.shape[0], ef_construction=ef_construction, M=M)\n",
    "        self.index.add_items(embeddings, np.arange(embeddings.shape[0]))\n",
    "        self.index.set_ef(50) \n",
    "        self.metadata = metadata or []\n",
    "    \n",
    "    def similarity_search(self, query_vector, k=5):\n",
    "        labels, distances = self.index.knn_query(query_vector, k=k)\n",
    "        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(labels[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 8: Procesar Documentos y Crear Índices**\n",
    "# - desdobla_doc(data2): Crea objetos Document a partir del contenido.\n",
    "# - Para JSON con ensayos clínicos, crea un Document por ensayo.\n",
    "# - Para TXT/PDF, un Document genérico.\n",
    "# - Genera embeddings y construye el índice HNSWlib.\n",
    "#\n",
    "# OPORTUNIDAD DE MEJORA:\n",
    "# - Validar mejor la estructura JSON.\n",
    "# - Realizar preprocesamiento de texto (limpieza) antes de embeddings.\n",
    "\n",
    "# %%\n",
    "def desdobla_doc(data2):\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    contenido = data2['content']\n",
    "    \n",
    "    if isinstance(contenido, list):\n",
    "        for entry in contenido:\n",
    "            if isinstance(entry, dict):\n",
    "                nctId = entry.get(\"IDestudio\", \"\")\n",
    "                briefTitle = entry.get(\"Title\", \"\")\n",
    "                summary = entry.get(\"Summary\", \"\")\n",
    "                studyType = entry.get(\"StudyType\", \"\")\n",
    "                country = entry.get(\"Countries\", \"\")\n",
    "                overallStatus = entry.get(\"OverallStatus\", \"\")\n",
    "                conditions = entry.get(\"Conditions\", \"\")\n",
    "                phases = entry.get(\"Phases\", \"\")\n",
    "\n",
    "                Summary = (\n",
    "                    f\"The study titled '{briefTitle}', of type '{studyType}', \"\n",
    "                    f\"investigates the condition(s): {conditions}. \"\n",
    "                    f\"Brief summary: {summary}. \"\n",
    "                    f\"Current status: {overallStatus}, taking place in {country}. \"\n",
    "                    f\"The study is classified under: {phases} phase. \"\n",
    "                    f\"For more info, search {nctId} on ClinicalTrials.\"\n",
    "                )\n",
    "                metadata = {\n",
    "                    \"Title\": briefTitle,\n",
    "                    \"Summary\": Summary,\n",
    "                    \"StudyType\": studyType,\n",
    "                    \"Countries\": country,\n",
    "                    \"Phases\": phases,\n",
    "                    \"IDestudio\": nctId\n",
    "                }\n",
    "                doc = Document(Summary, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(Summary)\n",
    "            else:\n",
    "                texto = str(entry)\n",
    "                metadata = {\"Summary\": texto}\n",
    "                doc = Document(texto, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(texto)\n",
    "    else:\n",
    "        texto = str(contenido)\n",
    "        metadata = {\"Summary\": texto}\n",
    "        doc = Document(texto, metadata)\n",
    "        documents.append(doc)\n",
    "        summaries.append(texto)\n",
    "\n",
    "    if documents:\n",
    "        embeddings = model.encode([doc.page_content for doc in documents], show_progress_bar=False)\n",
    "        embeddings = np.array(embeddings).astype(np.float32)\n",
    "        vector_store = HNSWIndex(embeddings, metadata=[doc.metadata for doc in documents])\n",
    "    else:\n",
    "        vector_store = None\n",
    "\n",
    "    return documents, vector_store\n",
    "\n",
    "trozos_archivos = []\n",
    "index_archivos = []\n",
    "for i in range(len(documentos)):\n",
    "    trozos, index = desdobla_doc(documentos[i])\n",
    "    trozos_archivos.append(trozos)\n",
    "    index_archivos.append(index)\n",
    "\n",
    "logging.info(\"Índices HNSWlib creados para todos los documentos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 9: Traducir Preguntas y Respuestas**\n",
    "# - traducir(texto, idioma_destino): Usa Gemini para traducir el texto solicitado.\n",
    "# - generate_embedding(texto): Genera embeddings para la pregunta en inglés.\n",
    "#\n",
    "# OPORTUNIDAD DE MEJORA:\n",
    "# - Implementar caché de traducciones.\n",
    "# - Detección de idioma para traducir sólo si es necesario.\n",
    "\n",
    "# %%\n",
    "def traducir(texto, idioma_destino):\n",
    "    start_time = time.time()\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Actúa como un traductor.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Por favor, traduce este texto al {idioma_destino}: {texto}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Traducción completada en {elapsed_time:.2f} segundos.\")\n",
    "        return respuesta.message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al traducir: {e}\")\n",
    "        return texto\n",
    "\n",
    "def generate_embedding(texto):\n",
    "    try:\n",
    "        embedding = model.encode([texto])\n",
    "        logging.info(f\"Embedding generado para el texto: {texto}\")\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar el embedding: {e}\")\n",
    "        # Devuelve embedding vacío como fallback\n",
    "        return np.zeros((1, 384))\n",
    "\n",
    "def obtener_contexto(pregunta, index, trozos, top_k=50):\n",
    "    \"\"\"\n",
    "    Recupera los trozos de texto más relevantes para responder la pregunta.\n",
    "    Traduce la pregunta al inglés antes de buscar en el índice.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pregunta_en_ingles = traducir(pregunta, \"inglés\")\n",
    "        logging.info(f\"Pregunta traducida al inglés: {pregunta_en_ingles}\")\n",
    "\n",
    "        pregunta_emb = generate_embedding(pregunta_en_ingles)\n",
    "        logging.info(\"Embedding generado para la pregunta.\")\n",
    "\n",
    "        results = index.similarity_search(pregunta_emb, k=top_k)\n",
    "        texto = \"\"\n",
    "        for entry in results:\n",
    "            resum = entry[0][\"Summary\"]\n",
    "            texto += resum + \"\\n\"\n",
    "\n",
    "        logging.info(\"Contexto relevante recuperado para la pregunta.\")\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al obtener el contexto: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 10: Generar Respuestas**\n",
    "# - categorizar_pregunta: Usa palabras clave para clasificar la pregunta (ej: 'ensayo', 'tratamiento').\n",
    "# - generar_prompt: Crea una instrucción específica según la categoría.\n",
    "# - generar_respuesta: \n",
    "#   - Envía el prompt y el contexto a Gemini.\n",
    "#   - Traduce la respuesta al español.\n",
    "#\n",
    "# OPORTUNIDAD DE MEJORA:\n",
    "# - Usar un modelo de clasificación semántica en lugar de palabras clave.\n",
    "# - Detectar el idioma de la pregunta y responder en el mismo idioma.\n",
    "\n",
    "# %%\n",
    "def categorizar_pregunta(pregunta):\n",
    "    categorias = {\n",
    "        \"tratamiento\": [\"tratamiento\", \"medicación\", \"cura\", \"terapia\", \"fármaco\"],\n",
    "        \"ensayo\": [\"ensayo\", \"estudio\", \"prueba\", \"investigación\", \"trial\"],\n",
    "        \"resultado\": [\"resultado\", \"efectividad\", \"resultados\", \"éxito\", \"fracaso\"],\n",
    "        \"prevención\": [\"prevención\", \"previene\", \"evitar\", \"reducción de riesgo\"]\n",
    "    }\n",
    "    for categoria, palabras in categorias.items():\n",
    "        if any(palabra in pregunta.lower() for palabra in palabras):\n",
    "            return categoria\n",
    "    return \"general\"\n",
    "\n",
    "def generar_prompt(categoria, pregunta):\n",
    "    prompts = {\n",
    "        \"tratamiento\": f\"Proporciona información sobre tratamientos en ensayos clínicos relacionados con: {pregunta}.\",\n",
    "        \"ensayo\": f\"Describe los ensayos clínicos actuales relacionados con: {pregunta}.\",\n",
    "        \"resultado\": f\"Explica los resultados más recientes de ensayos clínicos sobre: {pregunta}.\",\n",
    "        \"prevención\": f\"Ofrece información sobre prevención y ensayos clínicos para: {pregunta}.\"\n",
    "    }\n",
    "    return prompts.get(categoria, \"Por favor, responde la pregunta sobre ensayos clínicos.\")\n",
    "\n",
    "def es_saludo(pregunta):\n",
    "    saludos = [\"hola\", \"buen día\", \"buenas\", \"cómo estás\", \"cómo te llamas\", \"qué tal\", \"estás bien\", \"buenas tardes\", \"buenas noches\"]\n",
    "    return any(saludo in pregunta.lower() for saludo in saludos)\n",
    "\n",
    "def responder_saludo():\n",
    "    saludos_respuestas = [\n",
    "        \"¡Hola! Estoy para ayudarte con información sobre ensayos clínicos. ¿En qué puedo asistirte hoy?\",\n",
    "        \"¡Buenas! Tenés alguna pregunta sobre ensayos clínicos en enfermedades neuromusculares?\",\n",
    "        \"¡Hola! ¿Cómo puedo ayudarte con tus consultas sobre ensayos clínicos?\"\n",
    "    ]\n",
    "    import random\n",
    "    return random.choice(saludos_respuestas)\n",
    "\n",
    "def generar_respuesta(pregunta, contexto, prompt_especifico):\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Eres un experto médico.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"{prompt_especifico}\\nContexto: {contexto}\\nPregunta: {pregunta}\")\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Respuesta generada en inglés en {elapsed_time:.2f} segundos.\")\n",
    "        respuesta_en_espanol = traducir(respuesta.message.content, \"español\")\n",
    "        logging.info(\"Respuesta traducida al español.\")\n",
    "        return respuesta_en_espanol\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar la respuesta: {e}\")\n",
    "        return \"Lo siento, ocurrió un error al generar la respuesta.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 11: Función Principal para Responder Preguntas**\n",
    "# - Utiliza caché para no recalcular respuestas idénticas.\n",
    "# - Integra categorización, obtención de contexto y generación de respuesta.\n",
    "#\n",
    "# OPORTUNIDAD DE MEJORA:\n",
    "# - Expirar caché después de cierto tiempo.\n",
    "# - Manejar mejor errores de contexto.\n",
    "\n",
    "# %%\n",
    "def generar_hash(pregunta):\n",
    "    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()\n",
    "\n",
    "def obtener_respuesta_cacheada(pregunta):\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    if os.path.exists(archivo_cache):\n",
    "        try:\n",
    "            with open(archivo_cache, \"r\", encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                return datos.get(\"respuesta\", None)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al leer el caché: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def guardar_respuesta_cacheada(pregunta, respuesta):\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)\n",
    "        with open(archivo_cache, \"w\", encoding='utf-8') as f:\n",
    "            json.dump({\"pregunta\": pregunta, \"respuesta\": respuesta}, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Respuesta cacheada para la pregunta: '{pregunta}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar la respuesta en caché: {e}\")\n",
    "\n",
    "def responder_pregunta(pregunta, index, trozos):\n",
    "    try:\n",
    "        if index is None or not trozos:\n",
    "            logging.warning(\"No se encontraron índices o trozos para esta pregunta.\")\n",
    "            return \"No se encontró información para responder tu pregunta.\"\n",
    "\n",
    "        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)\n",
    "        if respuesta_cacheada:\n",
    "            logging.info(f\"Respuesta obtenida del caché para: '{pregunta}'\")\n",
    "            return respuesta_cacheada\n",
    "\n",
    "        categoria = categorizar_pregunta(pregunta)\n",
    "        logging.info(f\"Categoría de la pregunta: {categoria}\")\n",
    "        prompt_especifico = generar_prompt(categoria, pregunta)\n",
    "        logging.info(f\"Prompt específico: {prompt_especifico}\")\n",
    "\n",
    "        contexto = obtener_contexto(pregunta, index, trozos)\n",
    "        if not contexto.strip():\n",
    "            logging.warning(\"No se encontró contexto relevante.\")\n",
    "            respuesta = \"No pude encontrar información relevante para responder tu pregunta.\"\n",
    "            guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "            return respuesta\n",
    "\n",
    "        respuesta = generar_respuesta(pregunta, contexto, prompt_especifico)\n",
    "        guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso de responder pregunta: {e}\")\n",
    "        return \"Ocurrió un error al procesar tu pregunta.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 12: Interfaz CLI**\n",
    "# Ofrece una interfaz de línea de comando para interactuar con el chatbot.\n",
    "#\n",
    "# - Espera una pregunta del usuario.\n",
    "# - Responde saludos.\n",
    "# - Detecta si el usuario quiere salir.\n",
    "# - Llama a responder_pregunta() con la información necesaria.\n",
    "#\n",
    "# OPORTUNIDAD DE MEJORA:\n",
    "# - Crear una interfaz web amigable.\n",
    "# - Almacenar historial de preguntas y respuestas.\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    \n",
    "    if len(documentos) == 0:\n",
    "        print(\"No se cargaron documentos. Por favor, verifica el directorio 'data'.\")\n",
    "        logging.error(\"No se encontraron documentos. Finalizando.\")\n",
    "    else:\n",
    "        print(\"Bienvenido al Chatbot de Ensayos Clínicos\")\n",
    "        print(\"Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\")\n",
    "        print(\"Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\")\n",
    "        while True:\n",
    "            pregunta = input(\"Tu pregunta: \").strip()\n",
    "            if pregunta.lower() in ['salir', 'chau', 'exit', 'quit']:\n",
    "                print(\"¡Chau!\")\n",
    "                logging.info(\"El usuario ha finalizado la sesión.\")\n",
    "                break\n",
    "            if es_saludo(pregunta):\n",
    "                respuesta_saludo = responder_saludo()\n",
    "                print(respuesta_saludo)\n",
    "                logging.info(\"Se detectó un saludo.\")\n",
    "                continue\n",
    "            \n",
    "            idn = doc_enfermedad(pregunta)\n",
    "            index = index_archivos[idn] if idn < len(index_archivos) else None\n",
    "            trozos = trozos_archivos[idn] if idn < len(trozos_archivos) else []\n",
    "\n",
    "            respuesta = responder_pregunta(pregunta, index, trozos)\n",
    "            print(f\"Respuesta: {respuesta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 1: Verificar versión de Python**\n",
    "# Se valida que la versión de Python sea la requerida.\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "REQUIRED_VERSION = (3, 10, 12)\n",
    "current_version = sys.version_info\n",
    "\n",
    "if (current_version.major, current_version.minor, current_version.micro) != REQUIRED_VERSION:\n",
    "    logging.warning(f\"\"\"\n",
    "    **********************************************\n",
    "    ** Advertencia: Versión de Python no compatible **\n",
    "    **********************************************\n",
    "    Este chatbot está optimizado para Python {REQUIRED_VERSION[0]}.{REQUIRED_VERSION[1]}.{REQUIRED_VERSION[2]}.\n",
    "    La versión actual es Python {current_version.major}.{current_version.minor}.{current_version.micro}.\n",
    "    Algunas funcionalidades pueden no funcionar correctamente.\n",
    "    **********************************************\n",
    "    \"\"\")\n",
    "else:\n",
    "    logging.info(\"\"\"\n",
    "    **********************************************\n",
    "    ** Versión de Python compatible **\n",
    "    **********************************************\n",
    "    Python 3.10.12 detectado correctamente.\n",
    "    Todas las funcionalidades deberían operar sin problemas.\n",
    "    **********************************************\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 2: Instalación de Paquetes Necesarios**\n",
    "# Se utilizan las librerías indicadas en `requirements.txt`.\n",
    "\n",
    "# %%\n",
    "%pip install -r requirements.txt\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 3: Importar Librerías y Configurar Logging**\n",
    "# Importa las librerías necesarias y configura el sistema de logs.\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import hnswlib\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "logging.info(\"Librerías importadas correctamente.\")\n",
    "load_dotenv()\n",
    "logging.info(\"Variables de entorno cargadas desde el archivo .env.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 4: Cargar Documentos**\n",
    "# Carga documentos desde archivos o directorios en formatos `.txt`, `.json`, `.pdf`.\n",
    "\n",
    "# %%\n",
    "def load_documents(source, is_directory=False):\n",
    "    if not os.path.exists(source):\n",
    "        logging.error(f\"La fuente '{source}' no existe.\")\n",
    "        raise FileNotFoundError(f\"La fuente '{source}' no se encontró.\")\n",
    "\n",
    "    loaded_files = []\n",
    "    if is_directory:\n",
    "        logging.info(f\"Iniciando carga desde el directorio: {source}.\")\n",
    "        for filename in os.listdir(source):\n",
    "            filepath = os.path.join(source, filename)\n",
    "            if os.path.isfile(filepath) and filepath.endswith(('.txt', '.json', '.pdf')):\n",
    "                content = extract_content(filepath)\n",
    "                if content:\n",
    "                    loaded_files.append({\"filename\": filename, \"content\": content})\n",
    "                    logging.info(f\"Archivo '{filename}' cargado correctamente.\")\n",
    "    else:\n",
    "        logging.info(f\"Iniciando carga del archivo: {source}.\")\n",
    "        content = extract_content(source)\n",
    "        if content:\n",
    "            loaded_files.append({\"filename\": os.path.basename(source), \"content\": content})\n",
    "            logging.info(f\"Archivo '{os.path.basename(source)}' cargado correctamente.\")\n",
    "\n",
    "    logging.info(f\"{len(loaded_files)} documentos cargados.\")\n",
    "    return loaded_files\n",
    "\n",
    "def extract_content(filepath):\n",
    "    try:\n",
    "        if filepath.endswith('.txt'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            units = content.split(\"\\n-----\\n\")\n",
    "            return units\n",
    "        elif filepath.endswith('.json'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            return data\n",
    "        elif filepath.endswith('.pdf'):\n",
    "            reader = PdfReader(filepath)\n",
    "            return ''.join(page.extract_text() or '' for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al extraer contenido de '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "ruta_fuente = 'data'\n",
    "documentos = load_documents(ruta_fuente, is_directory=True)\n",
    "logging.info(f\"Se cargaron {len(documentos)} documentos exitosamente.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 5: Configurar la Clave API de Gemini**\n",
    "# Obtiene la clave desde las variables de entorno y crea la instancia del modelo Gemini.\n",
    "\n",
    "# %%\n",
    "gemini_llm = None\n",
    "\n",
    "def configure_gemini():\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        logging.error(\"La clave API de Gemini no está configurada.\")\n",
    "        raise EnvironmentError(\"Configura GEMINI_API_KEY en tu archivo .env.\")\n",
    "    gemini = Gemini(api_key=api_key)\n",
    "    logging.info(\"Gemini configurado correctamente.\")\n",
    "    return gemini\n",
    "\n",
    "gemini_llm = configure_gemini()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 6: Configurar el Modelo de Embeddings**\n",
    "# Utiliza SentenceTransformer para generar embeddings.\n",
    "\n",
    "# %%\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Precompute embeddings de filenames una sola vez\n",
    "archivos = [doc['filename'] for doc in documentos]\n",
    "archivos_embeddings = model.encode(archivos)\n",
    "logging.info(\"Embeddings de nombres de archivos precalculados.\")\n",
    "\n",
    "\n",
    "def doc_enfermedad(pregunta):\n",
    "    \"\"\"\n",
    "    Identifica el índice del documento más relevante para la pregunta,\n",
    "    usando embeddings precalculados de los nombres de archivo.\n",
    "    \"\"\"\n",
    "    if not documentos:\n",
    "        logging.warning(\"No se encontraron documentos. Índice por defecto: 0.\")\n",
    "        return 0\n",
    "    preg_embedding = model.encode(pregunta)\n",
    "    similarities = [util.cos_sim(preg_embedding, emb.reshape(1, -1)).item() for emb in archivos_embeddings]\n",
    "    max_index = similarities.index(max(similarities))\n",
    "    return max_index\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 7: Crear Clases para Documentos e Índices**\n",
    "# Document y HNSWIndex para manejar la indexación y búsqueda de textos.\n",
    "\n",
    "# %%\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Título: {self.metadata.get('Title', 'N/A')}\\n\"\n",
    "            f\"Resumen: {self.metadata.get('Summary', 'N/A')}\\n\"\n",
    "            f\"Tipo de Estudio: {self.metadata.get('StudyType', 'N/A')}\\n\"\n",
    "            f\"Paises donde se desarrolla el estudio: {self.metadata.get('Countries', 'N/A')}\\n\"\n",
    "            f\"Fase en que se encuentra el estudio: {self.metadata.get('Phases', 'N/A')}\\n\"\n",
    "            f\"Identificación en ClinicaTrial: {self.metadata.get('IDestudio', 'N/A')}.\\n\\n\"\n",
    "        )\n",
    "\n",
    "class HNSWIndex:\n",
    "    def __init__(self, embeddings, metadata=None, space='cosine', ef_construction=200, M=16):\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = hnswlib.Index(space=space, dim=self.dimension)\n",
    "        self.index.init_index(max_elements=embeddings.shape[0], ef_construction=ef_construction, M=M)\n",
    "        self.index.add_items(embeddings, np.arange(embeddings.shape[0]))\n",
    "        self.index.set_ef(50) \n",
    "        self.metadata = metadata or []\n",
    "    \n",
    "    def similarity_search(self, query_vector, k=5):\n",
    "        labels, distances = self.index.knn_query(query_vector, k=k)\n",
    "        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(labels[0])]\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 8: Procesar Documentos y Crear Índices**\n",
    "# Crea objetos Document e índices HNSWlib.\n",
    "\n",
    "# %%\n",
    "def desdobla_doc(data2):\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    contenido = data2['content']\n",
    "    \n",
    "    if isinstance(contenido, list):\n",
    "        for entry in contenido:\n",
    "            if isinstance(entry, dict):\n",
    "                nctId = entry.get(\"IDestudio\", \"\")\n",
    "                briefTitle = entry.get(\"Title\", \"\")\n",
    "                summary = entry.get(\"Summary\", \"\")\n",
    "                studyType = entry.get(\"StudyType\", \"\")\n",
    "                country = entry.get(\"Countries\", \"\")\n",
    "                overallStatus = entry.get(\"OverallStatus\", \"\")\n",
    "                conditions = entry.get(\"Conditions\", \"\")\n",
    "                phases = entry.get(\"Phases\", \"\")\n",
    "\n",
    "                Summary = (\n",
    "                    f\"The study titled '{briefTitle}', of type '{studyType}', \"\n",
    "                    f\"investigates the condition(s): {conditions}. \"\n",
    "                    f\"Brief summary: {summary}. \"\n",
    "                    f\"Current status: {overallStatus}, taking place in {country}. \"\n",
    "                    f\"The study is classified under: {phases} phase. \"\n",
    "                    f\"For more info, search {nctId} on ClinicalTrials.\"\n",
    "                )\n",
    "                metadata = {\n",
    "                    \"Title\": briefTitle,\n",
    "                    \"Summary\": Summary,\n",
    "                    \"StudyType\": studyType,\n",
    "                    \"Countries\": country,\n",
    "                    \"Phases\": phases,\n",
    "                    \"IDestudio\": nctId\n",
    "                }\n",
    "                doc = Document(Summary, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(Summary)\n",
    "            else:\n",
    "                # Entrada genérica\n",
    "                texto = str(entry)\n",
    "                metadata = {\"Summary\": texto}\n",
    "                doc = Document(texto, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(texto)\n",
    "    else:\n",
    "        texto = str(contenido)\n",
    "        metadata = {\"Summary\": texto}\n",
    "        doc = Document(texto, metadata)\n",
    "        documents.append(doc)\n",
    "        summaries.append(texto)\n",
    "\n",
    "    if documents:\n",
    "        embeddings = model.encode([doc.page_content for doc in documents], show_progress_bar=False)\n",
    "        embeddings = np.array(embeddings).astype(np.float32)\n",
    "        vector_store = HNSWIndex(embeddings, metadata=[doc.metadata for doc in documents])\n",
    "    else:\n",
    "        vector_store = None\n",
    "\n",
    "    return documents, vector_store\n",
    "\n",
    "trozos_archivos = []\n",
    "index_archivos = []\n",
    "for i in range(len(documentos)):\n",
    "    trozos, index = desdobla_doc(documentos[i])\n",
    "    trozos_archivos.append(trozos)\n",
    "    index_archivos.append(index)\n",
    "\n",
    "logging.info(\"Índices HNSWlib creados para todos los documentos.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 9: Traducir Preguntas y Respuestas**\n",
    "# Usa Gemini para traducir texto al idioma deseado.\n",
    "#\n",
    "# OPORTUNIDAD DE MEJORA: Implementar detección de idioma si se requiere.\n",
    "\n",
    "# %%\n",
    "translation_cache = {}\n",
    "embedding_cache = {}\n",
    "\n",
    "def traducir(texto, idioma_destino):\n",
    "    # TODO: Implementar detección de idioma si se desea, para evitar traducciones innecesarias.\n",
    "    # Por ahora, traducimos siempre.\n",
    "    cache_key = (texto, idioma_destino)\n",
    "    if cache_key in translation_cache:\n",
    "        logging.info(\"Traducción obtenida de caché.\")\n",
    "        return translation_cache[cache_key]\n",
    "\n",
    "    start_time = time.time()\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Actúa como un traductor.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Por favor, traduce este texto al {idioma_destino}: {texto}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Traducción completada en {elapsed_time:.2f} segundos.\")\n",
    "        translation_cache[cache_key] = respuesta.message.content.strip()\n",
    "        return respuesta.message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al traducir: {e}\")\n",
    "        return texto\n",
    "\n",
    "def generate_embedding(texto):\n",
    "    if texto in embedding_cache:\n",
    "        logging.info(\"Embedding obtenido de caché.\")\n",
    "        return embedding_cache[texto]\n",
    "\n",
    "    try:\n",
    "        embedding = model.encode([texto])\n",
    "        logging.info(f\"Embedding generado para el texto: {texto}\")\n",
    "        embedding_cache[texto] = embedding\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar el embedding: {e}\")\n",
    "        return np.zeros((1, 384))\n",
    "\n",
    "def obtener_contexto(pregunta, index, trozos, top_k=50):\n",
    "    try:\n",
    "        pregunta_en_ingles = traducir(pregunta, \"inglés\")\n",
    "        logging.info(f\"Pregunta traducida al inglés: {pregunta_en_ingles}\")\n",
    "\n",
    "        pregunta_emb = generate_embedding(pregunta_en_ingles)\n",
    "        logging.info(\"Embedding generado para la pregunta.\")\n",
    "\n",
    "        results = index.similarity_search(pregunta_emb, k=top_k)\n",
    "        texto = \"\"\n",
    "        for entry in results:\n",
    "            resum = entry[0][\"Summary\"]\n",
    "            texto += resum + \"\\n\"\n",
    "\n",
    "        logging.info(\"Contexto relevante recuperado para la pregunta.\")\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al obtener el contexto: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 10: Generar Respuestas**\n",
    "# Clasifica la pregunta, genera un prompt y obtiene una respuesta traducida al español.\n",
    "\n",
    "# %%\n",
    "def categorizar_pregunta(pregunta):\n",
    "    categorias = {\n",
    "        \"tratamiento\": [\"tratamiento\", \"medicación\", \"cura\", \"terapia\", \"fármaco\"],\n",
    "        \"ensayo\": [\"ensayo\", \"estudio\", \"prueba\", \"investigación\", \"trial\"],\n",
    "        \"resultado\": [\"resultado\", \"efectividad\", \"resultados\", \"éxito\", \"fracaso\"],\n",
    "        \"prevención\": [\"prevención\", \"previene\", \"evitar\", \"reducción de riesgo\"]\n",
    "    }\n",
    "    for categoria, palabras in categorias.items():\n",
    "        if any(palabra in pregunta.lower() for palabra in palabras):\n",
    "            return categoria\n",
    "    return \"general\"\n",
    "\n",
    "def generar_prompt(categoria, pregunta):\n",
    "    prompts = {\n",
    "        \"tratamiento\": f\"Proporciona información sobre tratamientos en ensayos clínicos relacionados con: {pregunta}.\",\n",
    "        \"ensayo\": f\"Describe los ensayos clínicos actuales relacionados con: {pregunta}.\",\n",
    "        \"resultado\": f\"Explica los resultados más recientes de ensayos clínicos sobre: {pregunta}.\",\n",
    "        \"prevención\": f\"Ofrece información sobre prevención y ensayos clínicos para: {pregunta}.\"\n",
    "    }\n",
    "    return prompts.get(categoria, \"Por favor, responde la pregunta sobre ensayos clínicos.\")\n",
    "\n",
    "def es_saludo(pregunta):\n",
    "    saludos = [\"hola\", \"buen día\", \"buenas\", \"cómo estás\", \"cómo te llamas\", \"qué tal\", \"estás bien\", \"buenas tardes\", \"buenas noches\"]\n",
    "    return any(saludo in pregunta.lower() for saludo in saludos)\n",
    "\n",
    "def responder_saludo():\n",
    "    saludos_respuestas = [\n",
    "        \"¡Hola! Estoy para ayudarte con información sobre ensayos clínicos. ¿En qué puedo asistirte hoy?\",\n",
    "        \"¡Buenas! Tenés alguna pregunta sobre ensayos clínicos en enfermedades neuromusculares?\",\n",
    "        \"¡Hola! ¿Cómo puedo ayudarte con tus consultas sobre ensayos clínicos?\"\n",
    "    ]\n",
    "    import random\n",
    "    return random.choice(saludos_respuestas)\n",
    "\n",
    "def generar_respuesta(pregunta, contexto, prompt_especifico):\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Eres un experto médico.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"{prompt_especifico}\\nContexto: {contexto}\\nPregunta: {pregunta}\")\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Respuesta generada en inglés en {elapsed_time:.2f} segundos.\")\n",
    "        respuesta_en_espanol = traducir(respuesta.message.content, \"español\")\n",
    "        logging.info(\"Respuesta traducida al español.\")\n",
    "        return respuesta_en_espanol\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar la respuesta: {e}\")\n",
    "        return \"Lo siento, ocurrió un error al generar la respuesta.\"\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 11: Función Principal para Responder Preguntas**\n",
    "# Integra caché, categorización, contexto y respuesta.\n",
    "\n",
    "# %%\n",
    "def generar_hash(pregunta):\n",
    "    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()\n",
    "\n",
    "def obtener_respuesta_cacheada(pregunta):\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    if os.path.exists(archivo_cache):\n",
    "        try:\n",
    "            with open(archivo_cache, \"r\", encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                return datos.get(\"respuesta\", None)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al leer el caché: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def guardar_respuesta_cacheada(pregunta, respuesta):\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)\n",
    "        with open(archivo_cache, \"w\", encoding='utf-8') as f:\n",
    "            json.dump({\"pregunta\": pregunta, \"respuesta\": respuesta}, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Respuesta cacheada para la pregunta: '{pregunta}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar la respuesta en caché: {e}\")\n",
    "\n",
    "def responder_pregunta(pregunta, index, trozos):\n",
    "    try:\n",
    "        if index is None or not trozos:\n",
    "            logging.warning(\"No se encontraron índices o trozos para esta pregunta.\")\n",
    "            return \"No se encontró información para responder tu pregunta.\"\n",
    "\n",
    "        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)\n",
    "        if respuesta_cacheada:\n",
    "            logging.info(f\"Respuesta obtenida del caché para: '{pregunta}'\")\n",
    "            return respuesta_cacheada\n",
    "\n",
    "        categoria = categorizar_pregunta(pregunta)\n",
    "        logging.info(f\"Categoría de la pregunta: {categoria}\")\n",
    "        prompt_especifico = generar_prompt(categoria, pregunta)\n",
    "        logging.info(f\"Prompt específico: {prompt_especifico}\")\n",
    "\n",
    "        contexto = obtener_contexto(pregunta, index, trozos)\n",
    "        if not contexto.strip():\n",
    "            logging.warning(\"No se encontró contexto relevante.\")\n",
    "            respuesta = \"No pude encontrar información relevante para responder tu pregunta.\"\n",
    "            guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "            return respuesta\n",
    "\n",
    "        respuesta = generar_respuesta(pregunta, contexto, prompt_especifico)\n",
    "        guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso de responder pregunta: {e}\")\n",
    "        return \"Ocurrió un error al procesar tu pregunta.\"\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ### **PASO 12: Interfaz CLI**\n",
    "# Proporciona interfaz en línea de comando.\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    \n",
    "    if len(documentos) == 0:\n",
    "        print(\"No se cargaron documentos. Por favor, verifica el directorio 'data'.\")\n",
    "        logging.error(\"No se encontraron documentos. Finalizando.\")\n",
    "    else:\n",
    "        print(\"Bienvenido al Chatbot de Ensayos Clínicos\")\n",
    "        print(\"Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\")\n",
    "        print(\"Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\")\n",
    "        while True:\n",
    "            pregunta = input(\"Tu pregunta: \").strip()\n",
    "            if pregunta.lower() in ['salir', 'chau', 'exit', 'quit']:\n",
    "                print(\"¡Chau!\")\n",
    "                logging.info(\"El usuario ha finalizado la sesión.\")\n",
    "                break\n",
    "\n",
    "            if es_saludo(pregunta):\n",
    "                respuesta_saludo = responder_saludo()\n",
    "                print(respuesta_saludo)\n",
    "                logging.info(\"Se detectó un saludo.\")\n",
    "                continue\n",
    "            \n",
    "            idn = doc_enfermedad(pregunta)\n",
    "            index = index_archivos[idn] if idn < len(index_archivos) else None\n",
    "            trozos = trozos_archivos[idn] if idn < len(trozos_archivos) else []\n",
    "\n",
    "            respuesta = responder_pregunta(pregunta, index, trozos)\n",
    "            print(f\"Respuesta: {respuesta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 23:49:44,322 - INFO - Cargando entorno y verificando variables...\n",
      "2024-12-06 23:49:45,307 - INFO - Gemini configurado correctamente.\n",
      "2024-12-06 23:49:45,309 - INFO - Iniciando carga desde el directorio: data.\n",
      "2024-12-06 23:49:45,318 - INFO - Archivo 'resumenes_lupus.json' cargado correctamente.\n",
      "2024-12-06 23:49:45,321 - INFO - Archivo 'resumenes_duchenne.json' cargado correctamente.\n",
      "2024-12-06 23:49:45,328 - INFO - Archivo 'resumenes_pompe.json' cargado correctamente.\n",
      "2024-12-06 23:49:45,330 - INFO - Archivo 'resumenes_becker.json' cargado correctamente.\n",
      "2024-12-06 23:49:45,332 - INFO - Archivo 'resumenes_glycogen storage disease.json' cargado correctamente.\n",
      "2024-12-06 23:49:45,334 - INFO - Archivo 'resumenes_myotonic dystrophy.json' cargado correctamente.\n",
      "2024-12-06 23:49:45,335 - INFO - 6 documentos cargados.\n",
      "2024-12-06 23:49:45,336 - INFO - Se cargaron 6 documentos exitosamente.\n",
      "2024-12-06 23:49:45,346 - INFO - Use pytorch device_name: mps\n",
      "2024-12-06 23:49:45,346 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2024-12-06 23:49:47,639 - INFO - Modelo de embeddings cargado.\n",
      "2024-12-06 23:52:03,835 - INFO - Índices HNSWlib creados para todos los documentos.\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.01it/s]\n",
      "2024-12-06 23:52:03,907 - INFO - Embeddings de nombres de archivos precalculados.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al Chatbot de Ensayos Clínicos\n",
      "Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\n",
      "Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "2024-12-07 00:06:51,908 - INFO - Categoría de la pregunta: ensayo\n",
      "2024-12-07 00:06:51,910 - INFO - Prompt específico: Describe los ensayos clínicos actuales relacionados con: ¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?.\n",
      "2024-12-07 00:06:51,911 - INFO - Texto ya en el idioma destino, no se traduce.\n",
      "2024-12-07 00:06:51,912 - INFO - Pregunta traducida al inglés: ¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]\n",
      "2024-12-07 00:06:52,086 - INFO - Embedding generado para el texto: ¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?\n",
      "2024-12-07 00:06:52,087 - INFO - Embedding generado para la pregunta.\n",
      "2024-12-07 00:06:52,095 - INFO - Contexto relevante recuperado para la pregunta.\n",
      "2024-12-07 00:06:55,464 - INFO - Respuesta generada en inglés en 3.37 segundos.\n",
      "2024-12-07 00:06:55,465 - INFO - Texto ya en el idioma destino, no se traduce.\n",
      "2024-12-07 00:06:55,467 - INFO - Respuesta traducida al español.\n",
      "2024-12-07 00:06:55,471 - INFO - Respuesta cacheada para la pregunta: '¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: Basándome en la información proporcionada, hay **dos** ensayos clínicos activos actualmente para la Distrofia Muscular de Duchenne:\n",
      "\n",
      "1. **NCT05712447:**  \"Duchenne Muscular Dystrophy Video Assessment Registry\"  (Estado: ACTIVE_NOT_RECRUITING)\n",
      "2. **NCT04587908:** \"A Phase 3 Study of TAS-205 in Patients With Duchenne Muscular Dystrophy（REACH-DMD）\" (Estado: ACTIVE_NOT_RECRUITING)\n",
      "\n",
      "\n",
      "Es importante destacar que \"activo\" puede interpretarse de diferentes maneras.  Estos dos estudios están activos en el sentido de que aún están recopilando y analizando datos, aunque no estén reclutando nuevos participantes.  Muchos otros estudios listados están completados, reclutamiento, o con estado desconocido, por lo que no se consideran activos en este momento.  Para una lista completa y actualizada de ensayos clínicos activos, se debe consultar directamente la base de datos de ClinicalTrials.gov.  Mi análisis se limita a la información proporcionada.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:07:01,125 - INFO - El usuario ha finalizado la sesión.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Chau!\n"
     ]
    }
   ],
   "source": [
    "# Ajustes solicitados aplicados sobre la misma base de código\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import hnswlib\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logging.info(\"Cargando entorno y verificando variables...\")\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    logging.error(\"La clave API de Gemini no está configurada.\")\n",
    "    raise EnvironmentError(\"Configura GEMINI_API_KEY en tu archivo .env.\")\n",
    "gemini_llm = Gemini(api_key=api_key)\n",
    "logging.info(\"Gemini configurado correctamente.\")\n",
    "\n",
    "\n",
    "# Carga de documentos\n",
    "def extract_content(filepath):\n",
    "    try:\n",
    "        if filepath.endswith('.txt'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            units = content.split(\"\\n-----\\n\")\n",
    "            return units\n",
    "        elif filepath.endswith('.json'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            return data\n",
    "        elif filepath.endswith('.pdf'):\n",
    "            reader = PdfReader(filepath)\n",
    "            return ''.join(page.extract_text() or '' for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al extraer contenido de '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "def load_documents(source, is_directory=False):\n",
    "    if not os.path.exists(source):\n",
    "        logging.error(f\"La fuente '{source}' no existe.\")\n",
    "        raise FileNotFoundError(f\"La fuente '{source}' no se encontró.\")\n",
    "\n",
    "    loaded_files = []\n",
    "    if is_directory:\n",
    "        logging.info(f\"Iniciando carga desde el directorio: {source}.\")\n",
    "        for filename in os.listdir(source):\n",
    "            filepath = os.path.join(source, filename)\n",
    "            if os.path.isfile(filepath) and filepath.endswith(('.txt', '.json', '.pdf')):\n",
    "                content = extract_content(filepath)\n",
    "                if content:\n",
    "                    loaded_files.append({\"filename\": filename, \"content\": content})\n",
    "                    logging.info(f\"Archivo '{filename}' cargado correctamente.\")\n",
    "    else:\n",
    "        logging.info(f\"Iniciando carga del archivo: {source}.\")\n",
    "        content = extract_content(source)\n",
    "        if content:\n",
    "            loaded_files.append({\"filename\": os.path.basename(source), \"content\": content})\n",
    "            logging.info(f\"Archivo '{os.path.basename(source)}' cargado correctamente.\")\n",
    "    logging.info(f\"{len(loaded_files)} documentos cargados.\")\n",
    "    return loaded_files\n",
    "\n",
    "ruta_fuente = 'data'\n",
    "documentos = load_documents(ruta_fuente, is_directory=True)\n",
    "logging.info(f\"Se cargaron {len(documentos)} documentos exitosamente.\")\n",
    "\n",
    "\n",
    "# Modelo de embeddings\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "logging.info(\"Modelo de embeddings cargado.\")\n",
    "\n",
    "\n",
    "# Clases Document e HNSWIndex\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (\n",
    "            f\"Título: {self.metadata.get('Title', 'N/A')}\\n\"\n",
    "            f\"Resumen: {self.metadata.get('Summary', 'N/A')}\\n\"\n",
    "            f\"Tipo de Estudio: {self.metadata.get('StudyType', 'N/A')}\\n\"\n",
    "            f\"Paises donde se desarrolla el estudio: {self.metadata.get('Countries', 'N/A')}\\n\"\n",
    "            f\"Fase en que se encuentra el estudio: {self.metadata.get('Phases', 'N/A')}\\n\"\n",
    "            f\"Identificación en ClinicaTrial: {self.metadata.get('IDestudio', 'N/A')}.\\n\\n\"\n",
    "        )\n",
    "\n",
    "class HNSWIndex:\n",
    "    def __init__(self, embeddings, metadata=None, space='cosine', ef_construction=200, M=16):\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = hnswlib.Index(space=space, dim=self.dimension)\n",
    "        self.index.init_index(max_elements=embeddings.shape[0], ef_construction=ef_construction, M=M)\n",
    "        self.index.add_items(embeddings, np.arange(embeddings.shape[0]))\n",
    "        self.index.set_ef(50) \n",
    "        self.metadata = metadata or []\n",
    "    \n",
    "    def similarity_search(self, query_vector, k=5):\n",
    "        labels, distances = self.index.knn_query(query_vector, k=k)\n",
    "        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(labels[0])]\n",
    "\n",
    "# (2) Validación de Datos en Documentos JSON en desdobla_doc\n",
    "def desdobla_doc(data2):\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    contenido = data2['content']\n",
    "    \n",
    "    if isinstance(contenido, list):\n",
    "        for entry in contenido:\n",
    "            if isinstance(entry, dict):\n",
    "                # Validar campos esperados\n",
    "                nctId = entry.get(\"IDestudio\", \"\")\n",
    "                briefTitle = entry.get(\"Title\", \"\")\n",
    "                summary = entry.get(\"Summary\", \"\")\n",
    "                studyType = entry.get(\"StudyType\", \"\")\n",
    "                country = entry.get(\"Countries\", \"\")\n",
    "                overallStatus = entry.get(\"OverallStatus\", \"\")\n",
    "                conditions = entry.get(\"Conditions\", \"\")\n",
    "                phases = entry.get(\"Phases\", \"\")\n",
    "\n",
    "                # Si algún campo crítico está vacío, loggear advertencia\n",
    "                if not briefTitle or not summary:\n",
    "                    logging.warning(f\"Entrada JSON con campos faltantes. Se utilizan valores por defecto. ID: {nctId}\")\n",
    "                \n",
    "                Summary = (\n",
    "                    f\"The study titled '{briefTitle}', of type '{studyType}', \"\n",
    "                    f\"investigates the condition(s): {conditions}. \"\n",
    "                    f\"Brief summary: {summary}. \"\n",
    "                    f\"Current status: {overallStatus}, taking place in {country}. \"\n",
    "                    f\"The study is classified under: {phases} phase. \"\n",
    "                    f\"For more info, search {nctId} on ClinicalTrials.\"\n",
    "                )\n",
    "                metadata = {\n",
    "                    \"Title\": briefTitle if briefTitle else \"N/A\",\n",
    "                    \"Summary\": Summary if summary else \"No summary available\",\n",
    "                    \"StudyType\": studyType if studyType else \"N/A\",\n",
    "                    \"Countries\": country if country else \"N/A\",\n",
    "                    \"Phases\": phases if phases else \"N/A\",\n",
    "                    \"IDestudio\": nctId if nctId else \"N/A\"\n",
    "                }\n",
    "                doc = Document(Summary, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(Summary)\n",
    "            else:\n",
    "                # Si no es dict, manejar como texto genérico\n",
    "                texto = str(entry)\n",
    "                metadata = {\"Summary\": texto}\n",
    "                documents.append(Document(texto, metadata))\n",
    "                summaries.append(texto)\n",
    "    else:\n",
    "        texto = str(contenido)\n",
    "        metadata = {\"Summary\": texto}\n",
    "        documents.append(Document(texto, metadata))\n",
    "        summaries.append(texto)\n",
    "\n",
    "    if documents:\n",
    "        embeddings = model.encode([doc.page_content for doc in documents], show_progress_bar=False)\n",
    "        embeddings = np.array(embeddings).astype(np.float32)\n",
    "        vector_store = HNSWIndex(embeddings, metadata=[doc.metadata for doc in documents])\n",
    "    else:\n",
    "        vector_store = None\n",
    "\n",
    "    return documents, vector_store\n",
    "\n",
    "trozos_archivos = []\n",
    "index_archivos = []\n",
    "for i in range(len(documentos)):\n",
    "    trozos, index = desdobla_doc(documentos[i])\n",
    "    trozos_archivos.append(trozos)\n",
    "    index_archivos.append(index)\n",
    "\n",
    "logging.info(\"Índices HNSWlib creados para todos los documentos.\")\n",
    "\n",
    "\n",
    "# Precomputation de embeddings de nombres\n",
    "archivos = [doc['filename'] for doc in documentos]\n",
    "archivos_embeddings = model.encode(archivos)\n",
    "logging.info(\"Embeddings de nombres de archivos precalculados.\")\n",
    "\n",
    "\n",
    "def doc_enfermedad(pregunta):\n",
    "    if not documentos:\n",
    "        logging.warning(\"No se encontraron documentos. Índice por defecto: 0.\")\n",
    "        return 0\n",
    "    preg_embedding = model.encode(pregunta)\n",
    "    similarities = [util.cos_sim(preg_embedding, emb.reshape(1, -1)).item() for emb in archivos_embeddings]\n",
    "    max_index = similarities.index(max(similarities))\n",
    "    return max_index\n",
    "\n",
    "\n",
    "# (4) Manejo de Idioma Dinámico: función para detectar idioma básico\n",
    "def detectar_idioma(texto):\n",
    "    # Heurística simple: si contiene palabras muy comunes del español, asumimos español.\n",
    "    # De lo contrario, asumimos inglés. Esto es muy rudimentario.\n",
    "    palabras_es = [\" el \", \" la \", \" de \", \" en \", \" y \", \" un \", \" una \", \" los \", \" las \"]\n",
    "    texto_lower = \" \" + texto.lower() + \" \"\n",
    "    score_es = sum(word in texto_lower for word in palabras_es)\n",
    "    # Si score_es > cierto umbral, asumimos español\n",
    "    if score_es > 2:\n",
    "        return \"es\"\n",
    "    else:\n",
    "        return \"en\"\n",
    "\n",
    "translation_cache = {}\n",
    "embedding_cache = {}\n",
    "\n",
    "def traducir(texto, idioma_destino):\n",
    "    idioma_origen = detectar_idioma(texto)\n",
    "    # Si el origen y el destino son iguales, no traducir\n",
    "    if (idioma_origen == \"es\" and idioma_destino == \"español\") or (idioma_origen == \"en\" and idioma_destino == \"inglés\"):\n",
    "        logging.info(\"Texto ya en el idioma destino, no se traduce.\")\n",
    "        return texto\n",
    "\n",
    "    cache_key = (texto, idioma_destino)\n",
    "    if cache_key in translation_cache:\n",
    "        logging.info(\"Traducción obtenida de caché.\")\n",
    "        return translation_cache[cache_key]\n",
    "\n",
    "    start_time = time.time()\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Actúa como un traductor.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Por favor, traduce este texto al {idioma_destino}: {texto}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Traducción completada en {elapsed_time:.2f} segundos.\")\n",
    "        translation_cache[cache_key] = respuesta.message.content.strip()\n",
    "        return respuesta.message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al traducir: {e}\")\n",
    "        return texto\n",
    "\n",
    "def generate_embedding(texto):\n",
    "    if texto in embedding_cache:\n",
    "        logging.info(\"Embedding obtenido de caché.\")\n",
    "        return embedding_cache[texto]\n",
    "    try:\n",
    "        embedding = model.encode([texto])\n",
    "        logging.info(f\"Embedding generado para el texto: {texto}\")\n",
    "        embedding_cache[texto] = embedding\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar el embedding: {e}\")\n",
    "        return np.zeros((1, 384))\n",
    "\n",
    "def obtener_contexto(pregunta, index, trozos, top_k=50):\n",
    "    try:\n",
    "        pregunta_en_ingles = traducir(pregunta, \"inglés\")\n",
    "        logging.info(f\"Pregunta traducida al inglés: {pregunta_en_ingles}\")\n",
    "        pregunta_emb = generate_embedding(pregunta_en_ingles)\n",
    "        logging.info(\"Embedding generado para la pregunta.\")\n",
    "\n",
    "        results = index.similarity_search(pregunta_emb, k=top_k)\n",
    "        texto = \"\"\n",
    "        for entry in results:\n",
    "            resum = entry[0][\"Summary\"]\n",
    "            texto += resum + \"\\n\"\n",
    "\n",
    "        logging.info(\"Contexto relevante recuperado para la pregunta.\")\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al obtener el contexto: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# (3) Mejoras en la Categorización de Preguntas\n",
    "def categorizar_pregunta(pregunta):\n",
    "    # Se amplían las palabras clave y se agregan sinónimos en español e inglés\n",
    "    categorias = {\n",
    "        \"tratamiento\": [\"tratamiento\", \"medicación\", \"cura\", \"terapia\", \"fármaco\", \"treatment\", \"drug\", \"therapy\"],\n",
    "        \"ensayo\": [\"ensayo\", \"estudio\", \"prueba\", \"investigación\", \"trial\", \"study\", \"clinical trial\", \"research\"],\n",
    "        \"resultado\": [\"resultado\", \"efectividad\", \"resultados\", \"éxito\", \"fracaso\", \"outcome\", \"result\", \"efficacy\"],\n",
    "        \"prevención\": [\"prevención\", \"previene\", \"evitar\", \"reducción de riesgo\", \"prevent\", \"avoid\", \"risk reduction\"]\n",
    "    }\n",
    "\n",
    "    pregunta_lower = pregunta.lower()\n",
    "    # Si encuentra una coincidencia en cualquier categoría, la asigna\n",
    "    for categoria, palabras in categorias.items():\n",
    "        for palabra in palabras:\n",
    "            if palabra in pregunta_lower:\n",
    "                return categoria\n",
    "\n",
    "    return \"general\"\n",
    "\n",
    "\n",
    "def generar_prompt(categoria, pregunta):\n",
    "    prompts = {\n",
    "        \"tratamiento\": f\"Proporciona información sobre tratamientos en ensayos clínicos relacionados con: {pregunta}.\",\n",
    "        \"ensayo\": f\"Describe los ensayos clínicos actuales relacionados con: {pregunta}.\",\n",
    "        \"resultado\": f\"Explica los resultados más recientes de ensayos clínicos sobre: {pregunta}.\",\n",
    "        \"prevención\": f\"Ofrece información sobre prevención y ensayos clínicos para: {pregunta}.\"\n",
    "    }\n",
    "    return prompts.get(categoria, \"Por favor, responde la pregunta sobre ensayos clínicos.\")\n",
    "\n",
    "def es_saludo(pregunta):\n",
    "    saludos = [\"hola\", \"buen día\", \"buenas\", \"cómo estás\", \"cómo te llamas\", \"qué tal\", \"estás bien\", \"buenas tardes\", \"buenas noches\"]\n",
    "    return any(saludo in pregunta.lower() for saludo in saludos)\n",
    "\n",
    "def responder_saludo():\n",
    "    saludos_respuestas = [\n",
    "        \"¡Hola! Estoy para ayudarte con información sobre ensayos clínicos. ¿En qué puedo asistirte hoy?\",\n",
    "        \"¡Buenas! Tenés alguna pregunta sobre ensayos clínicos en enfermedades neuromusculares?\",\n",
    "        \"¡Hola! ¿Cómo puedo ayudarte con tus consultas sobre ensayos clínicos?\"\n",
    "    ]\n",
    "    import random\n",
    "    return random.choice(saludos_respuestas)\n",
    "\n",
    "def generar_respuesta(pregunta, contexto, prompt_especifico):\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Eres un experto médico.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"{prompt_especifico}\\nContexto: {contexto}\\nPregunta: {pregunta}\")\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Respuesta generada en inglés en {elapsed_time:.2f} segundos.\")\n",
    "        respuesta_en_espanol = traducir(respuesta.message.content, \"español\")\n",
    "        logging.info(\"Respuesta traducida al español.\")\n",
    "        return respuesta_en_espanol\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar la respuesta: {e}\")\n",
    "        return \"Lo siento, ocurrió un error al generar la respuesta.\"\n",
    "\n",
    "def generar_hash(pregunta):\n",
    "    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()\n",
    "\n",
    "def obtener_respuesta_cacheada(pregunta):\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    if os.path.exists(archivo_cache):\n",
    "        try:\n",
    "            with open(archivo_cache, \"r\", encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                return datos.get(\"respuesta\", None)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al leer el caché: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def guardar_respuesta_cacheada(pregunta, respuesta):\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)\n",
    "        with open(archivo_cache, \"w\", encoding='utf-8') as f:\n",
    "            json.dump({\"pregunta\": pregunta, \"respuesta\": respuesta}, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Respuesta cacheada para la pregunta: '{pregunta}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar la respuesta en caché: {e}\")\n",
    "\n",
    "def responder_pregunta(pregunta, index, trozos):\n",
    "    try:\n",
    "        if index is None or not trozos:\n",
    "            logging.warning(\"No se encontraron índices o trozos para esta pregunta.\")\n",
    "            return \"No se encontró información para responder tu pregunta.\"\n",
    "\n",
    "        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)\n",
    "        if respuesta_cacheada:\n",
    "            logging.info(f\"Respuesta obtenida del caché para: '{pregunta}'\")\n",
    "            return respuesta_cacheada\n",
    "\n",
    "        categoria = categorizar_pregunta(pregunta)\n",
    "        logging.info(f\"Categoría de la pregunta: {categoria}\")\n",
    "        prompt_especifico = generar_prompt(categoria, pregunta)\n",
    "        logging.info(f\"Prompt específico: {prompt_especifico}\")\n",
    "\n",
    "        contexto = obtener_contexto(pregunta, index, trozos)\n",
    "        if not contexto.strip():\n",
    "            logging.warning(\"No se encontró contexto relevante.\")\n",
    "            respuesta = \"No pude encontrar información relevante para responder tu pregunta.\"\n",
    "            guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "            return respuesta\n",
    "\n",
    "        respuesta = generar_respuesta(pregunta, contexto, prompt_especifico)\n",
    "        guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso de responder pregunta: {e}\")\n",
    "        return \"Ocurrió un error al procesar tu pregunta.\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    \n",
    "    if len(documentos) == 0:\n",
    "        print(\"No se cargaron documentos. Por favor, verifica el directorio 'data'.\")\n",
    "        logging.error(\"No se encontraron documentos. Finalizando.\")\n",
    "    else:\n",
    "        print(\"Bienvenido al Chatbot de Ensayos Clínicos\")\n",
    "        print(\"Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\")\n",
    "        print(\"Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\")\n",
    "        while True:\n",
    "            pregunta = input(\"Tu pregunta: \").strip()\n",
    "            if pregunta.lower() in ['salir', 'chau', 'exit', 'quit']:\n",
    "                print(\"¡Chau!\")\n",
    "                logging.info(\"El usuario ha finalizado la sesión.\")\n",
    "                break\n",
    "\n",
    "            if es_saludo(pregunta):\n",
    "                respuesta_saludo = responder_saludo()\n",
    "                print(respuesta_saludo)\n",
    "                logging.info(\"Se detectó un saludo.\")\n",
    "                continue\n",
    "            \n",
    "            idn = doc_enfermedad(pregunta)\n",
    "            index = index_archivos[idn] if idn < len(index_archivos) else None\n",
    "            trozos = trozos_archivos[idn] if idn < len(trozos_archivos) else []\n",
    "\n",
    "            respuesta = responder_pregunta(pregunta, index, trozos)\n",
    "            print(f\"Respuesta: {respuesta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al Chatbot de Ensayos Clínicos\n",
      "Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\n",
      "Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.49it/s]\n",
      "2024-12-07 00:07:35,932 - INFO - Categoría de la pregunta: ensayo\n",
      "2024-12-07 00:07:35,933 - INFO - Prompt específico: Describe los ensayos clínicos actuales relacionados con: ¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?.\n",
      "2024-12-07 00:07:35,934 - INFO - Texto ya en el idioma destino, no se traduce.\n",
      "2024-12-07 00:07:35,934 - INFO - Pregunta traducida al inglés: ¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?\n",
      "2024-12-07 00:07:35,935 - INFO - Embedding obtenido de caché.\n",
      "2024-12-07 00:07:35,936 - INFO - Embedding generado para la pregunta.\n",
      "2024-12-07 00:07:35,938 - INFO - Contexto relevante recuperado para la pregunta.\n",
      "2024-12-07 00:07:37,980 - INFO - Respuesta generada en inglés en 2.04 segundos.\n",
      "2024-12-07 00:07:37,981 - INFO - Texto ya en el idioma destino, no se traduce.\n",
      "2024-12-07 00:07:37,982 - INFO - Respuesta traducida al español.\n",
      "2024-12-07 00:07:37,984 - INFO - Respuesta cacheada para la pregunta: '¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: Basándome en la información proporcionada, hay **dos** ensayos clínicos activos actualmente para la Distrofia Muscular de Duchenne:\n",
      "\n",
      "1. **NCT05712447:**  \"Duchenne Muscular Dystrophy Video Assessment Registry\"  (ACTIVO_NO_RECLUTANDO)\n",
      "2. **NCT04587908:** \"A Phase 3 Study of TAS-205 in Patients With Duchenne Muscular Dystrophy（REACH-DMD）\" (ACTIVO_NO_RECLUTANDO)\n",
      "\n",
      "\n",
      "Es importante destacar que esta respuesta se basa únicamente en los datos proporcionados.  El número real de ensayos clínicos activos para la Distrofia Muscular de Duchenne en todo el mundo es significativamente mayor y se puede encontrar información más completa en bases de datos como ClinicalTrials.gov realizando una búsqueda específica.  La información aquí presentada representa solo una pequeña muestra.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:08:00,093 - INFO - El usuario ha finalizado la sesión.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Chau!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    \n",
    "    if len(documentos) == 0:\n",
    "        print(\"No se cargaron documentos. Por favor, verifica el directorio 'data'.\")\n",
    "        logging.error(\"No se encontraron documentos. Finalizando.\")\n",
    "    else:\n",
    "        print(\"Bienvenido al Chatbot de Ensayos Clínicos\")\n",
    "        print(\"Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\")\n",
    "        print(\"Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\")\n",
    "        while True:\n",
    "            pregunta = input(\"Tu pregunta: \").strip()\n",
    "            if pregunta.lower() in ['salir', 'chau', 'exit', 'quit']:\n",
    "                print(\"¡Chau!\")\n",
    "                logging.info(\"El usuario ha finalizado la sesión.\")\n",
    "                break\n",
    "\n",
    "            if es_saludo(pregunta):\n",
    "                respuesta_saludo = responder_saludo()\n",
    "                print(respuesta_saludo)\n",
    "                logging.info(\"Se detectó un saludo.\")\n",
    "                continue\n",
    "            \n",
    "            idn = doc_enfermedad(pregunta)\n",
    "            index = index_archivos[idn] if idn < len(index_archivos) else None\n",
    "            trozos = trozos_archivos[idn] if idn < len(trozos_archivos) else []\n",
    "\n",
    "            respuesta = responder_pregunta(pregunta, index, trozos)\n",
    "            print(f\"Respuesta: {respuesta}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ### **PASO 1: Verificar versión de Python**\n",
    "# Se valida que la versión de Python sea la requerida.\n",
    "#\n",
    "# - **Líneas Clave:**\n",
    "#   - Importación de librerías para verificar la versión (`sys`, `os`).\n",
    "#   - Configuración de logging para advertencias y mensajes informativos.\n",
    "#   - Comparación de la versión actual con la requerida.\n",
    "# - **Oportunidad de Mejora:** Agregar soporte para versiones cercanas si no es crítica la compatibilidad exacta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:10:21,731 - INFO - \n",
      "    **********************************************\n",
      "    ** Versión de Python compatible **\n",
      "    **********************************************\n",
      "    Python 3.10.12 detectado correctamente.\n",
      "    Todas las funcionalidades deberían operar sin problemas.\n",
      "    **********************************************\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Desactivar advertencias de paralelización en tokenizadores\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "# Versión requerida\n",
    "REQUIRED_VERSION = (3, 10, 12)\n",
    "current_version = sys.version_info\n",
    "\n",
    "# Validar compatibilidad de versión\n",
    "if (current_version.major, current_version.minor, current_version.micro) != REQUIRED_VERSION:\n",
    "    logging.warning(f\"\"\"\n",
    "    **********************************************\n",
    "    ** Advertencia: Versión de Python no compatible **\n",
    "    **********************************************\n",
    "    Este chatbot está optimizado para Python {REQUIRED_VERSION[0]}.{REQUIRED_VERSION[1]}.{REQUIRED_VERSION[2]}.\n",
    "    La versión actual es Python {current_version.major}.{current_version.minor}.{current_version.micro}.\n",
    "    Algunas funcionalidades pueden no funcionar correctamente.\n",
    "    **********************************************\n",
    "    \"\"\")\n",
    "else:\n",
    "    logging.info(\"\"\"\n",
    "    **********************************************\n",
    "    ** Versión de Python compatible **\n",
    "    **********************************************\n",
    "    Python 3.10.12 detectado correctamente.\n",
    "    Todas las funcionalidades deberían operar sin problemas.\n",
    "    **********************************************\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Instalar las bibliotecas necesarias desde el archivo requirements.txt\n",
    "# Asegúrate de tener un archivo 'requirements.txt' en el directorio raíz con todas las dependencias listadas\n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:10:44,568 - INFO - Librerías importadas correctamente.\n",
      "2024-12-07 00:10:44,577 - INFO - Variables de entorno cargadas desde el archivo .env.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import hnswlib\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "# Configuración de logs para imprimir todo en consola\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Mensaje de confirmación de importación\n",
    "logging.info(\"Librerías importadas correctamente.\")\n",
    "\n",
    "# Cargar variables de entorno desde un archivo .env\n",
    "load_dotenv()\n",
    "logging.info(\"Variables de entorno cargadas desde el archivo .env.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:10:52,380 - INFO - Iniciando carga desde el directorio: data.\n",
      "2024-12-07 00:10:52,401 - INFO - Archivo 'resumenes_lupus.json' cargado correctamente.\n",
      "2024-12-07 00:10:52,405 - INFO - Archivo 'resumenes_duchenne.json' cargado correctamente.\n",
      "2024-12-07 00:10:52,407 - INFO - Archivo 'resumenes_pompe.json' cargado correctamente.\n",
      "2024-12-07 00:10:52,410 - INFO - Archivo 'resumenes_becker.json' cargado correctamente.\n",
      "2024-12-07 00:10:52,414 - INFO - Archivo 'resumenes_glycogen storage disease.json' cargado correctamente.\n",
      "2024-12-07 00:10:52,417 - INFO - Archivo 'resumenes_myotonic dystrophy.json' cargado correctamente.\n",
      "2024-12-07 00:10:52,418 - INFO - 6 documentos cargados.\n",
      "2024-12-07 00:10:52,424 - INFO - Se cargaron 6 documentos exitosamente.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def load_documents(source, is_directory=False):\n",
    "    \"\"\"\n",
    "    Carga documentos desde un archivo o directorio.\n",
    "    \n",
    "    Args:\n",
    "        source (str): Ruta al archivo o directorio.\n",
    "        is_directory (bool): Indica si la fuente es un directorio.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de diccionarios con 'filename' y 'content'.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source):\n",
    "        logging.error(f\"La fuente '{source}' no existe.\")\n",
    "        raise FileNotFoundError(f\"La fuente '{source}' no se encontró.\")\n",
    "\n",
    "    loaded_files = []\n",
    "    if is_directory:\n",
    "        logging.info(f\"Iniciando carga desde el directorio: {source}.\")\n",
    "        for filename in os.listdir(source):\n",
    "            filepath = os.path.join(source, filename)\n",
    "            if os.path.isfile(filepath) and filepath.endswith(('.txt', '.json', '.pdf')):\n",
    "                content = extract_content(filepath)\n",
    "                if content:\n",
    "                    loaded_files.append({\"filename\": filename, \"content\": content})\n",
    "                    logging.info(f\"Archivo '{filename}' cargado correctamente.\")\n",
    "    else:\n",
    "        logging.info(f\"Iniciando carga del archivo: {source}.\")\n",
    "        content = extract_content(source)\n",
    "        if content:\n",
    "            loaded_files.append({\"filename\": os.path.basename(source), \"content\": content})\n",
    "            logging.info(f\"Archivo '{os.path.basename(source)}' cargado correctamente.\")\n",
    "\n",
    "    logging.info(f\"{len(loaded_files)} documentos cargados.\")\n",
    "    return loaded_files\n",
    "\n",
    "def extract_content(filepath):\n",
    "    \"\"\"\n",
    "    Extrae el contenido del archivo según su tipo.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Ruta al archivo.\n",
    "    \n",
    "    Returns:\n",
    "        list o dict o str: Contenido procesado del archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.txt'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            units = content.split(\"\\n-----\\n\")\n",
    "            return units\n",
    "        elif filepath.endswith('.json'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            return data\n",
    "        elif filepath.endswith('.pdf'):\n",
    "            reader = PdfReader(filepath)\n",
    "            return ''.join(page.extract_text() or '' for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al extraer contenido de '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuración de ruta y carga de documentos\n",
    "ruta_fuente = 'data'  # Asegúrate de tener una carpeta 'data' con los documentos\n",
    "documentos = load_documents(ruta_fuente, is_directory=True)\n",
    "logging.info(f\"Se cargaron {len(documentos)} documentos exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:11:02,677 - INFO - Gemini configurado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "gemini_llm = None\n",
    "\n",
    "def configure_gemini():\n",
    "    \"\"\"\n",
    "    Configura la instancia de Gemini usando la clave API.\n",
    "    \n",
    "    Returns:\n",
    "        Gemini: Instancia configurada del modelo Gemini.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        logging.error(\"La clave API de Gemini no está configurada.\")\n",
    "        raise EnvironmentError(\"Configura GEMINI_API_KEY en tu archivo .env.\")\n",
    "    gemini = Gemini(api_key=api_key)\n",
    "    logging.info(\"Gemini configurado correctamente.\")\n",
    "    return gemini\n",
    "\n",
    "gemini_llm = configure_gemini()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:11:07,928 - INFO - Use pytorch device_name: mps\n",
      "2024-12-07 00:11:07,929 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Precomputar los embeddings de los nombres de archivo para eficiencia\n",
    "archivos = [doc['filename'] for doc in documentos]\n",
    "archivos_embeddings = model.encode(archivos)\n",
    "\n",
    "def doc_enfermedad(pregunta):\n",
    "    \"\"\"\n",
    "    Identifica el índice del documento más relevante para la enfermedad en la pregunta.\n",
    "    Utiliza embeddings precomputados de los nombres de archivo.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        int: Índice del documento más relevante.\n",
    "    \"\"\"\n",
    "    if not documentos:\n",
    "        logging.warning(\"No se encontraron documentos. Índice por defecto: 0.\")\n",
    "        return 0\n",
    "\n",
    "    # Generar embedding de la pregunta\n",
    "    preg_embedding = model.encode(pregunta)\n",
    "\n",
    "    # Calcular similitudes con los embeddings de los nombres de archivo\n",
    "    similarities = [util.cos_sim(preg_embedding, emb).item() for emb in archivos_embeddings]\n",
    "\n",
    "    # Obtener el índice con mayor similitud\n",
    "    max_index = similarities.index(max(similarities))\n",
    "    return max_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        \"\"\"\n",
    "        Inicializa un documento con su contenido y metadatos.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Texto del documento.\n",
    "            metadata (dict, optional): Metadatos asociados al documento.\n",
    "        \"\"\"\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Representación en string del documento.\n",
    "        \n",
    "        Returns:\n",
    "            str: Información formateada del documento.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            f\"Título: {self.metadata.get('Title', 'N/A')}\\n\"\n",
    "            f\"Resumen: {self.metadata.get('Summary', 'N/A')}\\n\"\n",
    "            f\"Tipo de Estudio: {self.metadata.get('StudyType', 'N/A')}\\n\"\n",
    "            f\"Paises donde se desarrolla el estudio: {self.metadata.get('Countries', 'N/A')}\\n\"\n",
    "            f\"Fase en que se encuentra el estudio: {self.metadata.get('Phases', 'N/A')}\\n\"\n",
    "            f\"Identificación en ClinicaTrial: {self.metadata.get('IDestudio', 'N/A')}.\\n\\n\"\n",
    "        )\n",
    "\n",
    "class HNSWIndex:\n",
    "    def __init__(self, embeddings, metadata=None, space='cosine', ef_construction=200, M=16):\n",
    "        \"\"\"\n",
    "        Inicializa el índice HNSWlib con los embeddings proporcionados.\n",
    "        \n",
    "        Args:\n",
    "            embeddings (np.ndarray): Matriz de embeddings.\n",
    "            metadata (list, optional): Lista de metadatos asociados a cada embedding.\n",
    "            space (str, optional): Espacio métrico para HNSWlib.\n",
    "            ef_construction (int, optional): Parámetro ef para la construcción del índice.\n",
    "            M (int, optional): Parámetro M para HNSWlib.\n",
    "        \"\"\"\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = hnswlib.Index(space=space, dim=self.dimension)\n",
    "        self.index.init_index(max_elements=embeddings.shape[0], ef_construction=ef_construction, M=M)\n",
    "        self.index.add_items(embeddings, np.arange(embeddings.shape[0]))\n",
    "        self.index.set_ef(50)  # Parámetro ef para consultas\n",
    "        self.metadata = metadata or []\n",
    "    \n",
    "    def similarity_search(self, query_vector, k=5):\n",
    "        \"\"\"\n",
    "        Realiza una búsqueda de los k vecinos más similares.\n",
    "        \n",
    "        Args:\n",
    "            query_vector (np.ndarray): Vector de consulta.\n",
    "            k (int, optional): Número de vecinos a buscar.\n",
    "        \n",
    "        Returns:\n",
    "            list: Lista de tuplas con metadatos y distancias.\n",
    "        \"\"\"\n",
    "        labels, distances = self.index.knn_query(query_vector, k=k)\n",
    "        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(labels[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:13:45,751 - INFO - Índices HNSWlib creados para todos los documentos.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def desdobla_doc(data2):\n",
    "    \"\"\"\n",
    "    Desdobla el contenido del documento en varios `Document` con metadatos.\n",
    "    Maneja JSON (asumiendo estructura de ensayos clínicos) o texto/PDF genérico.\n",
    "    \n",
    "    Args:\n",
    "        data2 (dict): Diccionario con 'filename' y 'content'.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Lista de `Document` y instancia de `HNSWIndex`.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    contenido = data2['content']\n",
    "    \n",
    "    if isinstance(contenido, list):\n",
    "        for entry in contenido:\n",
    "            if isinstance(entry, dict):\n",
    "                nctId = entry.get(\"IDestudio\", \"\")\n",
    "                briefTitle = entry.get(\"Title\", \"\")\n",
    "                summary = entry.get(\"Summary\", \"\")\n",
    "                studyType = entry.get(\"StudyType\", \"\")\n",
    "                country = entry.get(\"Countries\", \"\")\n",
    "                overallStatus = entry.get(\"OverallStatus\", \"\")\n",
    "                conditions = entry.get(\"Conditions\", \"\")\n",
    "                phases = entry.get(\"Phases\", \"\")\n",
    "\n",
    "                # Crear resumen en inglés para consistencia interna\n",
    "                Summary = (\n",
    "                    f\"The study titled '{briefTitle}', of type '{studyType}', \"\n",
    "                    f\"investigates the condition(s): {conditions}. \"\n",
    "                    f\"Brief summary: {summary}. \"\n",
    "                    f\"Current status: {overallStatus}, taking place in {country}. \"\n",
    "                    f\"The study is classified under: {phases} phase. \"\n",
    "                    f\"For more info, search {nctId} on ClinicalTrials.\"\n",
    "                )\n",
    "                metadata = {\n",
    "                    \"Title\": briefTitle,\n",
    "                    \"Summary\": Summary,\n",
    "                    \"StudyType\": studyType,\n",
    "                    \"Countries\": country,\n",
    "                    \"Phases\": phases,\n",
    "                    \"IDestudio\": nctId\n",
    "                }\n",
    "                doc = Document(Summary, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(Summary)\n",
    "            else:\n",
    "                # Si no es dict, tratar la entrada como texto genérico\n",
    "                texto = str(entry)\n",
    "                metadata = {\"Summary\": texto}\n",
    "                doc = Document(texto, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(texto)\n",
    "    else:\n",
    "        # Texto genérico (PDF o TXT)\n",
    "        texto = str(contenido)\n",
    "        metadata = {\"Summary\": texto}\n",
    "        doc = Document(texto, metadata)\n",
    "        documents.append(doc)\n",
    "        summaries.append(texto)\n",
    "\n",
    "    if documents:\n",
    "        embeddings = model.encode([doc.page_content for doc in documents], show_progress_bar=False)\n",
    "        embeddings = np.array(embeddings).astype(np.float32)\n",
    "        vector_store = HNSWIndex(embeddings, metadata=[doc.metadata for doc in documents])\n",
    "    else:\n",
    "        vector_store = None\n",
    "\n",
    "    return documents, vector_store\n",
    "\n",
    "# Procesar todos los documentos y crear sus respectivos índices\n",
    "trozos_archivos = []\n",
    "index_archivos = []\n",
    "for i in range(len(documentos)):\n",
    "    trozos, index = desdobla_doc(documentos[i])\n",
    "    trozos_archivos.append(trozos)\n",
    "    index_archivos.append(index)\n",
    "\n",
    "logging.info(\"Índices HNSWlib creados para todos los documentos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def traducir(texto, idioma_destino):\n",
    "    \"\"\"\n",
    "    Traduce texto al idioma especificado usando el modelo Gemini.\n",
    "    En caso de error, se devuelve el texto original.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto a traducir.\n",
    "        idioma_destino (str): Idioma de destino.\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto traducido o original en caso de fallo.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Actúa como un traductor.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Por favor, traduce este texto al {idioma_destino}: {texto}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Traducción completada en {elapsed_time:.2f} segundos.\")\n",
    "        return respuesta.message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al traducir: {e}\")\n",
    "        return texto  # fallback\n",
    "\n",
    "def generate_embedding(texto):\n",
    "    \"\"\"\n",
    "    Genera un embedding para el texto utilizando el modelo de embeddings.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto para generar el embedding.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Embedding generado o vector de ceros en caso de fallo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embedding = model.encode([texto])\n",
    "        logging.info(f\"Embedding generado para el texto: {texto}\")\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar el embedding: {e}\")\n",
    "        # Devuelve embedding vacío como fallback\n",
    "        return np.zeros((1, 384))\n",
    "    \n",
    "def obtener_contexto(pregunta, index, trozos, top_k=50):\n",
    "    \"\"\"\n",
    "    Recupera los trozos de texto más relevantes para responder la pregunta.\n",
    "    Traduce la pregunta al inglés antes de buscar en el índice.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        index (HNSWIndex): Índice de HNSWlib para buscar similitudes.\n",
    "        trozos (list): Lista de `Document` relacionados.\n",
    "        top_k (int, optional): Número de resultados a recuperar.\n",
    "    \n",
    "    Returns:\n",
    "        str: Contexto relevante concatenado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Traducir la pregunta al inglés\n",
    "        pregunta_en_ingles = traducir(pregunta, \"inglés\")\n",
    "        logging.info(f\"Pregunta traducida al inglés: {pregunta_en_ingles}\")\n",
    "\n",
    "        # Generar embedding de la pregunta traducida\n",
    "        pregunta_emb = generate_embedding(pregunta_en_ingles)\n",
    "        logging.info(\"Embedding generado para la pregunta.\")\n",
    "\n",
    "        # Buscar en el índice\n",
    "        results = index.similarity_search(pregunta_emb, k=top_k)\n",
    "        texto = \"\"\n",
    "        for entry in results:\n",
    "            resum = entry[0][\"Summary\"]\n",
    "            texto += resum + \"\\n\"\n",
    "\n",
    "        logging.info(\"Contexto relevante recuperado para la pregunta.\")\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al obtener el contexto: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def categorizar_pregunta(pregunta):\n",
    "    \"\"\"\n",
    "    Clasifica la pregunta en categorías basadas en palabras clave.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str: Categoría identificada.\n",
    "    \"\"\"\n",
    "    categorias = {\n",
    "        \"tratamiento\": [\"tratamiento\", \"medicación\", \"cura\", \"terapia\", \"fármaco\"],\n",
    "        \"ensayo\": [\"ensayo\", \"estudio\", \"prueba\", \"investigación\", \"trial\"],\n",
    "        \"resultado\": [\"resultado\", \"efectividad\", \"resultados\", \"éxito\", \"fracaso\"],\n",
    "        \"prevención\": [\"prevención\", \"previene\", \"evitar\", \"reducción de riesgo\"]\n",
    "    }\n",
    "    for categoria, palabras in categorias.items():\n",
    "        if any(palabra in pregunta.lower() for palabra in palabras):\n",
    "            return categoria\n",
    "    return \"general\"\n",
    "\n",
    "def generar_prompt(categoria, pregunta):\n",
    "    \"\"\"\n",
    "    Genera un prompt específico basado en la categoría de la pregunta.\n",
    "    \n",
    "    Args:\n",
    "        categoria (str): Categoría de la pregunta.\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str: Prompt generado.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"tratamiento\": f\"Proporciona información sobre tratamientos en ensayos clínicos relacionados con: {pregunta}.\",\n",
    "        \"ensayo\": f\"Describe los ensayos clínicos actuales relacionados con: {pregunta}.\",\n",
    "        \"resultado\": f\"Explica los resultados más recientes de ensayos clínicos sobre: {pregunta}.\",\n",
    "        \"prevención\": f\"Ofrece información sobre prevención y ensayos clínicos para: {pregunta}.\"\n",
    "    }\n",
    "    return prompts.get(categoria, \"Por favor, responde la pregunta sobre ensayos clínicos.\")\n",
    "\n",
    "def es_saludo(pregunta):\n",
    "    \"\"\"\n",
    "    Verifica si la pregunta del usuario es un saludo.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True si es un saludo, False de lo contrario.\n",
    "    \"\"\"\n",
    "    saludos = [\"hola\", \"buen día\", \"buenas\", \"cómo estás\", \"cómo te llamas\", \"qué tal\", \"estás bien\", \"buenas tardes\", \"buenas noches\"]\n",
    "    return any(saludo in pregunta.lower() for saludo in saludos)\n",
    "\n",
    "def responder_saludo():\n",
    "    \"\"\"\n",
    "    Genera una respuesta aleatoria a un saludo.\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta de saludo.\n",
    "    \"\"\"\n",
    "    saludos_respuestas = [\n",
    "        \"¡Hola! Estoy para ayudarte con información sobre ensayos clínicos. ¿En qué puedo asistirte hoy?\",\n",
    "        \"¡Buenas! Tenés alguna pregunta sobre ensayos clínicos en enfermedades neuromusculares?\",\n",
    "        \"¡Hola! ¿Cómo puedo ayudarte con tus consultas sobre ensayos clínicos?\"\n",
    "    ]\n",
    "    import random\n",
    "    return random.choice(saludos_respuestas)\n",
    "\n",
    "def generar_respuesta(pregunta, contexto, prompt_especifico):\n",
    "    \"\"\"\n",
    "    Genera una respuesta usando el contexto proporcionado y un prompt específico.\n",
    "    Primero genera la respuesta en inglés, luego la traduce al español.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        contexto (str): Contexto relevante recuperado.\n",
    "        prompt_especifico (str): Prompt adaptado a la categoría de la pregunta.\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta generada en español.\n",
    "    \"\"\"\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Eres un experto médico.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"{prompt_especifico}\\nContexto: {contexto}\\nPregunta: {pregunta}\")\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Respuesta generada en inglés en {elapsed_time:.2f} segundos.\")\n",
    "        # Traducir la respuesta al español\n",
    "        respuesta_en_espanol = traducir(respuesta.message.content, \"español\")\n",
    "        logging.info(\"Respuesta traducida al español.\")\n",
    "        return respuesta_en_espanol\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar la respuesta: {e}\")\n",
    "        return \"Lo siento, ocurrió un error al generar la respuesta.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def generar_hash(pregunta):\n",
    "    \"\"\"\n",
    "    Genera un hash SHA-256 para una pregunta dada.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str: Hash generado.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()\n",
    "\n",
    "def obtener_respuesta_cacheada(pregunta):\n",
    "    \"\"\"\n",
    "    Obtiene una respuesta cacheada para una pregunta si existe.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str o None: Respuesta cacheada o None si no existe.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    if os.path.exists(archivo_cache):\n",
    "        try:\n",
    "            with open(archivo_cache, \"r\", encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                return datos.get(\"respuesta\", None)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al leer el caché: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def guardar_respuesta_cacheada(pregunta, respuesta):\n",
    "    \"\"\"\n",
    "    Guarda una respuesta en caché para una pregunta dada.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        respuesta (str): Respuesta generada.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)\n",
    "        with open(archivo_cache, \"w\", encoding='utf-8') as f:\n",
    "            json.dump({\"pregunta\": pregunta, \"respuesta\": respuesta}, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Respuesta cacheada para la pregunta: '{pregunta}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar la respuesta en caché: {e}\")\n",
    "\n",
    "def responder_pregunta(pregunta, index, trozos):\n",
    "    \"\"\"\n",
    "    Integra categorización, obtención de contexto y generación de respuesta.\n",
    "    Incluye manejo de caché para respuestas repetidas.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        index (HNSWIndex): Índice de HNSWlib para búsqueda de contexto.\n",
    "        trozos (list): Lista de `Document` relacionados.\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta generada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if index is None or not trozos:\n",
    "            logging.warning(\"No se encontraron índices o trozos para esta pregunta.\")\n",
    "            return \"No se encontró información para responder tu pregunta.\"\n",
    "\n",
    "        # Verificar caché\n",
    "        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)\n",
    "        if respuesta_cacheada:\n",
    "            logging.info(f\"Respuesta obtenida del caché para: '{pregunta}'\")\n",
    "            return respuesta_cacheada\n",
    "\n",
    "        # Categorizar la pregunta\n",
    "        categoria = categorizar_pregunta(pregunta)\n",
    "        logging.info(f\"Categoría de la pregunta: {categoria}\")\n",
    "\n",
    "        # Generar prompt específico\n",
    "        prompt_especifico = generar_prompt(categoria, pregunta)\n",
    "        logging.info(f\"Prompt específico: {prompt_especifico}\")\n",
    "\n",
    "        # Obtener contexto relevante\n",
    "        contexto = obtener_contexto(pregunta, index, trozos)\n",
    "        if not contexto.strip():\n",
    "            logging.warning(\"No se encontró contexto relevante.\")\n",
    "            respuesta = \"No pude encontrar información relevante para responder tu pregunta.\"\n",
    "            guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "            return respuesta\n",
    "\n",
    "        # Generar la respuesta\n",
    "        respuesta = generar_respuesta(pregunta, contexto, prompt_especifico)\n",
    "\n",
    "        # Guardar la respuesta en caché\n",
    "        guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso de responder pregunta: {e}\")\n",
    "        return \"Ocurrió un error al procesar tu pregunta.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def generar_hash(pregunta):\n",
    "    \"\"\"\n",
    "    Genera un hash SHA-256 para una pregunta dada.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str: Hash generado.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()\n",
    "\n",
    "def obtener_respuesta_cacheada(pregunta):\n",
    "    \"\"\"\n",
    "    Obtiene una respuesta cacheada para una pregunta si existe.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str o None: Respuesta cacheada o None si no existe.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    if os.path.exists(archivo_cache):\n",
    "        try:\n",
    "            with open(archivo_cache, \"r\", encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                return datos.get(\"respuesta\", None)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al leer el caché: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def guardar_respuesta_cacheada(pregunta, respuesta):\n",
    "    \"\"\"\n",
    "    Guarda una respuesta en caché para una pregunta dada.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        respuesta (str): Respuesta generada.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)\n",
    "        with open(archivo_cache, \"w\", encoding='utf-8') as f:\n",
    "            json.dump({\"pregunta\": pregunta, \"respuesta\": respuesta}, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Respuesta cacheada para la pregunta: '{pregunta}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar la respuesta en caché: {e}\")\n",
    "\n",
    "def responder_pregunta(pregunta, index, trozos):\n",
    "    \"\"\"\n",
    "    Integra categorización, obtención de contexto y generación de respuesta.\n",
    "    Incluye manejo de caché para respuestas repetidas.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        index (HNSWIndex): Índice de HNSWlib para búsqueda de contexto.\n",
    "        trozos (list): Lista de `Document` relacionados.\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta generada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if index is None or not trozos:\n",
    "            logging.warning(\"No se encontraron índices o trozos para esta pregunta.\")\n",
    "            return \"No se encontró información para responder tu pregunta.\"\n",
    "\n",
    "        # Verificar caché\n",
    "        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)\n",
    "        if respuesta_cacheada:\n",
    "            logging.info(f\"Respuesta obtenida del caché para: '{pregunta}'\")\n",
    "            return respuesta_cacheada\n",
    "\n",
    "        # Categorizar la pregunta\n",
    "        categoria = categorizar_pregunta(pregunta)\n",
    "        logging.info(f\"Categoría de la pregunta: {categoria}\")\n",
    "\n",
    "        # Generar prompt específico\n",
    "        prompt_especifico = generar_prompt(categoria, pregunta)\n",
    "        logging.info(f\"Prompt específico: {prompt_especifico}\")\n",
    "\n",
    "        # Obtener contexto relevante\n",
    "        contexto = obtener_contexto(pregunta, index, trozos)\n",
    "        if not contexto.strip():\n",
    "            logging.warning(\"No se encontró contexto relevante.\")\n",
    "            respuesta = \"No pude encontrar información relevante para responder tu pregunta.\"\n",
    "            guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "            return respuesta\n",
    "\n",
    "        # Generar la respuesta\n",
    "        respuesta = generar_respuesta(pregunta, contexto, prompt_especifico)\n",
    "\n",
    "        # Guardar la respuesta en caché\n",
    "        guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso de responder pregunta: {e}\")\n",
    "        return \"Ocurrió un error al procesar tu pregunta.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al Chatbot de Ensayos Clínicos\n",
      "Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\n",
      "Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]\n",
      "2024-12-07 00:14:41,584 - INFO - Categoría de la pregunta: ensayo\n",
      "2024-12-07 00:14:41,585 - INFO - Prompt específico: Describe los ensayos clínicos actuales relacionados con: ¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?.\n",
      "2024-12-07 00:14:42,097 - INFO - Traducción completada en 0.51 segundos.\n",
      "2024-12-07 00:14:42,098 - INFO - Pregunta traducida al inglés: How many clinical trials are currently active for Duchenne Muscular Dystrophy?\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.54it/s]\n",
      "2024-12-07 00:14:42,198 - INFO - Embedding generado para el texto: How many clinical trials are currently active for Duchenne Muscular Dystrophy?\n",
      "2024-12-07 00:14:42,199 - INFO - Embedding generado para la pregunta.\n",
      "2024-12-07 00:14:42,200 - INFO - Contexto relevante recuperado para la pregunta.\n",
      "2024-12-07 00:14:47,660 - INFO - Respuesta generada en inglés en 5.46 segundos.\n",
      "2024-12-07 00:14:53,385 - INFO - Traducción completada en 5.72 segundos.\n",
      "2024-12-07 00:14:53,385 - INFO - Respuesta traducida al español.\n",
      "2024-12-07 00:14:53,387 - INFO - Respuesta cacheada para la pregunta: '¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: Basándome en la información proporcionada, hay **11 ensayos clínicos activos actualmente** para la Distrofia Muscular de Duchenne. Estos son:\n",
      "\n",
      "* **Registro de Duchenne (NCT02069756):** RECLUTANDO\n",
      "* **Estudio a largo plazo y de extensión de DS-5141b en pacientes con Distrofia Muscular de Duchenne (NCT04433234):** ACTIVO, NO RECLUTANDO\n",
      "* **Registro de evaluación por video de la Distrofia Muscular de Duchenne (NCT05712447):** ACTIVO, NO RECLUTANDO\n",
      "* **Estudio de SRP-4045 (Casimersen) y SRP-4053 (Golodirsen) en participantes con Distrofia Muscular de Duchenne (DMD) (NCT02500381):** ACTIVO, NO RECLUTANDO\n",
      "* **AFFINITY DUCHENNE: Terapia génica RGX-202 en participantes con Distrofia Muscular de Duchenne (DMD) (NCT05693142):** RECLUTANDO\n",
      "* **Un estudio de Fase 3 de TAS-205 en pacientes con Distrofia Muscular de Duchenne (REACH-DMD) (NCT04587908):** ACTIVO, NO RECLUTANDO\n",
      "* **Un estudio de la terapia génica SGT-003 en la Distrofia Muscular de Duchenne (INSPIRE DUCHENNE) (NCT06138639):** RECLUTANDO\n",
      "* **Un estudio observacional que compara Delandistrogene Moxeparvovec con el estándar de atención en participantes con Distrofia Muscular de Duchenne (NCT06270719):** RECLUTANDO\n",
      "* **Un estudio de CAP-1002 en pacientes ambulatorios y no ambulatorios con Distrofia Muscular de Duchenne (NCT05126758):** RECLUTANDO\n",
      "* **Hidroterapia en la Distrofia Muscular de Duchenne (DMD) (NCT06445985):** RECLUTANDO\n",
      "* **NS-089/NCNP-02-201 en niños con Distrofia Muscular de Duchenne (DMD) (NCT05996003):** RECLUTANDO\n",
      "* **Estudio de seguridad, tolerabilidad, farmacodinamia, eficacia y farmacocinética de DYNE-251 en participantes con Distrofia Muscular de Duchenne aptos para el salto del exón 51 (NCT05524883):** RECLUTANDO\n",
      "* **Un estudio de PGN-EDO51 en participantes con Distrofia Muscular de Duchenne aptos para el tratamiento de salto del exón 51 (NCT06079736):** RECLUTANDO\n",
      "* **Un estudio abierto para evaluar la eficacia y seguridad de Satralizumab en la Distrofia Muscular de Duchenne (NCT06450639):** RECLUTANDO\n",
      "\n",
      "\n",
      "Es importante notar que el estado de \"activo\" puede cambiar rápidamente. Esta respuesta refleja el estado en el momento del análisis de los datos proporcionados. Para obtener la información más actualizada, se recomienda consultar directamente la base de datos de ClinicalTrials.gov.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 00:14:56,189 - INFO - El usuario ha finalizado la sesión.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Chau!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    # Crear directorio de caché si no existe\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    \n",
    "    if len(documentos) == 0:\n",
    "        print(\"No se cargaron documentos. Por favor, verifica el directorio 'data'.\")\n",
    "        logging.error(\"No se encontraron documentos. Finalizando.\")\n",
    "    else:\n",
    "        print(\"Bienvenido al Chatbot de Ensayos Clínicos\")\n",
    "        print(\"Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\")\n",
    "        print(\"Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\")\n",
    "        while True:\n",
    "            pregunta = input(\"Tu pregunta: \").strip()\n",
    "            if pregunta.lower() in ['salir', 'chau', 'exit', 'quit']:\n",
    "                print(\"¡Chau!\")\n",
    "                logging.info(\"El usuario ha finalizado la sesión.\")\n",
    "                break\n",
    "            if es_saludo(pregunta):\n",
    "                respuesta_saludo = responder_saludo()\n",
    "                print(respuesta_saludo)\n",
    "                logging.info(\"Se detectó un saludo.\")\n",
    "                continue\n",
    "            \n",
    "            # Identificar la enfermedad (documento más relevante)\n",
    "            idn = doc_enfermedad(pregunta)\n",
    "            index = index_archivos[idn] if idn < len(index_archivos) else None\n",
    "            trozos = trozos_archivos[idn] if idn < len(trozos_archivos) else []\n",
    "\n",
    "            # Responder la pregunta\n",
    "            respuesta = responder_pregunta(pregunta, index, trozos)\n",
    "            print(f\"Respuesta: {respuesta}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 1: Verificar versión de Python**\n",
    "Se valida que la versión de Python sea la requerida.\n",
    "\n",
    "- **Líneas Clave:**\n",
    "  - Importación de librerías para verificar la versión (`sys`, `os`).\n",
    "  - Configuración de logging para advertencias y mensajes informativos.\n",
    "  - Comparación de la versión actual con la requerida.\n",
    "- **Oportunidad de Mejora:** Agregar soporte para versiones cercanas si no es crítica la compatibilidad exacta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Desactivar advertencias de paralelización en tokenizadores\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "# Versión requerida\n",
    "REQUIRED_VERSION = (3, 10, 12)\n",
    "current_version = sys.version_info\n",
    "\n",
    "# Validar compatibilidad de versión\n",
    "if (current_version.major, current_version.minor, current_version.micro) != REQUIRED_VERSION:\n",
    "    logging.warning(f\"\"\"\n",
    "    **********************************************\n",
    "    ** Advertencia: Versión de Python no compatible **\n",
    "    **********************************************\n",
    "    Este chatbot está optimizado para Python {REQUIRED_VERSION[0]}.{REQUIRED_VERSION[1]}.{REQUIRED_VERSION[2]}.\n",
    "    La versión actual es Python {current_version.major}.{current_version.minor}.{current_version.micro}.\n",
    "    Algunas funcionalidades pueden no funcionar correctamente.\n",
    "    **********************************************\n",
    "    \"\"\")\n",
    "else:\n",
    "    logging.info(\"\"\"\n",
    "    **********************************************\n",
    "    ** Versión de Python compatible **\n",
    "    **********************************************\n",
    "    Python 3.10.12 detectado correctamente.\n",
    "    Todas las funcionalidades deberían operar sin problemas.\n",
    "    **********************************************\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 2: Instalación de Paquetes Necesarios**\n",
    "Se listan las bibliotecas requeridas para el funcionamiento del chatbot.\n",
    "\n",
    "- **Líneas Clave:**\n",
    "  - Uso de `requirements.txt` para instalar dependencias.\n",
    "  - Listado de las principales bibliotecas usadas.\n",
    "- **Oportunidad de Mejora:** Implementar una verificación automática de instalación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar las bibliotecas necesarias desde el archivo requirements.txt\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 3: Importar Librerías y Configurar Logging**\n",
    "Importa todas las librerías necesarias y configura un sistema de logs.\n",
    "\n",
    "- **Líneas Clave:**\n",
    "  - Importación de librerías estándar (`os`, `json`, `logging`).\n",
    "  - Importación de librerías específicas (`hnswlib`, `sentence_transformers`).\n",
    "  - Configuración del sistema de logs.\n",
    "  - Carga de variables de entorno desde un archivo `.env`.\n",
    "- **Oportunidad de Mejora:** Separar configuraciones sensibles como la clave de API en una función de inicialización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import hnswlib\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core.llms import ChatMessage\n",
    "import time\n",
    "import hashlib\n",
    "import random\n",
    "from langdetect import detect\n",
    "\n",
    "# Configuración de logs para imprimir todo en consola\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# Mensaje de confirmación de importación\n",
    "logging.info(\"Librerías importadas correctamente.\")\n",
    "\n",
    "# Cargar variables de entorno desde un archivo .env\n",
    "load_dotenv()\n",
    "logging.info(\"Variables de entorno cargadas desde el archivo .env.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 4: Cargar Documentos**\n",
    "Carga documentos desde archivos o directorios.\n",
    "\n",
    "- **Líneas Clave:**\n",
    "  - `load_documents`: Carga archivos de tipo `.txt`, `.json`, y `.pdf`.\n",
    "  - `extract_content`: Procesa el contenido de los documentos dependiendo de su tipo.\n",
    "- **Oportunidad de Mejora:** Mejorar la validación y manejo de excepciones para formatos no soportados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents(source, is_directory=False):\n",
    "    \"\"\"\n",
    "    Carga documentos desde un archivo o directorio.\n",
    "    \n",
    "    Args:\n",
    "        source (str): Ruta al archivo o directorio.\n",
    "        is_directory (bool): Indica si la fuente es un directorio.\n",
    "    \n",
    "    Returns:\n",
    "        list: Lista de diccionarios con 'filename' y 'content'.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(source):\n",
    "        logging.error(f\"La fuente '{source}' no existe.\")\n",
    "        raise FileNotFoundError(f\"La fuente '{source}' no se encontró.\")\n",
    "\n",
    "    loaded_files = []\n",
    "    if is_directory:\n",
    "        logging.info(f\"Iniciando carga desde el directorio: {source}.\")\n",
    "        for filename in os.listdir(source):\n",
    "            filepath = os.path.join(source, filename)\n",
    "            if os.path.isfile(filepath) and filepath.endswith(('.txt', '.json', '.pdf')):\n",
    "                content = extract_content(filepath)\n",
    "                if content:\n",
    "                    loaded_files.append({\"filename\": filename, \"content\": content})\n",
    "                    logging.info(f\"Archivo '{filename}' cargado correctamente.\")\n",
    "    else:\n",
    "        logging.info(f\"Iniciando carga del archivo: {source}.\")\n",
    "        content = extract_content(source)\n",
    "        if content:\n",
    "            loaded_files.append({\"filename\": os.path.basename(source), \"content\": content})\n",
    "            logging.info(f\"Archivo '{os.path.basename(source)}' cargado correctamente.\")\n",
    "\n",
    "    logging.info(f\"{len(loaded_files)} documentos cargados.\")\n",
    "    return loaded_files\n",
    "\n",
    "def extract_content(filepath):\n",
    "    \"\"\"\n",
    "    Extrae el contenido del archivo según su tipo.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Ruta al archivo.\n",
    "    \n",
    "    Returns:\n",
    "        list o dict o str: Contenido procesado del archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.txt'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            units = content.split(\"\\n-----\\n\")\n",
    "            return units\n",
    "        elif filepath.endswith('.json'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            return data\n",
    "        elif filepath.endswith('.pdf'):\n",
    "            reader = PdfReader(filepath)\n",
    "            return ''.join(page.extract_text() or '' for page in reader.pages)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al extraer contenido de '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Configuración de ruta y carga de documentos\n",
    "ruta_fuente = 'data'  # Asegúrate de tener una carpeta 'data' con los documentos\n",
    "documentos = load_documents(ruta_fuente, is_directory=True)\n",
    "logging.info(f\"Se cargaron {len(documentos)} documentos exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 5: Configurar la Clave API de Gemini**\n",
    "- Obtiene la clave API desde las variables de entorno.\n",
    "- Crea una instancia del modelo Gemini para uso posterior.\n",
    "OPORTUNIDAD DE MEJORA: Manejo más robusto ante una clave inválida o expiración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_llm = None\n",
    "\n",
    "def configure_gemini():\n",
    "    \"\"\"\n",
    "    Configura la instancia de Gemini usando la clave API.\n",
    "    \n",
    "    Returns:\n",
    "        Gemini: Instancia configurada del modelo Gemini.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        logging.error(\"La clave API de Gemini no está configurada.\")\n",
    "        raise EnvironmentError(\"Configura GEMINI_API_KEY en tu archivo .env.\")\n",
    "    gemini = Gemini(api_key=api_key)\n",
    "    logging.info(\"Gemini configurado correctamente.\")\n",
    "    return gemini\n",
    "\n",
    "gemini_llm = configure_gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 6: Configurar el Modelo de Embeddings**\n",
    "- Se utiliza SentenceTransformer para generar embeddings de texto.\n",
    "- `doc_enfermedad(pregunta)`: Determina el índice del documento más relevante \n",
    "  según la similitud con el nombre del archivo.\n",
    "\n",
    "OPORTUNIDAD DE MEJORA:\n",
    "- En lugar de basarse en el nombre del archivo, usar embeddings del contenido \n",
    "  para mayor precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# Precomputar los embeddings de los nombres de archivo para eficiencia\n",
    "archivos = [doc['filename'] for doc in documentos]\n",
    "archivos_embeddings = model.encode(archivos)\n",
    "\n",
    "def doc_enfermedad(pregunta):\n",
    "    \"\"\"\n",
    "    Identifica el índice del documento más relevante para la enfermedad en la pregunta.\n",
    "    Utiliza embeddings precomputados de los nombres de archivo.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        int: Índice del documento más relevante.\n",
    "    \"\"\"\n",
    "    if not documentos:\n",
    "        logging.warning(\"No se encontraron documentos. Índice por defecto: 0.\")\n",
    "        return 0\n",
    "\n",
    "    # Generar embedding de la pregunta\n",
    "    preg_embedding = model.encode(pregunta)\n",
    "\n",
    "    # Calcular similitudes con los embeddings de los nombres de archivo\n",
    "    similarities = [util.cos_sim(preg_embedding, emb).item() for emb in archivos_embeddings]\n",
    "\n",
    "    # Obtener el índice con mayor similitud\n",
    "    max_index = similarities.index(max(similarities))\n",
    "    return max_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 7: Crear Clases para Documentos e Índices**\n",
    "- Clase `Document`: Representa un documento con su texto y metadatos.\n",
    "- Clase `HNSWIndex`: Crea un índice para recuperación rápida de información mediante embeddings.\n",
    "\n",
    "OPORTUNIDAD DE MEJORA:\n",
    "- Incluir más metadatos o estructuras de datos más complejas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        \"\"\"\n",
    "        Inicializa un documento con su contenido y metadatos.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Texto del documento.\n",
    "            metadata (dict, optional): Metadatos asociados al documento.\n",
    "        \"\"\"\n",
    "        self.page_content = text\n",
    "        self.metadata = metadata or {}\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Representación en string del documento.\n",
    "        \n",
    "        Returns:\n",
    "            str: Información formateada del documento.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            f\"Título: {self.metadata.get('Title', 'N/A')}\\n\"\n",
    "            f\"Resumen: {self.metadata.get('Summary', 'N/A')}\\n\"\n",
    "            f\"Tipo de Estudio: {self.metadata.get('StudyType', 'N/A')}\\n\"\n",
    "            f\"Paises donde se desarrolla el estudio: {self.metadata.get('Countries', 'N/A')}\\n\"\n",
    "            f\"Fase en que se encuentra el estudio: {self.metadata.get('Phases', 'N/A')}\\n\"\n",
    "            f\"Identificación en ClinicaTrial: {self.metadata.get('IDestudio', 'N/A')}.\\n\\n\"\n",
    "        )\n",
    "\n",
    "class HNSWIndex:\n",
    "    def __init__(self, embeddings, metadata=None, space='cosine', ef_construction=200, M=16):\n",
    "        \"\"\"\n",
    "        Inicializa el índice HNSWlib con los embeddings proporcionados.\n",
    "        \n",
    "        Args:\n",
    "            embeddings (np.ndarray): Matriz de embeddings.\n",
    "            metadata (list, optional): Lista de metadatos asociados a cada embedding.\n",
    "            space (str, optional): Espacio métrico para HNSWlib.\n",
    "            ef_construction (int, optional): Parámetro ef para la construcción del índice.\n",
    "            M (int, optional): Parámetro M para HNSWlib.\n",
    "        \"\"\"\n",
    "        self.dimension = embeddings.shape[1]\n",
    "        self.index = hnswlib.Index(space=space, dim=self.dimension)\n",
    "        self.index.init_index(max_elements=embeddings.shape[0], ef_construction=ef_construction, M=M)\n",
    "        self.index.add_items(embeddings, np.arange(embeddings.shape[0]))\n",
    "        self.index.set_ef(50)  # Parámetro ef para consultas\n",
    "        self.metadata = metadata or []\n",
    "    \n",
    "    def similarity_search(self, query_vector, k=5):\n",
    "        \"\"\"\n",
    "        Realiza una búsqueda de los k vecinos más similares.\n",
    "        \n",
    "        Args:\n",
    "            query_vector (np.ndarray): Vector de consulta.\n",
    "            k (int, optional): Número de vecinos a buscar.\n",
    "        \n",
    "        Returns:\n",
    "            list: Lista de tuplas con metadatos y distancias.\n",
    "        \"\"\"\n",
    "        labels, distances = self.index.knn_query(query_vector, k=k)\n",
    "        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(labels[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 8: Procesar Documentos y Crear Índices**\n",
    "- `desdobla_doc(data2)`: Crea objetos `Document` a partir del contenido.\n",
    "- Para JSON con ensayos clínicos, crea un `Document` por ensayo.\n",
    "- Para TXT/PDF, un `Document` genérico.\n",
    "- Genera embeddings y construye el índice `HNSWlib`.\n",
    "\n",
    "OPORTUNIDAD DE MEJORA:\n",
    "- Validar mejor la estructura JSON.\n",
    "- Realizar preprocesamiento de texto (limpieza) antes de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desdobla_doc(data2):\n",
    "    \"\"\"\n",
    "    Desdobla el contenido del documento en varios `Document` con metadatos.\n",
    "    Maneja JSON (asumiendo estructura de ensayos clínicos) o texto/PDF genérico.\n",
    "    \n",
    "    Args:\n",
    "        data2 (dict): Diccionario con 'filename' y 'content'.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Lista de `Document` y instancia de `HNSWIndex`.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    summaries = []\n",
    "    contenido = data2['content']\n",
    "    \n",
    "    if isinstance(contenido, list):\n",
    "        for entry in contenido:\n",
    "            if isinstance(entry, dict):\n",
    "                nctId = entry.get(\"IDestudio\", \"\")\n",
    "                briefTitle = entry.get(\"Title\", \"\")\n",
    "                summary = entry.get(\"Summary\", \"\")\n",
    "                studyType = entry.get(\"StudyType\", \"\")\n",
    "                country = entry.get(\"Countries\", \"\")\n",
    "                overallStatus = entry.get(\"OverallStatus\", \"\")\n",
    "                conditions = entry.get(\"Conditions\", \"\")\n",
    "                phases = entry.get(\"Phases\", \"\")\n",
    "\n",
    "                # Crear resumen en inglés para consistencia interna\n",
    "                Summary = (\n",
    "                    f\"The study titled '{briefTitle}', of type '{studyType}', \"\n",
    "                    f\"investigates the condition(s): {conditions}. \"\n",
    "                    f\"Brief summary: {summary}. \"\n",
    "                    f\"Current status: {overallStatus}, taking place in {country}. \"\n",
    "                    f\"The study is classified under: {phases} phase. \"\n",
    "                    f\"For more info, search {nctId} on ClinicalTrials.\"\n",
    "                )\n",
    "                metadata = {\n",
    "                    \"Title\": briefTitle,\n",
    "                    \"Summary\": Summary,\n",
    "                    \"StudyType\": studyType,\n",
    "                    \"Countries\": country,\n",
    "                    \"Phases\": phases,\n",
    "                    \"IDestudio\": nctId\n",
    "                }\n",
    "                doc = Document(Summary, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(Summary)\n",
    "            else:\n",
    "                # Si no es dict, tratar la entrada como texto genérico\n",
    "                texto = str(entry)\n",
    "                metadata = {\"Summary\": texto}\n",
    "                doc = Document(texto, metadata)\n",
    "                documents.append(doc)\n",
    "                summaries.append(texto)\n",
    "    else:\n",
    "        # Texto genérico (PDF o TXT)\n",
    "        texto = str(contenido)\n",
    "        metadata = {\"Summary\": texto}\n",
    "        doc = Document(texto, metadata)\n",
    "        documents.append(doc)\n",
    "        summaries.append(texto)\n",
    "\n",
    "    if documents:\n",
    "        embeddings = model.encode([doc.page_content for doc in documents], show_progress_bar=False)\n",
    "        embeddings = np.array(embeddings).astype(np.float32)\n",
    "        vector_store = HNSWIndex(embeddings, metadata=[doc.metadata for doc in documents])\n",
    "    else:\n",
    "        vector_store = None\n",
    "\n",
    "    return documents, vector_store\n",
    "\n",
    "# Procesar todos los documentos y crear sus respectivos índices\n",
    "trozos_archivos = []\n",
    "index_archivos = []\n",
    "for i in range(len(documentos)):\n",
    "    trozos, index = desdobla_doc(documentos[i])\n",
    "    trozos_archivos.append(trozos)\n",
    "    index_archivos.append(index)\n",
    "\n",
    "logging.info(\"Índices HNSWlib creados para todos los documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 9: Traducir Preguntas y Respuestas**\n",
    "- `traducir(texto, idioma_destino)`: Usa Gemini para traducir el texto solicitado.\n",
    "- `generate_embedding(texto)`: Genera embeddings para la pregunta en inglés.\n",
    "\n",
    "OPORTUNIDAD DE MEJORA:\n",
    "- Implementar caché de traducciones.\n",
    "- Detección de idioma para traducir sólo si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caché en memoria para embeddings de preguntas\n",
    "embedding_cache = {}\n",
    "\n",
    "# Caché en memoria para traducciones\n",
    "translation_cache = {}\n",
    "\n",
    "def traducir(texto, idioma_destino):\n",
    "    \"\"\"\n",
    "    Traduce texto al idioma especificado usando el modelo Gemini.\n",
    "    Usa caché para evitar traducciones repetidas.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto a traducir.\n",
    "        idioma_destino (str): Idioma de destino.\n",
    "    \n",
    "    Returns:\n",
    "        str: Texto traducido o original si ya está en el idioma deseado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        idioma_origen = detect(texto)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al detectar el idioma: {e}\")\n",
    "        idioma_origen = \"unknown\"\n",
    "\n",
    "    if idioma_origen.lower() == idioma_destino.lower():\n",
    "        logging.info(\"El texto ya está en el idioma de destino. No se realiza traducción.\")\n",
    "        return texto\n",
    "\n",
    "    if (texto, idioma_destino) in translation_cache:\n",
    "        logging.info(f\"Traducción obtenida del caché para el texto: {texto}\")\n",
    "        return translation_cache[(texto, idioma_destino)]\n",
    "\n",
    "    start_time = time.time()\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Actúa como un traductor.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Por favor, traduce este texto al {idioma_destino}: {texto}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Traducción completada en {elapsed_time:.2f} segundos.\")\n",
    "        traduccion = respuesta.message.content.strip()\n",
    "        translation_cache[(texto, idioma_destino)] = traduccion\n",
    "        return traduccion\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al traducir: {e}\")\n",
    "        return texto  # fallback\n",
    "\n",
    "def generate_embedding(texto):\n",
    "    \"\"\"\n",
    "    Genera un embedding para el texto utilizando el modelo de embeddings.\n",
    "    Usa caché para evitar recalcular embeddings de textos repetidos.\n",
    "    \n",
    "    Args:\n",
    "        texto (str): Texto para generar el embedding.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Embedding generado o vector de ceros en caso de fallo.\n",
    "    \"\"\"\n",
    "    if texto in embedding_cache:\n",
    "        logging.info(f\"Embedding obtenido del caché para el texto: {texto}\")\n",
    "        return embedding_cache[texto]\n",
    "    try:\n",
    "        embedding = model.encode([texto])\n",
    "        embedding_cache[texto] = embedding\n",
    "        logging.info(f\"Embedding generado para el texto: {texto}\")\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar el embedding: {e}\")\n",
    "        # Devuelve embedding vacío como fallback\n",
    "        return np.zeros((1, 384))\n",
    "\n",
    "def obtener_contexto(pregunta, index, trozos, top_k=50):\n",
    "    \"\"\"\n",
    "    Recupera los trozos de texto más relevantes para responder la pregunta.\n",
    "    Traduce la pregunta al inglés antes de buscar en el índice.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        index (HNSWIndex): Índice de HNSWlib para buscar similitudes.\n",
    "        trozos (list): Lista de `Document` relacionados.\n",
    "        top_k (int, optional): Número de resultados a recuperar.\n",
    "    \n",
    "    Returns:\n",
    "        str: Contexto relevante concatenado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Traducir la pregunta al inglés\n",
    "        pregunta_en_ingles = traducir(pregunta, \"inglés\")\n",
    "        logging.info(f\"Pregunta traducida al inglés: {pregunta_en_ingles}\")\n",
    "\n",
    "        # Generar embedding de la pregunta traducida\n",
    "        pregunta_emb = generate_embedding(pregunta_en_ingles)\n",
    "        logging.info(\"Embedding generado para la pregunta.\")\n",
    "\n",
    "        # Buscar en el índice\n",
    "        results = index.similarity_search(pregunta_emb, k=top_k)\n",
    "        texto = \"\"\n",
    "        for entry in results:\n",
    "            resum = entry[0][\"Summary\"]\n",
    "            texto += resum + \"\\n\"\n",
    "\n",
    "        logging.info(\"Contexto relevante recuperado para la pregunta.\")\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al obtener el contexto: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 10: Generar Respuestas**\n",
    "- `categorizar_pregunta(pregunta)`: Usa palabras clave para clasificar la pregunta (ej: 'ensayo', 'tratamiento').\n",
    "- `generar_prompt(categoria, pregunta)`: Crea una instrucción específica según la categoría.\n",
    "- `generar_respuesta(pregunta, contexto, prompt_especifico)`: \n",
    "  - Envía el prompt y el contexto a Gemini.\n",
    "  - Traduce la respuesta al español.\n",
    "\n",
    "OPORTUNIDAD DE MEJORA:\n",
    "- Usar un modelo de clasificación semántica en lugar de palabras clave.\n",
    "- Detectar el idioma de la pregunta y responder en el mismo idioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizar_pregunta(pregunta):\n",
    "    \"\"\"\n",
    "    Clasifica la pregunta en categorías basadas en palabras clave.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str: Categoría identificada.\n",
    "    \"\"\"\n",
    "    categorias = {\n",
    "        \"tratamiento\": [\"tratamiento\", \"medicación\", \"cura\", \"terapia\", \"fármaco\"],\n",
    "        \"ensayo\": [\"ensayo\", \"estudio\", \"prueba\", \"investigación\", \"trial\"],\n",
    "        \"resultado\": [\"resultado\", \"efectividad\", \"resultados\", \"éxito\", \"fracaso\"],\n",
    "        \"prevención\": [\"prevención\", \"previene\", \"evitar\", \"reducción de riesgo\"]\n",
    "    }\n",
    "    for categoria, palabras in categorias.items():\n",
    "        if any(palabra in pregunta.lower() for palabra in palabras):\n",
    "            return categoria\n",
    "    return \"general\"\n",
    "\n",
    "def generar_prompt(categoria, pregunta):\n",
    "    \"\"\"\n",
    "    Genera un prompt específico basado en la categoría de la pregunta.\n",
    "    \n",
    "    Args:\n",
    "        categoria (str): Categoría de la pregunta.\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str: Prompt generado.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"tratamiento\": f\"Proporciona información sobre tratamientos en ensayos clínicos relacionados con: {pregunta}.\",\n",
    "        \"ensayo\": f\"Describe los ensayos clínicos actuales relacionados con: {pregunta}.\",\n",
    "        \"resultado\": f\"Explica los resultados más recientes de ensayos clínicos sobre: {pregunta}.\",\n",
    "        \"prevención\": f\"Ofrece información sobre prevención y ensayos clínicos para: {pregunta}.\"\n",
    "    }\n",
    "    return prompts.get(categoria, \"Por favor, responde la pregunta sobre ensayos clínicos.\")\n",
    "\n",
    "def es_saludo(pregunta):\n",
    "    \"\"\"\n",
    "    Verifica si la pregunta del usuario es un saludo.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True si es un saludo, False de lo contrario.\n",
    "    \"\"\"\n",
    "    saludos = [\"hola\", \"buen día\", \"buenas\", \"cómo estás\", \"cómo te llamas\", \"qué tal\", \"estás bien\", \"buenas tardes\", \"buenas noches\"]\n",
    "    return any(saludo in pregunta.lower() for saludo in saludos)\n",
    "\n",
    "def responder_saludo():\n",
    "    \"\"\"\n",
    "    Genera una respuesta aleatoria a un saludo.\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta de saludo.\n",
    "    \"\"\"\n",
    "    saludos_respuestas = [\n",
    "        \"¡Hola! Estoy para ayudarte con información sobre ensayos clínicos. ¿En qué puedo asistirte hoy?\",\n",
    "        \"¡Buenas! Tenés alguna pregunta sobre ensayos clínicos en enfermedades neuromusculares?\",\n",
    "        \"¡Hola! ¿Cómo puedo ayudarte con tus consultas sobre ensayos clínicos?\"\n",
    "    ]\n",
    "    return random.choice(saludos_respuestas)\n",
    "\n",
    "def generar_respuesta(pregunta, contexto, prompt_especifico):\n",
    "    \"\"\"\n",
    "    Genera una respuesta usando el contexto proporcionado y un prompt específico.\n",
    "    Primero genera la respuesta en inglés, luego la traduce al español.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        contexto (str): Contexto relevante recuperado.\n",
    "        prompt_especifico (str): Prompt adaptado a la categoría de la pregunta.\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta generada en español.\n",
    "    \"\"\"\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Eres un experto médico.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"{prompt_especifico}\\nContexto: {contexto}\\nPregunta: {pregunta}\")\n",
    "    ]\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Respuesta generada en inglés en {elapsed_time:.2f} segundos.\")\n",
    "        # Traducir la respuesta al español\n",
    "        respuesta_en_espanol = traducir(respuesta.message.content, \"español\")\n",
    "        logging.info(\"Respuesta traducida al español.\")\n",
    "        return respuesta_en_espanol\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar la respuesta: {e}\")\n",
    "        return \"Lo siento, ocurrió un error al generar la respuesta.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 11: Función Principal para Responder Preguntas**\n",
    "- Utiliza caché para no recalcular respuestas idénticas.\n",
    "- Integra categorización, obtención de contexto y generación de respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_hash(pregunta):\n",
    "    \"\"\"\n",
    "    Genera un hash SHA-256 para una pregunta dada.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str: Hash generado.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()\n",
    "\n",
    "def obtener_respuesta_cacheada(pregunta):\n",
    "    \"\"\"\n",
    "    Obtiene una respuesta cacheada para una pregunta si existe.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "    \n",
    "    Returns:\n",
    "        str o None: Respuesta cacheada o None si no existe.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    if os.path.exists(archivo_cache):\n",
    "        try:\n",
    "            with open(archivo_cache, \"r\", encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                return datos.get(\"respuesta\", None)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al leer el caché: {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def guardar_respuesta_cacheada(pregunta, respuesta):\n",
    "    \"\"\"\n",
    "    Guarda una respuesta en caché para una pregunta dada.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        respuesta (str): Respuesta generada.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)\n",
    "        with open(archivo_cache, \"w\", encoding='utf-8') as f:\n",
    "            json.dump({\"pregunta\": pregunta, \"respuesta\": respuesta}, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Respuesta cacheada para la pregunta: '{pregunta}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar la respuesta en caché: {e}\")\n",
    "\n",
    "def responder_pregunta(pregunta, index, trozos):\n",
    "    \"\"\"\n",
    "    Integra categorización, obtención de contexto y generación de respuesta.\n",
    "    Incluye manejo de caché para respuestas repetidas.\n",
    "    \n",
    "    Args:\n",
    "        pregunta (str): Pregunta del usuario.\n",
    "        index (HNSWIndex): Índice de HNSWlib para búsqueda de contexto.\n",
    "        trozos (list): Lista de `Document` relacionados.\n",
    "    \n",
    "    Returns:\n",
    "        str: Respuesta generada.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if index is None or not trozos:\n",
    "            logging.warning(\"No se encontraron índices o trozos para esta pregunta.\")\n",
    "            return \"No se encontró información para responder tu pregunta.\"\n",
    "\n",
    "        # Verificar caché\n",
    "        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)\n",
    "        if respuesta_cacheada:\n",
    "            logging.info(f\"Respuesta obtenida del caché para: '{pregunta}'\")\n",
    "            return respuesta_cacheada\n",
    "\n",
    "        # Categorizar la pregunta\n",
    "        categoria = categorizar_pregunta(pregunta)\n",
    "        logging.info(f\"Categoría de la pregunta: {categoria}\")\n",
    "\n",
    "        # Generar prompt específico\n",
    "        prompt_especifico = generar_prompt(categoria, pregunta)\n",
    "        logging.info(f\"Prompt específico: {prompt_especifico}\")\n",
    "\n",
    "        # Obtener contexto relevante\n",
    "        contexto = obtener_contexto(pregunta, index, trozos)\n",
    "        if not contexto.strip():\n",
    "            logging.warning(\"No se encontró contexto relevante.\")\n",
    "            respuesta = \"No pude encontrar información relevante para responder tu pregunta.\"\n",
    "            guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "            return respuesta\n",
    "\n",
    "        # Generar la respuesta\n",
    "        respuesta = generar_respuesta(pregunta, contexto, prompt_especifico)\n",
    "\n",
    "        # Guardar la respuesta en caché\n",
    "        guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso de responder pregunta: {e}\")\n",
    "        return \"Ocurrió un error al procesar tu pregunta.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 12: Interfaz CLI**\n",
    "Ofrece una interfaz de línea de comando para interactuar con el chatbot.\n",
    "\n",
    "- **Líneas Clave:**\n",
    "  - Espera una pregunta del usuario.\n",
    "  - Responde saludos.\n",
    "  - Detecta si el usuario quiere salir.\n",
    "  - Llama a `responder_pregunta()` con la información necesaria.\n",
    "\n",
    "- **Oportunidad de Mejora:**\n",
    "  - Crear una interfaz web amigable.\n",
    "  - Almacenar historial de preguntas y respuestas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bienvenido al Chatbot de Ensayos Clínicos\n",
      "Conversemos sobre Ensayos Clínicos en enfermedades neuromusculares (Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miotónica, etc.).\n",
      "Escribí tu pregunta, indicando la enfermedad sobre la que quieres información. Escribí 'salir' para terminar.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 01:07:49,797 - INFO - Se detectó un saludo.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Hola! Estoy para ayudarte con información sobre ensayos clínicos. ¿En qué puedo asistirte hoy?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  8.67it/s]\n",
      "2024-12-07 01:08:00,751 - INFO - Categoría de la pregunta: ensayo\n",
      "2024-12-07 01:08:00,751 - INFO - Prompt específico: Describe los ensayos clínicos actuales relacionados con: ¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?.\n",
      "2024-12-07 01:08:00,762 - INFO - Traducción obtenida del caché para el texto: ¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?\n",
      "2024-12-07 01:08:00,763 - INFO - Pregunta traducida al inglés: How many clinical trials are currently active for Duchenne Muscular Dystrophy?\n",
      "2024-12-07 01:08:00,765 - INFO - Embedding obtenido del caché para el texto: How many clinical trials are currently active for Duchenne Muscular Dystrophy?\n",
      "2024-12-07 01:08:00,766 - INFO - Embedding generado para la pregunta.\n",
      "2024-12-07 01:08:00,768 - INFO - Contexto relevante recuperado para la pregunta.\n",
      "2024-12-07 01:08:07,205 - INFO - Respuesta generada en inglés en 6.44 segundos.\n",
      "2024-12-07 01:08:12,806 - INFO - Traducción completada en 5.59 segundos.\n",
      "2024-12-07 01:08:12,807 - INFO - Respuesta traducida al español.\n",
      "2024-12-07 01:08:12,809 - INFO - Respuesta cacheada para la pregunta: '¿Cuantos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta: Basándome en la información proporcionada, hay **11 ensayos clínicos activos actualmente** para la Distrofia Muscular de Duchenne. Estos son:\n",
      "\n",
      "* **Registro de Duchenne (NCT02069756):** RECLUTANDO\n",
      "* **Estudio a largo plazo y de extensión de DS-5141b en pacientes con Distrofia Muscular de Duchenne (NCT04433234):** ACTIVO, NO RECLUTANDO\n",
      "* **Registro de evaluación por video de la Distrofia Muscular de Duchenne (NCT05712447):** ACTIVO, NO RECLUTANDO\n",
      "* **Estudio de SRP-4045 (Casimersen) y SRP-4053 (Golodirsen) en participantes con Distrofia Muscular de Duchenne (DMD) (NCT02500381):** ACTIVO, NO RECLUTANDO\n",
      "* **AFFINITY DUCHENNE: Terapia génica RGX-202 en participantes con Distrofia Muscular de Duchenne (DMD) (NCT05693142):** RECLUTANDO\n",
      "* **Un estudio de Fase 3 de TAS-205 en pacientes con Distrofia Muscular de Duchenne (REACH-DMD) (NCT04587908):** ACTIVO, NO RECLUTANDO\n",
      "* **Un estudio de la terapia génica SGT-003 en la Distrofia Muscular de Duchenne (INSPIRE DUCHENNE) (NCT06138639):** RECLUTANDO\n",
      "* **Un estudio observacional que compara Delandistrogene Moxeparvovec con el estándar de atención en participantes con Distrofia Muscular de Duchenne (NCT06270719):** RECLUTANDO\n",
      "* **Un estudio de PGN-EDO51 en participantes con Distrofia Muscular de Duchenne aptos para tratamiento de omisión del exón 51 (NCT06079736):** RECLUTANDO\n",
      "* **Estudio para evaluar la seguridad y eficacia de PF-06939926 para el tratamiento de la Distrofia Muscular de Duchenne (NCT04281485):** ACTIVO, NO RECLUTANDO\n",
      "* **NS-089/NCNP-02-201 en niños con Distrofia Muscular de Duchenne (DMD) (NCT05996003):** RECLUTANDO\n",
      "* **Un estudio de CAP-1002 en pacientes ambulatorios y no ambulatorios con Distrofia Muscular de Duchenne (NCT05126758):** RECLUTANDO\n",
      "* **Un estudio abierto para evaluar la eficacia y seguridad de Satralizumab en la Distrofia Muscular de Duchenne (NCT06450639):** RECLUTANDO\n",
      "* **Hidroterapia en la Distrofia Muscular de Duchenne (DMD) (NCT06445985):** RECLUTANDO\n",
      "* **Estudio de seguridad, tolerabilidad, farmacodinamia, eficacia y farmacocinética de DYNE-251 en participantes con Distrofia Muscular de Duchenne aptos para la omisión del exón 51 (NCT05524883):** RECLUTANDO\n",
      "\n",
      "\n",
      "Es importante notar que el estado de \"activo\" puede variar rápidamente. Esta respuesta refleja el estado en el momento del análisis de los datos proporcionados. Para obtener la información más actualizada, se recomienda consultar directamente la base de datos de ClinicalTrials.gov.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 01:08:15,863 - INFO - El usuario ha finalizado la sesión.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Chau!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Crear directorio de caché si no existe\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "    \n",
    "    if len(documentos) == 0:\n",
    "        print(\"No se cargaron documentos. Por favor, verificá el directorio 'data'.\")\n",
    "        logging.error(\"No se encontraron documentos. Finalizando.\")\n",
    "    else:\n",
    "        # Mensaje de bienvenida e instrucciones para el usuario.\n",
    "        print(\"Bienvenido al chatbot de Ensayos Clínicos.\")\n",
    "        print(\"Conversemos sobre Ensayos Clínicos relacionados con las siguientes enfermedades neuromusculares:\")\n",
    "        print(\"- Distrofia Muscular de Duchenne o de Becker\")\n",
    "        print(\"- Enfermedad de Pompe\")\n",
    "        print(\"- Distrofia Miotónica\")\n",
    "        print(\"- Enfermedad de almacenamiento de glucógeno\")\n",
    "        print(\"Por favor, escribe tu pregunta indicando claramente la enfermedad sobre la que deseas información.\")\n",
    "        print(\"Escribí 'salir' para terminar la conversación.\")\n",
    "        while True:\n",
    "            pregunta = input(\"Tu pregunta: \").strip()\n",
    "            if pregunta.lower() in ['salir', 'chau', 'exit', 'quit']:\n",
    "                print(\"¡Chau!\")\n",
    "                logging.info(\"El usuario ha finalizado la sesión.\")\n",
    "                break\n",
    "            if es_saludo(pregunta):\n",
    "                respuesta_saludo = responder_saludo()\n",
    "                print(respuesta_saludo)\n",
    "                logging.info(\"Se detectó un saludo.\")\n",
    "                continue\n",
    "            \n",
    "            # Identificar la enfermedad (documento más relevante)\n",
    "            idn = doc_enfermedad(pregunta)\n",
    "            index = index_archivos[idn] if idn < len(index_archivos) else None\n",
    "            trozos = trozos_archivos[idn] if idn < len(trozos_archivos) else []\n",
    "\n",
    "            # Responder la pregunta\n",
    "            respuesta = responder_pregunta(pregunta, index, trozos)\n",
    "            print(f\"Respuesta: {respuesta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

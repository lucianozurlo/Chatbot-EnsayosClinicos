{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 1: Verificar versión de Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # Acceder a la información de la versión de Python.\n",
    "import os  # Manejo de rutas, archivos y operaciones del sistema.\n",
    "import logging  # Configuración y uso de logs para monitorear la ejecución.\n",
    "\n",
    "# Configurar la variable de entorno para desactivar la paralelización de tokenizadores y evitar advertencias\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    ")\n",
    "\n",
    "# Definir la versión requerida de Python\n",
    "REQUIRED_VERSION = (3, 10, 12)\n",
    "current_version = sys.version_info\n",
    "\n",
    "# Validar la versión de Python\n",
    "if (current_version.major, current_version.minor, current_version.micro) != REQUIRED_VERSION:\n",
    "    logging.warning(\"\"\"\n",
    "    **********************************************\n",
    "    ** Advertencia: Versión de Python no compatible **\n",
    "    **********************************************\n",
    "    Este chatbot está optimizado para Python 3.10.12.\n",
    "    La versión actual es Python {}.{}.{}.\n",
    "    Algunas funcionalidades pueden no funcionar correctamente.\n",
    "    **********************************************\n",
    "    \"\"\".format(current_version.major, current_version.minor, current_version.micro))\n",
    "else:\n",
    "    logging.info(\"\"\"\n",
    "    **********************************************\n",
    "    ** Versión de Python compatible **\n",
    "    **********************************************\n",
    "    Python 3.10.12 detectado correctamente.\n",
    "    Todas las funcionalidades deberían operar sin problemas.\n",
    "    **********************************************\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 2: Instalación de Paquetes Necesarios**\n",
    "Se instalan las bibliotecas necesarias para que el chatbot funcione correctamente.\n",
    "\n",
    "- **Transformers (`transformers`)**: Para el procesamiento de lenguaje natural.\n",
    "- **Sentence Transformers (`sentence_transformers`)**: Para crear embeddings eficientes de texto.\n",
    "- **HNSWlib (`hnswlib`)**: Realiza búsquedas rápidas de vecinos más cercanos.\n",
    "- **Numpy (`numpy<2.0`)**: Utiliza una versión compatible para operaciones matemáticas.\n",
    "- **PyPDF2 (`PyPDF2`)**: Manejo y extracción de texto desde archivos PDF.\n",
    "- **Dotenv (`python-dotenv`)**: Gestiona variables de entorno desde un archivo `.env`.\n",
    "- **Tenacity (`tenacity`)**: Manejo de reintentos con lógica exponencial.\n",
    "- **Llama Index (`llama-index` y extensiones para Gemini)**: Proporciona integración con el modelo Gemini.\n",
    "- **Tqdm (`tqdm`)**: Barra de progreso visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de bibliotecas necesarias\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 3: Importar Librerías y Configurar Logging**\n",
    "Se importan todas las librerías necesarias y se configura un sistema de logs para monitorear el flujo del programa.\n",
    "\n",
    "- **Importación de Librerías:** Incluye módulos estándar como `os`, `json`, y `logging`, y bibliotecas específicas del proyecto.\n",
    "- **Configuración del Logging:** \n",
    "  - Configura un formato estándar para los mensajes de log.\n",
    "  - Establece que los mensajes se impriman directamente en la consola.\n",
    "  - Define el nivel de logging como `INFO` para capturar detalles esenciales del flujo.\n",
    "- **Carga de Variables de Entorno:**\n",
    "  - Usa `load_dotenv()` para cargar claves de API u otras configuraciones sensibles desde un archivo `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías esenciales para el funcionamiento del chatbot.\n",
    "import os  # Manejo de rutas, archivos y operaciones del sistema.\n",
    "import json  # Manipulación de datos en formato JSON.\n",
    "import logging  # Configuración y uso de logs para monitorear la ejecución.\n",
    "import hnswlib  # Búsqueda eficiente de similitud utilizando índices de alta dimensionalidad.\n",
    "from sentence_transformers import SentenceTransformer, util  # Embeddings de texto y cálculo de similitud.\n",
    "import numpy as np  # Operaciones matemáticas avanzadas y estructuras de datos.\n",
    "from dotenv import load_dotenv  # Carga de variables de entorno desde un archivo `.env`.\n",
    "from PyPDF2 import PdfReader  # Extracción de texto de documentos PDF.\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type  # Gestión de reintentos en funciones críticas.\n",
    "from llama_index.llms.gemini import Gemini  # Interfaz para el modelo de lenguaje Gemini.\n",
    "from llama_index.core.llms import ChatMessage  # Estructuras de mensajes para interacción con LLMs.\n",
    "import time  # Manejo de tiempos y medición de duración de procesos.\n",
    "import hashlib  # Generación de hashes únicos para almacenamiento en caché.\n",
    "import random  # Generación de valores aleatorios, útil para respuestas personalizadas.\n",
    "import unicodedata  # Normalización de texto.\n",
    "import functools  # Para utilizar mecanismos de cacheo en funciones.\n",
    "\n",
    "\n",
    "logging.info(\"Librerías importadas correctamente.\")\n",
    "\n",
    "# Carga de variables de entorno desde un archivo .env para proteger información sensible como claves de API.\n",
    "load_dotenv()\n",
    "logging.info(\"Variables de entorno cargadas correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 4: Definir Funciones y Clases Base**\n",
    "Define clases para manejar documentos y realizar búsquedas eficientes en índices de texto.\n",
    "\n",
    "- **Clase `Document`:**\n",
    "  - Representa un documento con contenido (`page_content`) y metadatos.\n",
    "  - Implementa un método `__str__` para mostrar información relevante del documento.\n",
    "- **Clase `HNSWIndex`:**\n",
    "  - Implementa un índice de vecinos más cercanos usando HNSWlib.\n",
    "  - Admite búsquedas rápidas basadas en similitud de embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(texto):\n",
    "    \"\"\"\n",
    "    Normaliza el texto eliminando tildes, caracteres especiales, y convirtiendo a minúsculas.\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas.\n",
    "    texto = texto.lower()\n",
    "    # Eliminar tildes y normalizar caracteres unicode.\n",
    "    texto = unicodedata.normalize('NFD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Eliminar caracteres que no sean alfanuméricos o espacios.\n",
    "    texto = ''.join(char for char in texto if char.isalnum() or char.isspace())\n",
    "    return texto.strip()  # Eliminar espacios extra.\n",
    "\n",
    "# Clase para representar documentos cargados con contenido y metadatos.\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        self.page_content = text  # Contenido principal del documento.\n",
    "        self.metadata = metadata or {}  # Metadatos adicionales, si existen, o un diccionario vacío.\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Representación en formato legible de los metadatos del documento.\n",
    "        Accede a los metadatos de forma segura utilizando `.get()` para evitar errores si faltan claves.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            f\"Título: {self.metadata.get('Title', 'N/A')}\\n\"\n",
    "            f\"Resumen: {self.metadata.get('Summary', 'N/A')}\\n\"\n",
    "            f\"Tipo de Estudio: {self.metadata.get('StudyType', 'N/A')}\\n\"\n",
    "            f\"Paises donde se desarrolla el estudio: {self.metadata.get('Countries', 'N/A')}\\n\"\n",
    "            f\"Fase en que se encuentra el estudio: {self.metadata.get('Phases', 'N/A')}\\n\"\n",
    "            f\"Identificación en ClinicaTrial: {self.metadata.get('IDestudio', 'N/A')}.\\n\\n\"\n",
    "        )\n",
    "\n",
    "# Clase para manejar índices HNSWlib y realizar búsquedas eficientes de similitud.\n",
    "class HNSWIndex:\n",
    "    \"\"\"\n",
    "    Clase para manejar índices HNSWlib y realizar búsquedas eficientes de similitud.\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings, metadata=None, space='cosine', ef_construction=200, M=16):\n",
    "        self.dimension = embeddings.shape[1]  # Dimensión de los embeddings.\n",
    "        self.index = hnswlib.Index(space=space, dim=self.dimension)  # Inicializa el índice HNSW con métrica de coseno.\n",
    "        self.index.init_index(max_elements=embeddings.shape[0], ef_construction=ef_construction, M=M)  \n",
    "        # Configura el índice con los parámetros de construcción.\n",
    "        self.index.add_items(embeddings, np.arange(embeddings.shape[0]))  # Añade los embeddings al índice.\n",
    "        self.index.set_ef(50)  # Configura el parámetro `ef` para consultas (balance entre velocidad y precisión).\n",
    "        self.metadata = metadata or []  # Metadatos asociados a los embeddings.\n",
    "    \n",
    "    def similarity_search(self, query_vector, k=5):\n",
    "        \"\"\"\n",
    "        Realiza una búsqueda de los `k` elementos más similares al vector de consulta.\n",
    "        Retorna una lista de tuplas con metadatos y distancias.\n",
    "        \"\"\"\n",
    "        labels, distances = self.index.knn_query(query_vector, k=k)  # Busca los `k` vecinos más cercanos.\n",
    "        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(labels[0])]  # Asocia los resultados con metadatos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 5: Cargar Documentos**\n",
    "Se cargan documentos desde archivos o directorios para analizarlos y extraer contenido relevante.\n",
    "\n",
    "- **Función `extract_content`:**\n",
    "  - Maneja la lógica para extraer contenido dependiendo del formato:\n",
    "    - **TXT:** Divide el contenido en bloques usando delimitadores.\n",
    "    - **JSON:** Carga y devuelve el contenido en formato de diccionario.\n",
    "    - **PDF:** Extrae texto de todas las páginas utilizando `PdfReader`.\n",
    "- **Función `load_documents`:**\n",
    "  - Permite cargar documentos de varios formatos (`.txt`, `.json`, `.pdf`).\n",
    "  - Verifica si la fuente existe y lanza un error si no es válida.\n",
    "  - Itera sobre los archivos en un directorio o procesa un único archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(filepath):\n",
    "    \"\"\"\n",
    "    Extrae el contenido del archivo según su tipo (.txt, .json, .pdf).\n",
    "    Si el archivo es .txt, lo divide en unidades por el delimitador '\\n-----\\n'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.txt'):  # Si es un archivo de texto:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:  # Abre el archivo con codificación UTF-8.\n",
    "                content = file.read()  # Lee el contenido del archivo.\n",
    "            # Dividir el contenido en unidades por el delimitador\n",
    "            units = content.split(\"\\n-----\\n\")  # Divide el texto en bloques por el delimitador.\n",
    "            return units  # Devuelve una lista de bloques de texto.\n",
    "        elif filepath.endswith('.json'):  # Si es un archivo JSON:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:  # Abre el archivo con codificación UTF-8.\n",
    "                return json.load(file)  # Carga y retorna el contenido en formato JSON.\n",
    "        elif filepath.endswith('.pdf'):  # Si es un archivo PDF:\n",
    "            reader = PdfReader(filepath)  # Instancia el lector PDF.\n",
    "            return ''.join(page.extract_text() or '' for page in reader.pages)  # Extrae y concatena el texto de todas las páginas.\n",
    "    except Exception as e:  # Captura cualquier error que ocurra durante la extracción.\n",
    "        logging.error(f\"Error al extraer contenido de '{filepath}': {e}\")  # Log del error.\n",
    "        return None  # Retorna `None` si ocurre un error.\n",
    "    \n",
    "def load_documents(source, is_directory=False):\n",
    "    \"\"\"\n",
    "    Carga documentos desde un archivo o un directorio. Soporta .txt, .json y .pdf.\n",
    "    \"\"\"\n",
    "    loaded_files = []\n",
    "    if is_directory:\n",
    "        for filename in os.listdir(source):\n",
    "            filepath = os.path.join(source, filename)\n",
    "            if os.path.isfile(filepath) and filepath.endswith(('.txt', '.json', '.pdf')):\n",
    "                content = extract_content(filepath)\n",
    "                if content:\n",
    "                    loaded_files.append({\"filename\": normalizar_texto(filename), \"content\": content})\n",
    "    else:\n",
    "        content = extract_content(source)\n",
    "        if content:\n",
    "            loaded_files.append({\"filename\": normalizar_texto(os.path.basename(source)), \"content\": content})\n",
    "    return loaded_files\n",
    "\n",
    "# Cargar documentos (True: busca en directorio, False: busca archivo)\n",
    "ruta_fuente = 'data'  # Define la ruta del directorio de documentos.\n",
    "documentos = load_documents(ruta_fuente, is_directory=True)  # Llama a la función para cargar documentos desde el directorio.\n",
    "\n",
    "# Visualizar cuántos documentos fueron cargados\n",
    "logging.info(f\"Se cargaron {len(documentos)} documentos exitosamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 6: Configurar la Clave API de Gemini**\n",
    "Configura la conexión al modelo Gemini utilizando la clave API proporcionada en un archivo `.env`.\n",
    "\n",
    "- **Función `configure_gemini`:**\n",
    "  - Recupera la clave API desde las variables de entorno.\n",
    "  - Inicializa la instancia de Gemini usando la clave recuperada.\n",
    "  - Lanza un error si la clave no está configurada correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_gemini():\n",
    "    \"\"\"\n",
    "    Configura la instancia de Gemini utilizando la clave API almacenada en variables de entorno.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        logging.error(\"La clave API de Gemini no está configurada.\")\n",
    "        raise EnvironmentError(\"Configura GEMINI_API_KEY en tu archivo .env.\")\n",
    "    gemini_llm = Gemini(api_key=api_key)\n",
    "    logging.info(\"Gemini configurado correctamente.\")\n",
    "    return gemini_llm\n",
    "\n",
    "# Configurar Gemini\n",
    "gemini_llm = configure_gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 7: Configurar el Modelo de Embeddings**\n",
    "Configura el modelo preentrenado de embeddings que se usará para calcular similitudes de texto.\n",
    "\n",
    "- **Modelo Utilizado:** `\"paraphrase-multilingual-MiniLM-L12-v2\"`.\n",
    "- **Carga del Modelo:** Se inicializa con `SentenceTransformer` para generar embeddings.\n",
    "- **Función `doc_enfermedad`:**\n",
    "  - Compara embeddings de la pregunta con los nombres de archivos cargados.\n",
    "  - Devuelve el índice del archivo más relevante basado en la similitud de coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_model(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\"):\n",
    "    \"\"\"\n",
    "    Carga el modelo de embeddings.\n",
    "    \"\"\"\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "model = load_embedding_model()\n",
    "\n",
    "# Precomputar embeddings de nombres de archivos\n",
    "doc_filenames = [doc['filename'] for doc in documentos]\n",
    "doc_filenames_embeddings = model.encode(doc_filenames, show_progress_bar=True)\n",
    "logging.info(\"Embeddings de nombres de archivos precomputados.\")\n",
    "\n",
    "def generate_embedding(texto):\n",
    "    \"\"\"\n",
    "    Genera un embedding para una pregunta o texto.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        embedding = model.encode([texto])\n",
    "        logging.info(\"Embedding generado para el texto.\")\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar el embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "def doc_enfermedad(pregunta_normalizada):\n",
    "    \"\"\"\n",
    "    Identifica el índice del documento más relevante para la pregunta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        preg_embedding = generate_embedding(pregunta_normalizada)\n",
    "        similarities = [util.cos_sim(preg_embedding, doc_emb).item() for doc_emb in doc_filenames_embeddings]\n",
    "        max_index = similarities.index(max(similarities))\n",
    "        logging.debug(f\"Índice máximo: {max_index} con similitud {similarities[max_index]}\")\n",
    "        return max_index\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en doc_enfermedad: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 8: Procesar Documentos e Índices**\n",
    "Convierte documentos en bloques manejables y genera índices para búsquedas rápidas.\n",
    "\n",
    "- **Función `desdobla_doc`:**\n",
    "  - Extrae información relevante de cada documento.\n",
    "  - Crea embeddings para los textos procesados.\n",
    "  - Genera un índice HNSWlib con estos embeddings.\n",
    "- **Proceso por Archivo:** Itera sobre los documentos cargados para crear bloques e índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar documentos y crear índice\n",
    "def desdobla_doc(data2):\n",
    "    \"\"\"\n",
    "    Convierte el contenido de los datos proporcionados en instancias de Document y crea un índice HNSWlib.\n",
    "    \"\"\"\n",
    "    documents = []  # Lista para almacenar los objetos Document.\n",
    "    summaries = []  # Lista para almacenar los resúmenes generados.\n",
    "\n",
    "    for entry in data2['content']:  # Itera sobre cada entrada en los datos.\n",
    "        # Extrae información relevante del contenido, manejando valores faltantes con cadenas vacías.\n",
    "        nctId = entry.get(\"IDestudio\", \"\")\n",
    "        briefTitle = entry.get(\"Title\", \"\")\n",
    "        summary = entry.get(\"Summary\", \"\")\n",
    "        studyType = entry.get(\"StudyType\", \"\")\n",
    "        country = entry.get(\"Countries\", \"\")\n",
    "        overallStatus = entry.get(\"OverallStatus\", \"\")\n",
    "        conditions = entry.get(\"Conditions\", \"\")\n",
    "        phases = entry.get(\"Phases\", \"\") or entry.get(\"Phasess\", \"\")\n",
    "\n",
    "        # Genera un resumen del estudio utilizando los datos extraídos.\n",
    "        Summary = (\n",
    "            f\"The study titled '{briefTitle}', of type '{studyType}', \"\n",
    "            f\"is being conducted to investigate the condition(s) {conditions}. \"\n",
    "            f\"This study is briefly summarized as follows: {summary}. \"\n",
    "            f\"Currently, the study status is {overallStatus}, and it is taking place in {country}. \"\n",
    "            f\"The study is classified under {phases} phase. \"\n",
    "            f\"For more information, search {nctId} on ClinicalTrials.\"\n",
    "        )\n",
    "\n",
    "        # Asocia el resumen con sus metadatos.\n",
    "        metadata = {\"Summary\": Summary}\n",
    "\n",
    "        # Crea una instancia de Document y la agrega a la lista.\n",
    "        documents.append(Document(Summary, metadata))\n",
    "        summaries.append(Summary)  # Almacena el resumen para posibles referencias.\n",
    "\n",
    "    # Genera embeddings para todos los documentos utilizando el modelo cargado.\n",
    "    embeddings = model.encode([doc.page_content for doc in documents], show_progress_bar=True)\n",
    "    embeddings = np.array(embeddings).astype(np.float32)  # Convierte los embeddings a float32 para compatibilidad.\n",
    "\n",
    "    # Crea un índice HNSWlib utilizando los embeddings generados.\n",
    "    vector_store = HNSWIndex(embeddings, metadata=[doc.metadata for doc in documents])\n",
    "\n",
    "    return documents, vector_store  # Retorna los documentos procesados y el índice creado.\n",
    "\n",
    "# Genera trozos e índices para las enfermedades y los almacena en listas.\n",
    "trozos_archivos = []  # Lista para almacenar los bloques procesados de documentos.\n",
    "index_archivos = []  # Lista para almacenar los índices HNSWlib.\n",
    "\n",
    "for i in range(len(documentos)):  # Itera sobre cada conjunto de documentos.\n",
    "    trozos, index = desdobla_doc(documentos[i])  # Procesa y crea el índice para cada conjunto.\n",
    "    trozos_archivos.append(trozos)  # Almacena los bloques procesados.\n",
    "    index_archivos.append(index)  # Almacena el índice creado.\n",
    "\n",
    "logging.info(\"Índices HNSWlib creados para todos los documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 9: Traducir Preguntas y Respuestas**\n",
    "Se encarga de traducir texto entre idiomas usando el modelo Gemini.\n",
    "\n",
    "- **Función `traducir`:**\n",
    "  - Envía el texto a traducir como un mensaje al modelo Gemini.\n",
    "  - Retorna la traducción y registra el tiempo tomado para la operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_exponential(min=1, max=10), stop=stop_after_attempt(3), retry=retry_if_exception_type(Exception))\n",
    "def traducir(texto, idioma_destino):\n",
    "    \"\"\"\n",
    "    Traduce texto al idioma especificado utilizando el modelo Gemini.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Inicia el contador para medir el tiempo de traducción.\n",
    "\n",
    "    # Prepara los mensajes para enviar al modelo de lenguaje.\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Acts as a translator.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Please translate this text into {idioma_destino}: {texto}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time  # Calcula el tiempo transcurrido.\n",
    "        logging.info(f\"Traducción completada en {elapsed_time:.2f} segundos.\")\n",
    "        return respuesta.message.content.strip()  # Devuelve la traducción limpia.\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al traducir: {e}\")\n",
    "        raise  # Vuelve a lanzar la excepción para que el decorador @retry pueda manejarla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 10: Generar Respuestas**\n",
    "Genera respuestas específicas basadas en la pregunta del usuario y el contexto recuperado.\n",
    "\n",
    "- **Función `categorizar_pregunta`:**\n",
    "  - Clasifica la pregunta en categorías como \"tratamiento\", \"ensayo\", etc.\n",
    "- **Función `generar_prompt`:**\n",
    "  - Crea prompts personalizados según la categoría.\n",
    "- **Función `es_saludo`:**\n",
    "  - Detecta si la entrada del usuario es un saludo.\n",
    "- **Función `responder_saludo`:**\n",
    "  - Devuelve una respuesta aleatoria de saludo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejorar la Generación de Respuestas con Prompts Específicos\n",
    "def categorizar_pregunta(pregunta_en_ingles):\n",
    "    \"\"\"\n",
    "    Clasifica la pregunta en categorías específicas en inglés.\n",
    "    \"\"\"\n",
    "    categorias = {\n",
    "        \"treatment\": [\"treatment\", \"medication\", \"cure\", \"therapy\", \"drug\", \"intervention\", \"medications\", \"therapies\"],\n",
    "        \"trial\": [\"trial\", \"trials\", \"study\", \"studies\", \"test\", \"research\", \"clinical trial\", \"clinical trials\"],\n",
    "        \"criteria\": [\"criteria\", \"inclusion\", \"exclusion\", \"participants\", \"eligibility\"],\n",
    "        \"result\": [\"result\", \"results\", \"effectiveness\", \"outcome\", \"outcomes\", \"success\", \"failure\"],\n",
    "        \"location\": [\"city\", \"cities\", \"country\", \"countries\", \"location\", \"locations\", \"place\", \"places\"],\n",
    "        \"prevention\": [\"prevention\", \"prevent\", \"avoiding\", \"avoid\", \"risk reduction\", \"reduce risk\"],\n",
    "        \"duration\": [\"duration\", \"years\", \"months\", \"timeframe\", \"period\", \"length\"],\n",
    "    }\n",
    "    for categoria, palabras in categorias.items():\n",
    "        if any(palabra in pregunta_en_ingles.lower() for palabra in palabras):\n",
    "            return categoria\n",
    "    return \"general\"\n",
    "\n",
    "def generar_prompt(categoria, pregunta_en_ingles):\n",
    "    \"\"\"\n",
    "    Genera un prompt específico basado en la categoría de la pregunta en inglés.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"treatment\": f\"Provide detailed information about treatments related to: {pregunta_en_ingles}.\",\n",
    "        \"trial\": f\"Describe current clinical trials related to: {pregunta_en_ingles}.\",\n",
    "        \"criteria\": f\"Explain inclusion and exclusion criteria for clinical trials on: {pregunta_en_ingles}.\",\n",
    "        \"result\": f\"Explain the most recent results of clinical trials on: {pregunta_en_ingles}.\",\n",
    "        \"location\": f\"Indicate the geographical locations where clinical trials are being conducted for: {pregunta_en_ingles}.\",\n",
    "        \"prevention\": f\"Offer prevention strategies for: {pregunta_en_ingles}.\",\n",
    "        \"duration\": f\"Describe the typical duration of clinical trials on: {pregunta_en_ingles}.\",\n",
    "    }\n",
    "    return prompts.get(categoria, f\"Please answer the following question about clinical trials: {pregunta_en_ingles}\")\n",
    "\n",
    "# Se define una función para detectar si una pregunta es un saludo.\n",
    "def es_saludo(pregunta):\n",
    "    \"\"\"\n",
    "    Detecta si la entrada del usuario es un saludo.\n",
    "    \"\"\"\n",
    "    saludos = [\"hola\", \"buen día\", \"buenas\", \"cómo estás\", \"cómo te llamas\", \"qué tal\", \"estás bien\", \"buenas tardes\", \"buenas noches\"]\n",
    "    return any(saludo in pregunta.lower() for saludo in saludos)\n",
    "\n",
    "def responder_saludo():\n",
    "    \"\"\"\n",
    "    Devuelve una respuesta aleatoria de saludo.\n",
    "    \"\"\"\n",
    "    saludos_respuestas = [\n",
    "        \"¡Hola! Estoy para ayudarte con información sobre ensayos clínicos. ¿En qué puedo asistirte hoy?\",\n",
    "        \"¡Buenas! ¿Tenés alguna pregunta sobre ensayos clínicos en enfermedades neuromusculares?\",\n",
    "        \"¡Hola! ¿Cómo puedo ayudarte con tus consultas sobre ensayos clínicos?\"\n",
    "    ]\n",
    "    return random.choice(saludos_respuestas)\n",
    "\n",
    "def generar_respuesta(pregunta_en_ingles, contexto, prompt_especifico):\n",
    "    \"\"\"\n",
    "    Genera una respuesta usando el contexto proporcionado y un prompt específico.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"You are a medical expert.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"{prompt_especifico}\\nContext: {contexto}\\nQuestion: {pregunta_en_ingles}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logging.info(f\"Respuesta generada en inglés en {elapsed_time:.2f} segundos.\")\n",
    "        return respuesta.message.content.strip()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar la respuesta: {e}\")\n",
    "        return \"I'm sorry, there was an error generating the response.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 11: Función Principal para Responder Preguntas**\n",
    "Integra todos los pasos anteriores para generar respuestas completas.\n",
    "\n",
    "- **Función `obtener_respuesta_cacheada`:**\n",
    "  - Verifica si una pregunta ya tiene respuesta en caché.\n",
    "- **Función `guardar_respuesta_cacheada`:**\n",
    "  - Almacena la respuesta generada en caché.\n",
    "- **Función `responder_pregunta`:**\n",
    "  - Traduce la pregunta.\n",
    "  - Recupera el contexto relevante.\n",
    "  - Genera la respuesta y la almacena en caché."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_hash(pregunta):\n",
    "    \"\"\"\n",
    "    Genera un hash SHA-256 único para la pregunta proporcionada.\n",
    "    Este hash se utiliza como identificador para el almacenamiento en caché.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()  # Convierte la pregunta a un hash hexadecimal.\n",
    "\n",
    "def obtener_respuesta_cacheada(pregunta):\n",
    "    \"\"\"\n",
    "    Recupera una respuesta previamente generada desde el caché, si existe.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    if os.path.exists(archivo_cache):\n",
    "        try:\n",
    "            with open(archivo_cache, \"r\", encoding='utf-8') as f:\n",
    "                datos = json.load(f)\n",
    "                return datos.get(\"respuesta\", None)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al leer el caché para la pregunta '{pregunta}': {e}\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def guardar_respuesta_cacheada(pregunta, respuesta):\n",
    "    \"\"\"\n",
    "    Almacena una respuesta en el caché para consultas futuras.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)\n",
    "        with open(archivo_cache, \"w\", encoding='utf-8') as f:\n",
    "            json.dump({\"pregunta\": pregunta, \"respuesta\": respuesta}, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(f\"Respuesta cacheada para la pregunta: '{pregunta}'\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al guardar la respuesta en caché para la pregunta '{pregunta}': {e}\")\n",
    "\n",
    "def obtener_contexto(pregunta, index, trozos, top_k=50):\n",
    "    \"\"\"\n",
    "    Recupera los trozos de texto más relevantes para responder la pregunta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # No traducir la pregunta al inglés antes de generar el embedding\n",
    "        pregunta_normalizada = normalizar_texto(pregunta)\n",
    "        pregunta_emb = generate_embedding(pregunta_normalizada)\n",
    "        if pregunta_emb is None:\n",
    "            logging.warning(\"No se pudo generar el embedding de la pregunta.\")\n",
    "            return \"No se pudo procesar la pregunta debido a un error al generar el embedding.\"\n",
    "    \n",
    "        # Buscar en el índice\n",
    "        results = index.similarity_search(pregunta_emb, k=top_k)\n",
    "        if not results:\n",
    "            logging.warning(\"No se encontró contexto relevante.\")\n",
    "            return \"No se encontró información relevante para esta pregunta.\"\n",
    "    \n",
    "        texto = \"\\n\".join([entry[0][\"Summary\"] for entry in results])\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al obtener el contexto: {e}\")\n",
    "        return \"Hubo un problema al recuperar la información. Por favor, intenta con otra pregunta.\"\n",
    "\n",
    "def responder_pregunta(pregunta, index, trozos):\n",
    "    \"\"\"\n",
    "    Responde una pregunta del usuario integrando:\n",
    "    - Búsqueda en caché.\n",
    "    - Traducción y recuperación de contexto.\n",
    "    - Generación de respuestas personalizadas.\n",
    "    \n",
    "    Incluye manejo de caché para optimizar el tiempo de respuesta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar si la respuesta ya está en el caché.\n",
    "        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)\n",
    "        if respuesta_cacheada:\n",
    "            logging.info(f\"Respuesta obtenida del caché para la pregunta: '{pregunta}'\")\n",
    "            return respuesta_cacheada\n",
    "\n",
    "        # Traducir la pregunta al inglés para la generación de la respuesta.\n",
    "        pregunta_en_ingles = traducir(pregunta, \"inglés\")\n",
    "        if not pregunta_en_ingles or len(pregunta_en_ingles) < 5:\n",
    "            logging.warning(\"Traducción de la pregunta fallida o insuficiente.\")\n",
    "            respuesta = \"No se pudo procesar tu pregunta debido a un error en la traducción.\"\n",
    "            return respuesta\n",
    "\n",
    "        # Categorizar la pregunta para generar prompts específicos.\n",
    "        categoria = categorizar_pregunta(pregunta_en_ingles)\n",
    "        logging.info(f\"Categoría de la pregunta: {categoria}\")\n",
    "\n",
    "        # Generar un prompt basado en la categoría.\n",
    "        prompt_especifico = generar_prompt(categoria, pregunta_en_ingles)\n",
    "        logging.info(f\"Prompt específico generado: {prompt_especifico}\")\n",
    "\n",
    "        # Obtener el contexto relevante para la pregunta.\n",
    "        contexto = obtener_contexto(pregunta, index, trozos)\n",
    "        logging.debug(f\"Contexto recuperado: {contexto[:200] if contexto else 'Sin contexto'}\")\n",
    "        if not contexto:\n",
    "            logging.warning(\"No se encontró un contexto relevante para la pregunta.\")\n",
    "            respuesta = \"No pude encontrar información relevante para responder tu pregunta.\"\n",
    "            guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "            return respuesta\n",
    "\n",
    "        # Generar la respuesta final utilizando el contexto y el prompt.\n",
    "        respuesta = generar_respuesta(pregunta_en_ingles, contexto, prompt_especifico)\n",
    "        logging.debug(f\"Respuesta generada: {respuesta[:200] if respuesta else 'Sin respuesta'}\")\n",
    "\n",
    "        try:\n",
    "            # Traducir la respuesta al español.\n",
    "            respuesta_en_espanol = traducir(respuesta, \"español\")\n",
    "            logging.info(f\"Respuesta traducida al español: {respuesta_en_espanol}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al traducir la respuesta: {e}\")\n",
    "            respuesta_en_espanol = \"Lo siento, ocurrió un error al traducir la respuesta. A continuación, la respuesta en inglés:\\n\\n\" + respuesta\n",
    "\n",
    "        # Guardar la respuesta generada en el caché.\n",
    "        guardar_respuesta_cacheada(pregunta, respuesta_en_espanol)\n",
    "\n",
    "        return respuesta_en_espanol\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso de responder pregunta: {e}\")\n",
    "        return \"Ocurrió un error al procesar tu pregunta.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 12: Interfaz CLI**\n",
    "Proporciona una interfaz interactiva en línea de comandos para que los usuarios interactúen con el chatbot.\n",
    "\n",
    "- **Inicialización del Caché:** Crea el directorio de caché si no existe.\n",
    "- **Bucle Principal:**\n",
    "  - Permite al usuario ingresar preguntas.\n",
    "  - Responde saludos inmediatamente si se detectan.\n",
    "  - Recupera el índice y bloques relacionados con la enfermedad mencionada.\n",
    "  - Genera y muestra la respuesta al usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Asegurar que el directorio de caché exista.\n",
    "    os.makedirs(\"cache\", exist_ok=True)\n",
    "\n",
    "    # Mensaje de bienvenida e instrucciones para el usuario.\n",
    "    print(\"Bienvenido al chatbot de Ensayos Clínicos.\")\n",
    "    print(\"Conversemos sobre Ensayos Clínicos relacionados con las siguientes enfermedades neuromusculares:\")\n",
    "    print(\"- Distrofia Muscular de Duchenne o de Becker\")\n",
    "    print(\"- Enfermedad de Pompe\")\n",
    "    print(\"- Distrofia Miotónica\")\n",
    "    print(\"- Enfermedad de almacenamiento de glucógeno\")\n",
    "    print(\"Por favor, escribe tu pregunta indicando claramente la enfermedad sobre la que deseas información.\")\n",
    "    print(\"Escribí 'salir' para terminar la conversación.\")\n",
    "    \n",
    "    while True:\n",
    "        # Solicitar la pregunta al usuario\n",
    "        pregunta = input(\"Tu pregunta: \").strip()  # Recoge la entrada del usuario y elimina espacios innecesarios.\n",
    "\n",
    "        # Verificar si el usuario desea salir\n",
    "        if pregunta.lower() in ['salir', 'chau', 'exit', 'quit']:  # Frases que permiten terminar la sesión.\n",
    "            print(\"¡Chau!\")\n",
    "            logging.info(\"El usuario ha finalizado la sesión.\")\n",
    "            break \n",
    "\n",
    "        # Verificar si la entrada es un saludo\n",
    "        if es_saludo(pregunta):\n",
    "            respuesta_saludo = responder_saludo()\n",
    "            print(respuesta_saludo)\n",
    "            logging.info(\"Se detectó un saludo del usuario.\")\n",
    "            continue\n",
    "\n",
    "        # Normalizar la pregunta\n",
    "        pregunta_normalizada = normalizar_texto(pregunta)\n",
    "        logging.info(f\"Pregunta procesada: {pregunta_normalizada}\")\n",
    "        logging.debug(f\"Pregunta original: {pregunta}\")\n",
    "\n",
    "        # Identificar la enfermedad relacionada con la pregunta\n",
    "        idn = doc_enfermedad(pregunta_normalizada)\n",
    "        if idn is None:\n",
    "            logging.warning(f\"No se pudo identificar la enfermedad para la pregunta: '{pregunta}'\")\n",
    "            respuesta = \"Lo siento, no pude identificar la enfermedad relacionada con tu pregunta.\"\n",
    "            print(f\"Respuesta: {respuesta}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            index = index_archivos[idn]\n",
    "            trozos = trozos_archivos[idn]\n",
    "\n",
    "            # Generar la respuesta para la pregunta del usuario\n",
    "            respuesta = responder_pregunta(pregunta_normalizada, index, trozos)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al generar la respuesta para la pregunta '{pregunta}': {e}\")\n",
    "            respuesta = \"Lo siento, ocurrió un error al generar la respuesta.\"\n",
    "\n",
    "        print(f\"Respuesta: {respuesta}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

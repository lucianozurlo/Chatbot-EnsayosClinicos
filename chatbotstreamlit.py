import os
import json
import logging
import hnswlib
from sentence_transformers import SentenceTransformer, util
import numpy as np
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
from llama_index.llms.gemini import Gemini
from llama_index.core.llms import ChatMessage
import time
import hashlib
import streamlit as st
import random

# Configuraci√≥n de logs para imprimir todo en consola
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)

logging.info("Librer√≠as importadas correctamente.")

# Cargar variables de entorno desde un archivo .env
load_dotenv()
logging.info("Variables de entorno cargadas desde el archivo .env.")

# Definici√≥n de clases necesarias

class Document:
    def __init__(self, text, metadata=None):
        """
        Inicializa un documento con su contenido y metadatos.
        
        Args:
            text (str): Texto del documento.
            metadata (dict, optional): Metadatos asociados al documento.
        """
        self.page_content = text
        self.metadata = metadata or {}
    
    def __str__(self):
        """
        Representaci√≥n en string del documento.
        
        Returns:
            str: Informaci√≥n formateada del documento.
        """
        return (
            f"T√≠tulo: {self.metadata.get('Title', 'N/A')}\n"
            f"Resumen: {self.metadata.get('Summary', 'N/A')}\n"
            f"Tipo de Estudio: {self.metadata.get('StudyType', 'N/A')}\n"
            f"Paises donde se desarrolla el estudio: {self.metadata.get('Countries', 'N/A')}\n"
            f"Fase en que se encuentra el estudio: {self.metadata.get('Phases', 'N/A')}\n"
            f"Identificaci√≥n en ClinicaTrial: {self.metadata.get('IDestudio', 'N/A')}.\n\n"
        )

class HNSWIndex:
    def __init__(self, embeddings, metadata=None, space='cosine', ef_construction=200, M=16):
        """
        Inicializa el √≠ndice HNSWlib con los embeddings proporcionados.
        
        Args:
            embeddings (np.ndarray): Matriz de embeddings.
            metadata (list, optional): Lista de metadatos asociados a cada embedding.
            space (str, optional): Espacio m√©trico para HNSWlib.
            ef_construction (int, optional): Par√°metro ef para la construcci√≥n del √≠ndice.
            M (int, optional): Par√°metro M para HNSWlib.
        """
        self.dimension = embeddings.shape[1]
        self.index = hnswlib.Index(space=space, dim=self.dimension)
        self.index.init_index(max_elements=embeddings.shape[0], ef_construction=ef_construction, M=M)
        self.index.add_items(embeddings, np.arange(embeddings.shape[0]))
        self.index.set_ef(50)  # Par√°metro ef para consultas
        self.metadata = metadata or []
    
    def similarity_search(self, query_vector, k=5):
        """
        Realiza una b√∫squeda de los k vecinos m√°s similares.
        
        Args:
            query_vector (np.ndarray): Vector de consulta.
            k (int, optional): N√∫mero de vecinos a buscar.
        
        Returns:
            list: Lista de tuplas con metadatos y distancias.
        """
        labels, distances = self.index.knn_query(query_vector, k=k)
        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(labels[0])]

@st.cache_resource
def cargar_modelo_embeddings(model_name):
    model = SentenceTransformer(model_name)
    return model

@st.cache_data
def cargar_y_procesar_documentos(ruta_fuente, _model):
    documentos = load_documents(ruta_fuente, is_directory=True)
    logging.info(f"Se cargaron {len(documentos)} documentos exitosamente.")
    
    # Precomputar los embeddings de los nombres de archivo para eficiencia
    archivos = [doc['filename'] for doc in documentos]
    archivos_embeddings = _model.encode(archivos)
    
    # Procesar todos los documentos y crear sus respectivos √≠ndices
    trozos_archivos = []
    index_archivos = []
    for i in range(len(documentos)):
        trozos, index = desdobla_doc(documentos[i], _model)
        trozos_archivos.append(trozos)
        index_archivos.append(index)
    
    logging.info("√çndices HNSWlib creados para todos los documentos.")
    
    return documentos, archivos, archivos_embeddings, trozos_archivos, index_archivos

@st.cache_resource
def configurar_gemini_cached():
    return configure_gemini()

def load_documents(source, is_directory=False):
    """
    Carga documentos desde un archivo o directorio.
    
    Args:
        source (str): Ruta al archivo o directorio.
        is_directory (bool): Indica si la fuente es un directorio.
    
    Returns:
        list: Lista de diccionarios con 'filename' y 'content'.
    """
    if not os.path.exists(source):
        logging.error(f"La fuente '{source}' no existe.")
        raise FileNotFoundError(f"La fuente '{source}' no se encontr√≥.")

    loaded_files = []
    if is_directory:
        logging.info(f"Iniciando carga desde el directorio: {source}.")
        for filename in os.listdir(source):
            filepath = os.path.join(source, filename)
            if os.path.isfile(filepath) and filepath.endswith(('.txt', '.json', '.pdf')):
                content = extract_content(filepath)
                if content:
                    loaded_files.append({"filename": filename, "content": content})
                    logging.info(f"Archivo '{filename}' cargado correctamente.")
    else:
        logging.info(f"Iniciando carga del archivo: {source}.")
        content = extract_content(source)
        if content:
            loaded_files.append({"filename": os.path.basename(source), "content": content})
            logging.info(f"Archivo '{os.path.basename(source)}' cargado correctamente.")

    logging.info(f"{len(loaded_files)} documentos cargados.")
    return loaded_files

def extract_content(filepath):
    """
    Extrae el contenido del archivo seg√∫n su tipo.
    
    Args:
        filepath (str): Ruta al archivo.
    
    Returns:
        list o dict o str: Contenido procesado del archivo.
    """
    try:
        if filepath.endswith('.txt'):
            with open(filepath, 'r', encoding='utf-8') as file:
                content = file.read()
            units = content.split("\n-----\n")
            return units
        elif filepath.endswith('.json'):
            with open(filepath, 'r', encoding='utf-8') as file:
                data = json.load(file)
            return data
        elif filepath.endswith('.pdf'):
            reader = PdfReader(filepath)
            return ''.join(page.extract_text() or '' for page in reader.pages)
    except Exception as e:
        logging.error(f"Error al extraer contenido de '{filepath}': {e}")
        return None

def desdobla_doc(data2, model):
    """
    Desdobla el contenido del documento en varios `Document` con metadatos.
    Maneja JSON (asumiendo estructura de ensayos cl√≠nicos) o texto/PDF gen√©rico.
    
    Args:
        data2 (dict): Diccionario con 'filename' y 'content'.
        model (SentenceTransformer): Modelo para generar embeddings.
    
    Returns:
        tuple: Lista de `Document` y instancia de `HNSWIndex`.
    """
    documents = []
    summaries = []
    contenido = data2['content']
    
    if isinstance(contenido, list):
        for entry in contenido:
            if isinstance(entry, dict):
                nctId = entry.get("IDestudio", "")
                briefTitle = entry.get("Title", "")
                summary = entry.get("Summary", "")
                studyType = entry.get("StudyType", "")
                country = entry.get("Countries", "")
                overallStatus = entry.get("OverallStatus", "")
                conditions = entry.get("Conditions", "")
                phases = entry.get("Phases", "")

                # Crear resumen en ingl√©s para consistencia interna
                Summary = (
                    f"The study titled '{briefTitle}', of type '{studyType}', "
                    f"investigates the condition(s): {conditions}. "
                    f"Brief summary: {summary}. "
                    f"Current status: {overallStatus}, taking place in {country}. "
                    f"The study is classified under: {phases} phase. "
                    f"For more info, search {nctId} on ClinicalTrials."
                )
                metadata = {
                    "Title": briefTitle,
                    "Summary": Summary,
                    "StudyType": studyType,
                    "Countries": country,
                    "Phases": phases,
                    "IDestudio": nctId
                }
                doc = Document(Summary, metadata)
                documents.append(doc)
                summaries.append(Summary)
            else:
                # Si no es dict, tratar la entrada como texto gen√©rico
                texto = str(entry)
                metadata = {"Summary": texto}
                doc = Document(texto, metadata)
                documents.append(doc)
                summaries.append(texto)
    else:
        # Texto gen√©rico (PDF o TXT)
        texto = str(contenido)
        metadata = {"Summary": texto}
        doc = Document(texto, metadata)
        documents.append(doc)
        summaries.append(texto)

    if documents:
        embeddings = model.encode([doc.page_content for doc in documents], show_progress_bar=False)
        embeddings = np.array(embeddings).astype(np.float32)
        vector_store = HNSWIndex(embeddings, metadata=[doc.metadata for doc in documents])
    else:
        vector_store = None

    return documents, vector_store

def configure_gemini():
    """
    Configura la instancia de Gemini usando la clave API.
    
    Returns:
        Gemini: Instancia configurada del modelo Gemini.
    """
    api_key = os.getenv("GEMINI_API_KEY")
    if not api_key:
        logging.error("La clave API de Gemini no est√° configurada.")
        raise EnvironmentError("Configura GEMINI_API_KEY en tu archivo .env.")
    gemini = Gemini(api_key=api_key)
    logging.info("Gemini configurado correctamente.")
    return gemini

def traducir(texto, idioma_destino, gemini_llm):
    """
    Traduce texto al idioma especificado usando el modelo Gemini.
    Sin detecci√≥n de idioma.
    
    Args:
        texto (str): Texto a traducir.
        idioma_destino (str): Idioma de destino.
        gemini_llm (Gemini): Instancia del modelo Gemini.
    
    Returns:
        str: Texto traducido o original en caso de fallo.
    """
    start_time = time.time()
    mensajes = [
        ChatMessage(role="system", content="Act√∫a como un traductor."),
        ChatMessage(role="user", content=f"Por favor, traduce este texto al {idioma_destino}: {texto}")
    ]
    try:
        respuesta = gemini_llm.chat(mensajes)
        elapsed_time = time.time() - start_time
        logging.info(f"Traducci√≥n completada en {elapsed_time:.2f} segundos.")
        return respuesta.message.content.strip()
    except Exception as e:
        logging.error(f"Error al traducir: {e}")
        return texto  # fallback

def generate_embedding(texto, model, embedding_cache):
    """
    Genera un embedding para el texto utilizando el modelo de embeddings.
    Usa cach√© para evitar recalcular embeddings de textos repetidos.
    
    Args:
        texto (str): Texto para generar el embedding.
        model (SentenceTransformer): Modelo para generar embeddings.
        embedding_cache (dict): Cach√© de embeddings.
    
    Returns:
        np.ndarray: Embedding generado o vector de ceros en caso de fallo.
    """
    if texto in embedding_cache:
        logging.info(f"Embedding obtenido del cach√© para el texto: {texto}")
        return embedding_cache[texto]
    try:
        embedding = model.encode([texto])
        embedding_cache[texto] = embedding
        logging.info(f"Embedding generado para el texto: {texto}")
        return embedding
    except Exception as e:
        logging.error(f"Error al generar el embedding: {e}")
        # Devuelve embedding vac√≠o como fallback
        return np.zeros((1, 384))

def obtener_contexto(pregunta, index, trozos, model, gemini_llm, embedding_cache, top_k=50):
    """
    Recupera los trozos de texto m√°s relevantes para responder la pregunta.
    Traduce la pregunta al ingl√©s antes de buscar en el √≠ndice.
    
    Args:
        pregunta (str): Pregunta del usuario.
        index (HNSWIndex): √çndice de HNSWlib para buscar similitudes.
        trozos (list): Lista de `Document` relacionados.
        model (SentenceTransformer): Modelo para generar embeddings.
        gemini_llm (Gemini): Instancia del modelo Gemini.
        embedding_cache (dict): Cach√© de embeddings.
        top_k (int, optional): N√∫mero de resultados a recuperar.
    
    Returns:
        str: Contexto relevante concatenado.
    """
    try:
        # Traducir la pregunta al ingl√©s
        pregunta_en_ingles = traducir(pregunta, "ingl√©s", gemini_llm)
        logging.info(f"Pregunta traducida al ingl√©s: {pregunta_en_ingles}")

        # Generar embedding de la pregunta traducida
        pregunta_emb = generate_embedding(pregunta_en_ingles, model, embedding_cache)
        logging.info("Embedding generado para la pregunta.")

        # Buscar en el √≠ndice
        results = index.similarity_search(pregunta_emb, k=top_k)
        texto = ""
        for entry in results:
            resum = entry[0]["Summary"]
            texto += resum + "\n"

        logging.info("Contexto relevante recuperado para la pregunta.")
        return texto
    except Exception as e:
        logging.error(f"Error al obtener el contexto: {e}")
        return ""

def categorizar_pregunta(pregunta):
    """
    Clasifica la pregunta en categor√≠as basadas en palabras clave.
    
    Args:
        pregunta (str): Pregunta del usuario.
    
    Returns:
        str: Categor√≠a identificada.
    """
    categorias = {
        "tratamiento": ["tratamiento", "medicaci√≥n", "cura", "terapia", "f√°rmaco"],
        "ensayo": ["ensayo", "estudio", "prueba", "investigaci√≥n", "trial"],
        "resultado": ["resultado", "efectividad", "resultados", "√©xito", "fracaso"],
        "prevenci√≥n": ["prevenci√≥n", "previene", "evitar", "reducci√≥n de riesgo"]
    }
    for categoria, palabras in categorias.items():
        if any(palabra in pregunta.lower() for palabra in palabras):
            return categoria
    return "general"

def generar_prompt(categoria, pregunta):
    """
    Genera un prompt espec√≠fico basado en la categor√≠a de la pregunta.
    
    Args:
        categoria (str): Categor√≠a de la pregunta.
        pregunta (str): Pregunta del usuario.
    
    Returns:
        str: Prompt generado.
    """
    prompts = {
        "tratamiento": f"Proporciona informaci√≥n sobre tratamientos en ensayos cl√≠nicos relacionados con: {pregunta}.",
        "ensayo": f"Describe los ensayos cl√≠nicos actuales relacionados con: {pregunta}.",
        "resultado": f"Explica los resultados m√°s recientes de ensayos cl√≠nicos sobre: {pregunta}.",
        "prevenci√≥n": f"Ofrece informaci√≥n sobre prevenci√≥n y ensayos cl√≠nicos para: {pregunta}."
    }
    return prompts.get(categoria, "Por favor, responde la pregunta sobre ensayos cl√≠nicos.")

def es_saludo(pregunta):
    """
    Verifica si la pregunta del usuario es un saludo.
    
    Args:
        pregunta (str): Pregunta del usuario.
    
    Returns:
        bool: True si es un saludo, False de lo contrario.
    """
    saludos = ["hola", "buen d√≠a", "buenas", "c√≥mo est√°s", "c√≥mo te llamas", "qu√© tal", "est√°s bien", "buenas tardes", "buenas noches"]
    return any(saludo in pregunta.lower() for saludo in saludos)

def responder_saludo():
    """
    Genera una respuesta aleatoria a un saludo.
    
    Returns:
        str: Respuesta de saludo.
    """
    saludos_respuestas = [
        "¬°Hola! Estoy para ayudarte con informaci√≥n sobre ensayos cl√≠nicos. ¬øEn qu√© puedo asistirte hoy?",
        "¬°Buenas! ¬øTienes alguna pregunta sobre ensayos cl√≠nicos en enfermedades neuromusculares?",
        "¬°Hola! ¬øC√≥mo puedo ayudarte con tus consultas sobre ensayos cl√≠nicos?"
    ]
    return random.choice(saludos_respuestas)

def generar_respuesta(pregunta, contexto, prompt_especifico, gemini_llm, model, embedding_cache):
    """
    Genera una respuesta usando el contexto proporcionado y un prompt espec√≠fico.
    Primero genera la respuesta en ingl√©s, luego la traduce al espa√±ol.
    
    Args:
        pregunta (str): Pregunta del usuario.
        contexto (str): Contexto relevante recuperado.
        prompt_especifico (str): Prompt adaptado a la categor√≠a de la pregunta.
        gemini_llm (Gemini): Instancia del modelo Gemini.
        model (SentenceTransformer): Modelo para generar embeddings.
        embedding_cache (dict): Cach√© de embeddings.
    
    Returns:
        str: Respuesta generada en espa√±ol.
    """
    mensajes = [
        ChatMessage(role="system", content="Eres un experto m√©dico."),
        ChatMessage(role="user", content=f"{prompt_especifico}\nContexto: {contexto}\nPregunta: {pregunta}")
    ]
    start_time = time.time()
    try:
        respuesta = gemini_llm.chat(mensajes)
        elapsed_time = time.time() - start_time
        logging.info(f"Respuesta generada en ingl√©s en {elapsed_time:.2f} segundos.")
        # Traducir la respuesta al espa√±ol
        respuesta_en_espanol = traducir(respuesta.message.content, "espa√±ol", gemini_llm)
        logging.info("Respuesta traducida al espa√±ol.")
        return respuesta_en_espanol
    except Exception as e:
        logging.error(f"Error al generar la respuesta: {e}")
        return "Lo siento, ocurri√≥ un error al generar la respuesta."

def generar_hash(pregunta):
    """
    Genera un hash SHA-256 para una pregunta dada.
    
    Args:
        pregunta (str): Pregunta del usuario.
    
    Returns:
        str: Hash generado.
    """
    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()

def obtener_respuesta_cacheada(pregunta):
    """
    Obtiene una respuesta cacheada para una pregunta si existe.
    
    Args:
        pregunta (str): Pregunta del usuario.
    
    Returns:
        str o None: Respuesta cacheada o None si no existe.
    """
    hash_pregunta = generar_hash(pregunta)
    archivo_cache = f"cache/{hash_pregunta}.json"
    if os.path.exists(archivo_cache):
        try:
            with open(archivo_cache, "r", encoding='utf-8') as f:
                datos = json.load(f)
                return datos.get("respuesta", None)
        except Exception as e:
            logging.error(f"Error al leer el cach√©: {e}")
            return None
    return None

def guardar_respuesta_cacheada(pregunta, respuesta):
    """
    Guarda una respuesta en cach√© para una pregunta dada.
    
    Args:
        pregunta (str): Pregunta del usuario.
        respuesta (str): Respuesta generada.
    """
    hash_pregunta = generar_hash(pregunta)
    archivo_cache = f"cache/{hash_pregunta}.json"
    try:
        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)
        with open(archivo_cache, "w", encoding='utf-8') as f:
            json.dump({"pregunta": pregunta, "respuesta": respuesta}, f, ensure_ascii=False, indent=4)
        logging.info(f"Respuesta cacheada para la pregunta: '{pregunta}'")
    except Exception as e:
        logging.error(f"Error al guardar la respuesta en cach√©: {e}")

def responder_pregunta(pregunta, index, trozos, model, gemini_llm, embedding_cache):
    """
    Integra categorizaci√≥n, obtenci√≥n de contexto y generaci√≥n de respuesta.
    Incluye manejo de cach√© para respuestas repetidas.
    
    Args:
        pregunta (str): Pregunta del usuario.
        index (HNSWIndex): √çndice de HNSWlib para b√∫squeda de contexto.
        trozos (list): Lista de `Document` relacionados.
        model (SentenceTransformer): Modelo para generar embeddings.
        gemini_llm (Gemini): Instancia del modelo Gemini.
        embedding_cache (dict): Cach√© de embeddings.
    
    Returns:
        str: Respuesta generada.
    """
    try:
        if index is None or not trozos:
            logging.warning("No se encontraron √≠ndices o trozos para esta pregunta.")
            return "No se encontr√≥ informaci√≥n para responder tu pregunta."

        # Verificar cach√©
        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)
        if respuesta_cacheada:
            logging.info(f"Respuesta obtenida del cach√© para: '{pregunta}'")
            return respuesta_cacheada

        # Categorizar la pregunta
        categoria = categorizar_pregunta(pregunta)
        logging.info(f"Categor√≠a de la pregunta: {categoria}")

        # Generar prompt espec√≠fico
        prompt_especifico = generar_prompt(categoria, pregunta)
        logging.info(f"Prompt espec√≠fico: {prompt_especifico}")

        # Obtener contexto relevante
        contexto = obtener_contexto(pregunta, index, trozos, model, gemini_llm, embedding_cache)
        if not contexto.strip():
            logging.warning("No se encontr√≥ contexto relevante.")
            respuesta = "No pude encontrar informaci√≥n relevante para responder tu pregunta."
            guardar_respuesta_cacheada(pregunta, respuesta)
            return respuesta

        # Generar la respuesta
        respuesta = generar_respuesta(pregunta, contexto, prompt_especifico, gemini_llm, model, embedding_cache)

        # Guardar la respuesta en cach√©
        guardar_respuesta_cacheada(pregunta, respuesta)
        return respuesta
    except Exception as e:
        logging.error(f"Error en el proceso de responder pregunta: {e}")
        return "Ocurri√≥ un error al procesar tu pregunta."

def doc_enfermedad(pregunta):
    """
    Identifica el √≠ndice del documento m√°s relevante para la enfermedad en la pregunta.
    Utiliza embeddings precomputados de los nombres de archivo.
    
    Args:
        pregunta (str): Pregunta del usuario.
    
    Returns:
        int: √çndice del documento m√°s relevante.
    """
    if not documentos:
        logging.warning("No se encontraron documentos. √çndice por defecto: 0.")
        return 0

    # Generar embedding de la pregunta
    preg_embedding = model.encode(pregunta)

    # Calcular similitudes con los embeddings de los nombres de archivo
    similarities = [util.cos_sim(preg_embedding, emb).item() for emb in archivos_embeddings]

    # Obtener el √≠ndice con mayor similitud
    max_index = similarities.index(max(similarities))
    return max_index

# Cargar y procesar documentos usando funciones cacheadas
ruta_fuente = 'data'  # Aseg√∫rate de tener una carpeta 'data' con los documentos
model = cargar_modelo_embeddings("all-MiniLM-L6-v2")
documentos, archivos, archivos_embeddings, trozos_archivos, index_archivos = cargar_y_procesar_documentos(ruta_fuente, model)

# Configurar la clave API de Gemini usando funci√≥n cacheada
gemini_llm = configurar_gemini_cached()

# Inicializar cach√©s
embedding_cache = {}
translation_cache = {}

# Crear directorio de cach√© si no existe
os.makedirs("cache", exist_ok=True)

# Inicializar historial en el estado de Streamlit
if 'historial' not in st.session_state:
    st.session_state.historial = []

# Configurar la p√°gina para usar toda la anchura disponible
st.set_page_config(page_title="ü§ñ Chatbot de Ensayos Cl√≠nicos", layout="wide")

# T√≠tulo de la aplicaci√≥n
st.title("ü§ñ Chatbot de Ensayos Cl√≠nicos")

# Descripci√≥n
st.markdown("""
Bienvenido al **Chatbot de Ensayos Cl√≠nicos**.
Conversemos sobre ensayos cl√≠nicos en enfermedades neuromusculares 
(Distrofia Muscular de Duchenne o Becker, Enfermedad de Pompe, Distrofia Miot√≥nica, etc.).

**Instrucciones:**
- Escribe tu pregunta en el campo de abajo, indicando la enfermedad sobre la que quieres informaci√≥n.
- El historial de la conversaci√≥n aparecer√° arriba.
""")

# Crear contenedores para el historial y el input
historial_container = st.container()
input_container = st.container()

with historial_container:
    st.markdown("### üó®Ô∏è Historial de Conversaci√≥n")
    st.write("")  # Espacio adicional

    # A√±adir un contenedor con scroll para el historial
    scrollable_area = st.empty()

    # Funci√≥n para renderizar el historial
    def render_historial():
        with scrollable_area.container():
            st.markdown('<div class="scrollable-content">', unsafe_allow_html=True)
            for sender, message in st.session_state.historial:
                if sender == "Usuario":
                    st.markdown(f"<p style='text-align: right;'><strong>üßë T√∫:</strong> {message}</p>", unsafe_allow_html=True)
                else:
                    st.markdown(f"<p style='text-align: left;'><strong>ü§ñ Chatbot:</strong> {message}</p>", unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)

    # Renderizar el historial inicial
    render_historial()

with input_container:
    st.markdown("---")  # L√≠nea divisoria
    # Aplicar estilos CSS para el √°rea de chat con scroll
    st.markdown("""
    <style>
    .scrollable-content {
        height: 500px;
        overflow-y: auto;
        padding: 10px;
        background-color: #F5F5F5;
        border-radius: 5px;
    }
    </style>
    """, unsafe_allow_html=True)

    # Entrada de usuario
    col1, col2 = st.columns([9, 1])
    with col1:
        pregunta = st.text_input("Tu pregunta:", key="input_pregunta")
    with col2:
        enviar = st.button("Enviar", use_container_width=True)

    if enviar:
        if not pregunta.strip():
            st.warning("‚ö†Ô∏è Por favor, ingresa una pregunta.")
        else:
            if es_saludo(pregunta):
                respuesta_saludo = responder_saludo()
                st.session_state.historial.append(("Usuario", pregunta))
                st.session_state.historial.append(("Chatbot", respuesta_saludo))
            else:
                # Identificar la enfermedad (documento m√°s relevante)
                idn = doc_enfermedad(pregunta)
                index = index_archivos[idn] if idn < len(index_archivos) else None
                trozos = trozos_archivos[idn] if idn < len(trozos_archivos) else []

                # Responder la pregunta
                respuesta = responder_pregunta(pregunta, index, trozos, model, gemini_llm, embedding_cache)
                st.session_state.historial.append(("Usuario", pregunta))
                st.session_state.historial.append(("Chatbot", respuesta))

        # Limpiar el campo de entrada despu√©s de enviar
        st.session_state.input_pregunta = ""

        # Renderizar el historial actualizado
        render_historial()

    # Actualizar el scroll al final despu√©s de cada mensaje
    st.markdown("""
    <script>
    var scrollableDiv = document.querySelector('.scrollable-content');
    if(scrollableDiv){
        scrollableDiv.scrollTop = scrollableDiv.scrollHeight;
    }
    </script>
    """, unsafe_allow_html=True)

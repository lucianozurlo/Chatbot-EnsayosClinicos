{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 0: Verificar versión de Python**\n",
    "\n",
    "- **Verificación de la Versión de Python:** Se utiliza `print(sys.version)` para mostrar la versión actual de Python instalada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  # Acceder a la información de la versión de Python.\n",
    "import os  # Manejo de rutas, archivos y operaciones del sistema.\n",
    "import logging  # Configuración y uso de logs para monitorear la ejecución.\n",
    "\n",
    "# Configurar la variable de entorno para desactivar la paralelización de tokenizadores y evitar la advertencia de Huggingface\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Configuración de logging para monitorear la ejecución del programa.\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Define el nivel INFO para capturar detalles importantes pero no excesivos.\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',  # Especifica un formato estándar para los mensajes de log.\n",
    "    handlers=[logging.StreamHandler()],  # Envía los logs directamente a la consola.\n",
    "    force=True  # Forzar la reconfiguración de logging, incluso si ya fue configurado previamente.\n",
    ")\n",
    "\n",
    "# Definir la versión requerida de Python\n",
    "REQUIRED_VERSION = (3, 10, 12)\n",
    "current_version = sys.version_info\n",
    "\n",
    "# Validar la versión de Python\n",
    "if (current_version.major, current_version.minor, current_version.micro) != REQUIRED_VERSION:\n",
    "    logging.warning(\"\"\"\n",
    "    **********************************************\n",
    "    ** Advertencia: Versión de Python no compatible **\n",
    "    **********************************************\n",
    "    Este chatbot está optimizado para Python 3.10.12.\n",
    "    La versión actual es Python {}.{}.{}.\n",
    "    Algunas funcionalidades pueden no funcionar correctamente.\n",
    "    **********************************************\n",
    "    \"\"\".format(current_version.major, current_version.minor, current_version.micro))\n",
    "else:\n",
    "    logging.info(\"\"\"\n",
    "    **********************************************\n",
    "    ** Versión de Python compatible **\n",
    "    **********************************************\n",
    "    Python 3.10.12 detectado correctamente.\n",
    "    Todas las funcionalidades deberían operar sin problemas.\n",
    "    **********************************************\n",
    "    \"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 1: Instalación de Paquetes Necesarios**\n",
    "Se instalan las bibliotecas necesarias para que el chatbot funcione correctamente.\n",
    "\n",
    "- **Transformers (`transformers`)**: Para el procesamiento de lenguaje natural.\n",
    "- **Sentence Transformers (`sentence_transformers`)**: Para crear embeddings eficientes de texto.\n",
    "- **HNSWlib (`hnswlib`)**: Realiza búsquedas rápidas de vecinos más cercanos.\n",
    "- **Numpy (`numpy<2.0`)**: Utiliza una versión compatible para operaciones matemáticas.\n",
    "- **PyPDF2 (`PyPDF2`)**: Manejo y extracción de texto desde archivos PDF.\n",
    "- **Dotenv (`python-dotenv`)**: Gestiona variables de entorno desde un archivo `.env`.\n",
    "- **Tenacity (`tenacity`)**: Manejo de reintentos con lógica exponencial.\n",
    "- **Llama Index (`llama-index` y extensiones para Gemini)**: Proporciona integración con el modelo Gemini.\n",
    "- **Tqdm (`tqdm`)**: Barra de progreso visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalación de bibliotecas necesarias\n",
    "%pip install transformers  # Instala la biblioteca Transformers para modelado de lenguaje natural y generación de texto.\n",
    "%pip install sentence_transformers  # Añade soporte para modelos preentrenados de embeddings de texto.\n",
    "%pip install hnswlib  # Instala HNSWlib para realizar búsquedas rápidas de vecinos más cercanos.\n",
    "%pip install numpy<2.0  # Especifica la instalación de una versión compatible de Numpy (menor que 2.0) para evitar conflictos.\n",
    "%pip install PyPDF2  # Instala PyPDF2 para manejo de archivos PDF y extracción de texto.\n",
    "%pip install python-dotenv  # Añade soporte para cargar y gestionar variables de entorno desde un archivo `.env`.\n",
    "%pip install tenacity  # Proporciona herramientas para manejar reintentos de llamadas API con lógica exponencial.\n",
    "%pip install llama-index  # Instala llama-index, una biblioteca para integrar y estructurar datos con modelos de lenguaje.\n",
    "%pip install llama-index-llms-gemini  # Extiende llama-index para trabajar con Gemini como modelo de lenguaje.\n",
    "%pip install llama-index-embeddings-gemini  # Permite generar y utilizar embeddings con el modelo Gemini.\n",
    "%pip install tqdm  # Añade barras de progreso visuales para mejorar la experiencia del usuario durante procesos largos.\n",
    "%pip install unidecode  # Normaliza texto eliminando acentos y otros caracteres no ASCII."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 2: Importar Librerías y Configurar Logging**\n",
    "Se importan todas las librerías necesarias y se configura un sistema de logs para monitorear el flujo del programa.\n",
    "\n",
    "- **Importación de Librerías:** Incluye módulos estándar como `os`, `json`, y `logging`, y bibliotecas específicas del proyecto.\n",
    "- **Configuración del Logging:** \n",
    "  - Configura un formato estándar para los mensajes de log.\n",
    "  - Establece que los mensajes se impriman directamente en la consola.\n",
    "  - Define el nivel de logging como `INFO` para capturar detalles esenciales del flujo.\n",
    "- **Carga de Variables de Entorno:**\n",
    "  - Usa `load_dotenv()` para cargar claves de API u otras configuraciones sensibles desde un archivo `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías esenciales para el funcionamiento del chatbot.\n",
    "import os  # Manejo de rutas, archivos y operaciones del sistema.\n",
    "import json  # Manipulación de datos en formato JSON.\n",
    "import logging  # Configuración y uso de logs para monitorear la ejecución.\n",
    "import hnswlib  # Búsqueda eficiente de similitud utilizando índices de alta dimensionalidad.\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # Modelos preentrenados y tokenizadores para procesamiento de texto.\n",
    "from sentence_transformers import SentenceTransformer, util  # Embeddings de texto y cálculo de similitud.\n",
    "import numpy as np  # Operaciones matemáticas avanzadas y estructuras de datos.\n",
    "from dotenv import load_dotenv  # Carga de variables de entorno desde un archivo `.env`.\n",
    "from PyPDF2 import PdfReader  # Extracción de texto de documentos PDF.\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt  # Gestión de reintentos en funciones críticas.\n",
    "from llama_index.llms.gemini import Gemini  # Interfaz para el modelo de lenguaje Gemini.\n",
    "from llama_index.core.llms import ChatMessage  # Estructuras de mensajes para interacción con LLMs.\n",
    "import time  # Manejo de tiempos y medición de duración de procesos.\n",
    "import hashlib  # Generación de hashes únicos para almacenamiento en caché.\n",
    "import random  # Generación de valores aleatorios, útil para respuestas personalizadas.\n",
    "import unicodedata  # Normaliza texto de preguntas\n",
    "\n",
    "logging.info(\"Librerías importadas correctamente.\")  # Log para confirmar la importación exitosa de todas las librerías.\n",
    "\n",
    "# Carga de variables de entorno desde un archivo .env para proteger información sensible como claves de API.\n",
    "load_dotenv()  # Carga las variables de entorno necesarias.\n",
    "logging.info(\"Variables de entorno cargadas desde el archivo .env.\")  # Log para confirmar que las variables se cargaron correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 3: Cargar Documentos**\n",
    "Se cargan documentos desde archivos o directorios para analizarlos y extraer contenido relevante.\n",
    "\n",
    "- **Función `load_documents`:**\n",
    "  - Permite cargar documentos de varios formatos (`.txt`, `.json`, `.pdf`).\n",
    "  - Verifica si la fuente existe y lanza un error si no es válida.\n",
    "  - Itera sobre los archivos en un directorio o procesa un único archivo.\n",
    "- **Función `extract_content`:**\n",
    "  - Maneja la lógica para extraer contenido dependiendo del formato:\n",
    "    - **TXT:** Divide el contenido en bloques usando delimitadores.\n",
    "    - **JSON:** Carga y devuelve el contenido en formato de diccionario.\n",
    "    - **PDF:** Extrae texto de todas las páginas utilizando `PdfReader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_texto(texto):\n",
    "    \"\"\"\n",
    "    Normaliza el texto eliminando tildes, caracteres especiales, y convirtiendo a minúsculas.\n",
    "    \"\"\"\n",
    "    # Convertir a minúsculas.\n",
    "    texto = texto.lower()\n",
    "    # Eliminar tildes y normalizar caracteres unicode.\n",
    "    texto = unicodedata.normalize('NFD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Eliminar caracteres que no sean alfanuméricos o espacios.\n",
    "    texto = ''.join(char for char in texto if char.isalnum() or char.isspace())\n",
    "    return texto.strip()  # Eliminar espacios extra.\n",
    "\n",
    "def load_documents(source, is_directory=False):\n",
    "    \"\"\"\n",
    "    Carga documentos desde un archivo o un directorio. Soporta .txt, .json y .pdf.\n",
    "    \"\"\"\n",
    "    loaded_files = []\n",
    "    if is_directory:\n",
    "        for filename in os.listdir(source):\n",
    "            filepath = os.path.join(source, filename)\n",
    "            if os.path.isfile(filepath) and filepath.endswith(('.txt', '.json', '.pdf')):\n",
    "                content = extract_content(filepath)\n",
    "                if content:\n",
    "                    loaded_files.append({\"filename\": normalizar_texto(filename), \"content\": content})\n",
    "    else:\n",
    "        content = extract_content(source)\n",
    "        if content:\n",
    "            loaded_files.append({\"filename\": normalizar_texto(os.path.basename(source)), \"content\": content})\n",
    "    return loaded_files\n",
    "\n",
    "def extract_content(filepath):\n",
    "    \"\"\"\n",
    "    Extrae el contenido del archivo según su tipo (.txt, .json, .pdf).\n",
    "    Si el archivo es .txt, lo divide en unidades por el delimitador '\\n-----\\n'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if filepath.endswith('.txt'):  # Si es un archivo de texto:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:  # Abre el archivo con codificación UTF-8.\n",
    "                content = file.read()  # Lee el contenido del archivo.\n",
    "            # Dividir el contenido en unidades por el delimitador\n",
    "            units = content.split(\"\\n-----\\n\")  # Divide el texto en bloques por el delimitador.\n",
    "            return units  # Devuelve una lista de bloques de texto.\n",
    "        elif filepath.endswith('.json'):  # Si es un archivo JSON:\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:  # Abre el archivo con codificación UTF-8.\n",
    "                return json.load(file)  # Carga y retorna el contenido en formato JSON.\n",
    "        elif filepath.endswith('.pdf'):  # Si es un archivo PDF:\n",
    "            reader = PdfReader(filepath)  # Instancia el lector PDF.\n",
    "            return ''.join(page.extract_text() or '' for page in reader.pages)  # Extrae y concatena el texto de todas las páginas.\n",
    "    except Exception as e:  # Captura cualquier error que ocurra durante la extracción.\n",
    "        logging.error(f\"Error al extraer contenido de '{filepath}': {e}\")  # Log del error.\n",
    "        return None  # Retorna `None` si ocurre un error.\n",
    "\n",
    "# Cargar documentos (True: busca en directorio, False: busca archivo)\n",
    "ruta_fuente = 'data'  # Define la ruta del directorio de documentos.\n",
    "documentos = load_documents(ruta_fuente, is_directory=True)  # Llama a la función para cargar documentos desde el directorio.\n",
    "\n",
    "# Visualizar cuántos documentos fueron cargados\n",
    "logging.info(f\"Se cargaron {len(documentos)} documentos exitosamente.\")  # Log con el número total de documentos cargados.\n",
    "\n",
    "# Precompute embeddings de nombres de archivos\n",
    "model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"  # Modelo para embeddings\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "doc_filenames = [doc['filename'] for doc in documentos]\n",
    "doc_filenames_embeddings = model.encode(doc_filenames, show_progress_bar=True)\n",
    "logging.info(\"Embeddings de nombres de archivos precomputados.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 4: Configurar la Clave API de Gemini**\n",
    "Configura la conexión al modelo Gemini utilizando la clave API proporcionada en un archivo `.env`.\n",
    "\n",
    "- **Función `configure_gemini`:**\n",
    "  - Recupera la clave API desde las variables de entorno.\n",
    "  - Inicializa la instancia de Gemini usando la clave recuperada.\n",
    "  - Lanza un error si la clave no está configurada correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_llm = None  # Variable global para almacenar la instancia del modelo Gemini.\n",
    "\n",
    "def configure_gemini():\n",
    "    \"\"\"\n",
    "    Configura la instancia de Gemini utilizando la clave API almacenada en variables de entorno.\n",
    "    \"\"\"\n",
    "    global gemini_llm  # Permite modificar la variable global gemini_llm.\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")  # Recupera la clave API desde el archivo .env.\n",
    "    if not api_key:  # Verifica si la clave API está configurada.\n",
    "        logging.error(\"La clave API de Gemini no está configurada.\")  # Log de error si no se encuentra la clave.\n",
    "        raise EnvironmentError(\"Configura GEMINI_API_KEY en tu archivo .env.\")  # Lanza una excepción si la clave no existe.\n",
    "    gemini_llm = Gemini(api_key=api_key)  # Inicializa una instancia de Gemini con la clave API.\n",
    "    logging.info(\"Gemini configurado correctamente.\")  # Log para confirmar que la configuración fue exitosa.\n",
    "\n",
    "# Configurar Gemini\n",
    "configure_gemini()  # Llama a la función para inicializar y configurar el modelo Gemini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 5: Configurar el Modelo de Embeddings**\n",
    "Configura el modelo preentrenado de embeddings que se usará para calcular similitudes de texto.\n",
    "\n",
    "- **Modelo Utilizado:** `\"paraphrase-multilingual-MiniLM-L12-v2\"`.\n",
    "- **Carga del Modelo:** Se inicializa con `SentenceTransformer` para generar embeddings.\n",
    "- **Función `doc_enfermedad`:**\n",
    "  - Compara embeddings de la pregunta con los nombres de archivos cargados.\n",
    "  - Devuelve el índice del archivo más relevante basado en la similitud de coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_enfermedad(pregunta):\n",
    "    \"\"\"\n",
    "    Identifica el archivo de donde leerá la información sobre la enfermedad en la pregunta,\n",
    "    buscando el mayor embedding entre ella y los nombres de los archivos en documentos.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Generar embedding para la pregunta\n",
    "        preg_embedding = model.encode(pregunta)\n",
    "        \n",
    "        # Calcular similitudes de coseno usando embeddings precomputados\n",
    "        similarities = [util.cos_sim(preg_embedding, doc_emb).item() for doc_emb in doc_filenames_embeddings]\n",
    "        \n",
    "        # Identificar el índice con la mayor similitud\n",
    "        max_index = similarities.index(max(similarities))\n",
    "        \n",
    "        logging.debug(f\"Similitudes: {similarities}\")\n",
    "        logging.debug(f\"Índice máximo: {max_index} con similitud {similarities[max_index]}\")\n",
    "        \n",
    "        return max_index\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en doc_enfermedad: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 6: Crear Clases para Documentos e Índices**\n",
    "Define clases para manejar documentos y realizar búsquedas eficientes en índices de texto.\n",
    "\n",
    "- **Clase `Document`:**\n",
    "  - Representa un documento con contenido (`page_content`) y metadatos.\n",
    "  - Implementa un método `__str__` para mostrar información relevante del documento.\n",
    "- **Clase `HNSWIndex`:**\n",
    "  - Implementa un índice de vecinos más cercanos usando HNSWlib.\n",
    "  - Admite búsquedas rápidas basadas en similitud de embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para representar documentos cargados con contenido y metadatos.\n",
    "class Document:\n",
    "    def __init__(self, text, metadata=None):\n",
    "        self.page_content = text  # Contenido principal del documento.\n",
    "        self.metadata = metadata or {}  # Metadatos adicionales, si existen, o un diccionario vacío.\n",
    "    \n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Representación en formato legible de los metadatos del documento.\n",
    "        Accede a los metadatos de forma segura utilizando `.get()` para evitar errores si faltan claves.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            f\"Título: {self.metadata.get('Title', 'N/A')}\\n\"\n",
    "            f\"Resumen: {self.metadata.get('Summary', 'N/A')}\\n\"\n",
    "            f\"Tipo de Estudio: {self.metadata.get('StudyType', 'N/A')}\\n\"\n",
    "            f\"Paises donde se desarrolla el estudio: {self.metadata.get('Countries', 'N/A')}\\n\"\n",
    "            f\"Fase en que se encuentra el estudio: {self.metadata.get('Phases', 'N/A')}\\n\"\n",
    "            f\"Identificación en ClinicaTrial: {self.metadata.get('IDestudio', 'N/A')}.\\n\\n\"\n",
    "        )\n",
    "\n",
    "# Clase para manejar índices HNSWlib y realizar búsquedas eficientes de similitud.\n",
    "class HNSWIndex:\n",
    "    def __init__(self, embeddings, metadata=None, space='cosine', ef_construction=200, M=16):\n",
    "        self.dimension = embeddings.shape[1]  # Dimensión de los embeddings.\n",
    "        self.index = hnswlib.Index(space=space, dim=self.dimension)  # Inicializa el índice HNSW con métrica de coseno.\n",
    "        self.index.init_index(max_elements=embeddings.shape[0], ef_construction=ef_construction, M=M)  \n",
    "        # Configura el índice con los parámetros de construcción.\n",
    "        self.index.add_items(embeddings, np.arange(embeddings.shape[0]))  # Añade los embeddings al índice.\n",
    "        self.index.set_ef(50)  # Configura el parámetro `ef` para consultas (balance entre velocidad y precisión).\n",
    "        self.metadata = metadata or []  # Metadatos asociados a los embeddings.\n",
    "    \n",
    "    def similarity_search(self, query_vector, k=5):\n",
    "        \"\"\"\n",
    "        Realiza una búsqueda de los `k` elementos más similares al vector de consulta.\n",
    "        Retorna una lista de tuplas con metadatos y distancias.\n",
    "        \"\"\"\n",
    "        labels, distances = self.index.knn_query(query_vector, k=k)  # Busca los `k` vecinos más cercanos.\n",
    "        return [(self.metadata[i], distances[0][j]) for j, i in enumerate(labels[0])]  # Asocia los resultados con metadatos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 7: Procesar Documentos y Crear Índices**\n",
    "Convierte documentos en bloques manejables y genera índices para búsquedas rápidas.\n",
    "\n",
    "- **Función `desdobla_doc`:**\n",
    "  - Extrae información relevante de cada documento.\n",
    "  - Crea embeddings para los textos procesados.\n",
    "  - Genera un índice HNSWlib con estos embeddings.\n",
    "- **Proceso por Archivo:** Itera sobre los documentos cargados para crear bloques e índices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procesar documentos y crear índice\n",
    "def desdobla_doc(data2):\n",
    "    \"\"\"\n",
    "    Convierte el contenido de los datos proporcionados en instancias de Document y crea un índice HNSWlib.\n",
    "    \"\"\"\n",
    "    documents = []  # Lista para almacenar los objetos Document.\n",
    "    summaries = []  # Lista para almacenar los resúmenes generados.\n",
    "\n",
    "    for entry in data2['content']:  # Itera sobre cada entrada en los datos.\n",
    "        # Extrae información relevante del contenido, manejando valores faltantes con cadenas vacías.\n",
    "        nctId = entry.get(\"IDestudio\", \"\")\n",
    "        briefTitle = entry.get(\"Title\", \"\")\n",
    "        summary = entry.get(\"Summary\", \"\")\n",
    "        studyType = entry.get(\"StudyType\", \"\")\n",
    "        country = entry.get(\"Countries\", \"\")\n",
    "        overallStatus = entry.get(\"OverallStatus\", \"\")\n",
    "        conditions = entry.get(\"Conditions\", \"\")\n",
    "        phases = entry.get(\"Phases\", \"\")  # Corregir \"Phasess\" a \"Phases\"\n",
    "\n",
    "        # Genera un resumen del estudio utilizando los datos extraídos.\n",
    "        Summary = (\n",
    "            f\"The study titled '{briefTitle}', of type '{studyType}', \"\n",
    "            f\"is being conducted to investigate the condition(s) {conditions}. \"\n",
    "            f\"This study is briefly summarized as follows: {summary}. \"\n",
    "            f\"Currently, the study status is {overallStatus}, and it is taking place in {country}. \"\n",
    "            f\"The study is classified under {phases} phase. \"\n",
    "            f\"For more information, search {nctId} on ClinicalTrials.\"\n",
    "        )\n",
    "\n",
    "        # Asocia el resumen con sus metadatos.\n",
    "        metadata = {\"Summary\": Summary}\n",
    "\n",
    "        # Crea una instancia de Document y la agrega a la lista.\n",
    "        documents.append(Document(Summary, metadata))\n",
    "        summaries.append(Summary)  # Almacena el resumen para posibles referencias.\n",
    "\n",
    "    # Genera embeddings para todos los documentos utilizando el modelo cargado.\n",
    "    embeddings = model.encode([doc.page_content for doc in documents], show_progress_bar=True)\n",
    "    embeddings = np.array(embeddings).astype(np.float32)  # Convierte los embeddings a float32 para compatibilidad.\n",
    "\n",
    "    # Crea un índice HNSWlib utilizando los embeddings generados.\n",
    "    vector_store = HNSWIndex(embeddings, metadata=[doc.metadata for doc in documents])\n",
    "\n",
    "    return documents, vector_store  # Retorna los documentos procesados y el índice creado.\n",
    "\n",
    "# Genera trozos e índices para las enfermedades y los almacena en listas.\n",
    "trozos_archivos = []  # Lista para almacenar los bloques procesados de documentos.\n",
    "index_archivos = []  # Lista para almacenar los índices HNSWlib.\n",
    "\n",
    "for i in range(len(documentos)):  # Itera sobre cada conjunto de documentos.\n",
    "    trozos, index = desdobla_doc(documentos[i])  # Procesa y crea el índice para cada conjunto.\n",
    "    trozos_archivos.append(trozos)  # Almacena los bloques procesados.\n",
    "    index_archivos.append(index)  # Almacena el índice creado.\n",
    "\n",
    "logging.info(\"Índices HNSWlib creados para todos los documentos.\")  # Log para confirmar la creación exitosa de los índices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 8: Traducir Preguntas y Respuestas**\n",
    "Se encarga de traducir texto entre idiomas usando el modelo Gemini.\n",
    "\n",
    "- **Función `traducir`:**\n",
    "  - Envía el texto a traducir como un mensaje al modelo Gemini.\n",
    "  - Retorna la traducción y registra el tiempo tomado para la operación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traducir(texto, idioma_destino):\n",
    "    \"\"\"\n",
    "    Traduce texto al idioma especificado utilizando el modelo Gemini.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Inicia el contador para medir el tiempo de traducción.\n",
    "\n",
    "    # Prepara los mensajes para enviar al modelo de lenguaje.\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Actúa como un traductor.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"Por favor, traduce este texto al {idioma_destino}: {texto}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time  # Calcula el tiempo transcurrido.\n",
    "        logging.info(f\"Traducción completada: {respuesta.message.content.strip()} en {elapsed_time:.2f} segundos.\")\n",
    "        return respuesta.message.content.strip()  # Devuelve la traducción limpia.\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al traducir: {e}\")  # Registra el error en los logs.\n",
    "        return texto  # Devuelve el texto original como fallback.\n",
    "\n",
    "def generate_embedding(texto):\n",
    "    \"\"\"\n",
    "    Genera un embedding para una pregunta o texto.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        texto_normalizado = normalizar_texto(texto)\n",
    "        embedding = model.encode([texto_normalizado])\n",
    "        logging.info(f\"Embedding generado para el texto: {texto_normalizado}\")\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar el embedding: {e}\")\n",
    "        return np.zeros((1, 768))  # Ajusta el tamaño según tu modelo\n",
    "\n",
    "def obtener_contexto(pregunta, index, trozos, top_k=50):\n",
    "    \"\"\"\n",
    "    Recupera los trozos de texto más relevantes para responder la pregunta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pregunta_normalizada = normalizar_texto(pregunta)\n",
    "        pregunta_en_ingles = traducir(pregunta_normalizada, \"inglés\")\n",
    "        if not pregunta_en_ingles or len(pregunta_en_ingles) < 5:  # Validación básica.\n",
    "            logging.warning(\"Traducción fallida o insuficiente.\")\n",
    "            return \"No se pudo procesar la pregunta debido a un error en la traducción.\"\n",
    "        pregunta_emb = generate_embedding(pregunta_en_ingles)\n",
    "\n",
    "        # Buscar en el índice\n",
    "        results = index.similarity_search(pregunta_emb, k=top_k)\n",
    "        if not results:\n",
    "            logging.warning(\"No se encontró contexto relevante.\")\n",
    "            return \"No se encontró información relevante para esta pregunta.\"\n",
    "\n",
    "        texto = \"\\n\".join([entry[0][\"Summary\"] for entry in results])\n",
    "        return texto\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al obtener el contexto: {e}\")\n",
    "        return \"Hubo un problema al recuperar la información. Por favor, intenta con otra pregunta.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 9: Generar Respuestas**\n",
    "Genera respuestas específicas basadas en la pregunta del usuario y el contexto recuperado.\n",
    "\n",
    "- **Función `categorizar_pregunta`:**\n",
    "  - Clasifica la pregunta en categorías como \"tratamiento\", \"ensayo\", etc.\n",
    "- **Función `generar_prompt`:**\n",
    "  - Crea prompts personalizados según la categoría.\n",
    "- **Función `es_saludo`:**\n",
    "  - Detecta si la entrada del usuario es un saludo.\n",
    "- **Función `responder_saludo`:**\n",
    "  - Devuelve una respuesta aleatoria de saludo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejorar la Generación de Respuestas con Prompts Específicos\n",
    "def categorizar_pregunta(pregunta):\n",
    "    \"\"\"\n",
    "    Clasifica la pregunta en categorías específicas como 'tratamiento', 'ensayo clínico', etc.\n",
    "    \"\"\"\n",
    "    categorias = {\n",
    "        \"tratamiento\": [\"tratamiento\", \"medicación\", \"cura\", \"terapia\", \"fármaco\", \"intervención\", \"intervenciones\"],\n",
    "        \"ensayo\": [\"ensayo\", \"ensayos\", \"estudio\", \"estudios\", \"prueba\", \"investigación\", \"trial\"],\n",
    "        \"criterios\": [\"criterios\", \"inclusión\", \"exclusión\", \"participantes\"],\n",
    "        \"resultado\": [\"resultado\", \"efectividad\", \"resultados\", \"éxito\", \"fracaso\"],\n",
    "        \"ubicación\": [\"ciudad\", \"ciudades\", \"país\", \"países\", \"ubicación\", \"localización\"],\n",
    "        \"prevención\": [\"prevención\", \"previene\", \"evitar\", \"reducción de riesgo\"],\n",
    "        \"duración\": [\"duración\", \"años\", \"meses\", \"plazo\"],\n",
    "    }\n",
    "    for categoria, palabras in categorias.items():\n",
    "        if any(palabra in pregunta.lower() for palabra in palabras):\n",
    "            return categoria\n",
    "    return \"general\"\n",
    "\n",
    "def generar_prompt(categoria, pregunta):\n",
    "    \"\"\"\n",
    "    Genera un prompt específico basado en la categoría de la pregunta.\n",
    "    \"\"\"\n",
    "    prompts = {\n",
    "        \"tratamiento\": f\"Proporciona información detallada sobre tratamientos relacionados con: {pregunta}.\",\n",
    "        \"ensayo\": f\"Describe los ensayos clínicos actuales relacionados con: {pregunta}.\",\n",
    "        \"criterios\": f\"Explica los criterios de inclusión y exclusión para los ensayos clínicos sobre: {pregunta}.\",\n",
    "        \"resultado\": f\"Explica los resultados más recientes de ensayos clínicos sobre: {pregunta}.\",\n",
    "        \"ubicación\": f\"Indica las ubicaciones geográficas donde se están llevando a cabo ensayos clínicos para: {pregunta}.\",\n",
    "        \"prevención\": f\"Ofrece estrategias de prevención para: {pregunta}.\",\n",
    "        \"duración\": f\"Describe la duración típica de los ensayos clínicos sobre: {pregunta}.\",\n",
    "    }\n",
    "    return prompts.get(categoria, \"Por favor, responde la pregunta sobre ensayos clínicos.\")\n",
    "\n",
    "# Se define una función para detectar si una pregunta es un saludo.\n",
    "def es_saludo(pregunta):\n",
    "    saludos = [\"hola\", \"buen día\", \"buenas\", \"cómo estás\", \"cómo te llamas\", \"qué tal\", \"estás bien\", \"buenas tardes\", \"buenas noches\"]\n",
    "    return any(saludo in pregunta.lower() for saludo in saludos)\n",
    "\n",
    "def responder_saludo():\n",
    "    saludos_respuestas = [\n",
    "        \"¡Hola! Estoy para ayudarte con información sobre ensayos clínicos. ¿En qué puedo asistirte hoy?\",\n",
    "        \"¡Buenas! Tenés alguna pregunta sobre ensayos clínicos en enfermedades neuromusculares?\",\n",
    "        \"¡Hola! ¿Cómo puedo ayudarte con tus consultas sobre ensayos clínicos?\"\n",
    "    ]\n",
    "    return random.choice(saludos_respuestas)\n",
    "\n",
    "def generar_respuesta(pregunta, contexto, prompt_especifico):\n",
    "    \"\"\"\n",
    "    Genera una respuesta usando el contexto proporcionado y un prompt específico.\n",
    "    \"\"\"\n",
    "    start_time = time.time()  # Medir el tiempo de generación de respuesta\n",
    "\n",
    "    mensajes = [\n",
    "        ChatMessage(role=\"system\", content=\"Eres un experto médico.\"),\n",
    "        ChatMessage(role=\"user\", content=f\"{prompt_especifico}\\nContexto: {contexto}\\nPregunta: {pregunta}\")\n",
    "    ]\n",
    "    try:\n",
    "        respuesta = gemini_llm.chat(mensajes)\n",
    "        elapsed_time = time.time() - start_time  # Tiempo de generación de respuesta\n",
    "        logging.info(f\"Respuesta generada en inglés: {respuesta.message.content.strip()} en {elapsed_time:.2f} segundos.\")\n",
    "\n",
    "        # Traducir la respuesta al español\n",
    "        respuesta_en_espanol = traducir(respuesta.message.content, \"español\")\n",
    "        logging.info(f\"Respuesta traducida al español: {respuesta_en_espanol}\")\n",
    "        return respuesta_en_espanol\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar la respuesta: {e}\")\n",
    "        return \"Lo siento, ocurrió un error al generar la respuesta.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 10: Función Principal para Responder Preguntas**\n",
    "Integra todos los pasos anteriores para generar respuestas completas.\n",
    "\n",
    "- **Función `obtener_respuesta_cacheada`:**\n",
    "  - Verifica si una pregunta ya tiene respuesta en caché.\n",
    "- **Función `guardar_respuesta_cacheada`:**\n",
    "  - Almacena la respuesta generada en caché.\n",
    "- **Función `responder_pregunta`:**\n",
    "  - Traduce la pregunta.\n",
    "  - Recupera el contexto relevante.\n",
    "  - Genera la respuesta y la almacena en caché."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_hash(pregunta):\n",
    "    \"\"\"\n",
    "    Genera un hash SHA-256 único para la pregunta proporcionada.\n",
    "    Este hash se utiliza como identificador para el almacenamiento en caché.\n",
    "    \"\"\"\n",
    "    return hashlib.sha256(pregunta.encode('utf-8')).hexdigest()  # Convierte la pregunta a un hash hexadecimal.\n",
    "\n",
    "def obtener_respuesta_cacheada(pregunta):\n",
    "    \"\"\"\n",
    "    Recupera una respuesta previamente generada desde el caché, si existe.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)  # Genera el hash único para la pregunta.\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"  # Define la ruta del archivo de caché.\n",
    "    if os.path.exists(archivo_cache):  # Verifica si el archivo de caché existe.\n",
    "        try:\n",
    "            with open(archivo_cache, \"r\", encoding='utf-8') as f:  # Abre el archivo en modo lectura.\n",
    "                datos = json.load(f)  # Carga los datos en formato JSON.\n",
    "                return datos.get(\"respuesta\", None)  # Retorna la respuesta si está disponible.\n",
    "        except Exception as e:  # Manejo de errores al leer el archivo.\n",
    "            logging.error(f\"Error al leer el caché para la pregunta '{pregunta}': {e}\")\n",
    "            return None  # Retorna None en caso de error.\n",
    "    return None  # Retorna None si el archivo de caché no existe.\n",
    "\n",
    "def guardar_respuesta_cacheada(pregunta, respuesta):\n",
    "    \"\"\"\n",
    "    Almacena una respuesta en el caché para consultas futuras.\n",
    "    \"\"\"\n",
    "    hash_pregunta = generar_hash(pregunta)  # Genera el hash único para la pregunta.\n",
    "    archivo_cache = f\"cache/{hash_pregunta}.json\"  # Define la ruta del archivo de caché.\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(archivo_cache), exist_ok=True)  # Asegura que el directorio del caché exista.\n",
    "        with open(archivo_cache, \"w\", encoding='utf-8') as f:  # Abre el archivo en modo escritura.\n",
    "            json.dump({\"pregunta\": pregunta, \"respuesta\": respuesta}, f, ensure_ascii=False, indent=4)  \n",
    "            # Guarda la pregunta y la respuesta en formato JSON.\n",
    "        logging.info(f\"Respuesta cacheada para la pregunta: '{pregunta}'\")  # Log de éxito al guardar en caché.\n",
    "    except Exception as e:  # Manejo de errores al guardar en caché.\n",
    "        logging.error(f\"Error al guardar la respuesta en caché para la pregunta '{pregunta}': {e}\")\n",
    "\n",
    "def responder_pregunta(pregunta, index, trozos):\n",
    "    \"\"\"\n",
    "    Responde una pregunta del usuario integrando:\n",
    "    - Búsqueda en caché.\n",
    "    - Traducción y recuperación de contexto.\n",
    "    - Generación de respuestas personalizadas.\n",
    "\n",
    "    Incluye manejo de caché para optimizar el tiempo de respuesta.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Verificar si la respuesta ya está en el caché.\n",
    "        respuesta_cacheada = obtener_respuesta_cacheada(pregunta)\n",
    "        if respuesta_cacheada:\n",
    "            logging.info(f\"Respuesta obtenida del caché para la pregunta: '{pregunta}'\")\n",
    "            return respuesta_cacheada\n",
    "\n",
    "        # Categorizar la pregunta para generar prompts específicos.\n",
    "        categoria = categorizar_pregunta(pregunta)\n",
    "        logging.info(f\"Categoría de la pregunta: {categoria}\")\n",
    "\n",
    "        # Generar un prompt basado en la categoría.\n",
    "        prompt_especifico = generar_prompt(categoria, pregunta)\n",
    "        logging.info(f\"Prompt específico generado: {prompt_especifico}\")\n",
    "\n",
    "        # Obtener el contexto relevante para la pregunta.\n",
    "        contexto = obtener_contexto(pregunta, index, trozos)\n",
    "        logging.debug(f\"Contexto recuperado: {contexto[:200] if contexto else 'Sin contexto'}\")\n",
    "        if not contexto:\n",
    "            logging.warning(\"No se encontró un contexto relevante para la pregunta.\")\n",
    "            respuesta = \"No pude encontrar información relevante para responder tu pregunta.\"\n",
    "            guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "            return respuesta\n",
    "\n",
    "        # Generar la respuesta final utilizando el contexto y el prompt.\n",
    "        respuesta = generar_respuesta(pregunta, contexto, prompt_especifico)\n",
    "        logging.debug(f\"Respuesta generada: {respuesta[:200] if respuesta else 'Sin respuesta'}\")\n",
    "\n",
    "        # Guardar la respuesta generada en el caché.\n",
    "        guardar_respuesta_cacheada(pregunta, respuesta)\n",
    "\n",
    "        return respuesta\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en el proceso de responder pregunta: {e}\")\n",
    "        return \"Ocurrió un error al procesar tu pregunta.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PASO 11: Interfaz CLI**\n",
    "Proporciona una interfaz interactiva en línea de comandos para que los usuarios interactúen con el chatbot.\n",
    "\n",
    "- **Inicialización del Caché:** Crea el directorio de caché si no existe.\n",
    "- **Bucle Principal:**\n",
    "  - Permite al usuario ingresar preguntas.\n",
    "  - Responde saludos inmediatamente si se detectan.\n",
    "  - Recupera el índice y bloques relacionados con la enfermedad mencionada.\n",
    "  - Genera y muestra la respuesta al usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Asegurar que el directorio de caché exista\n",
    "    os.makedirs(\"cache\", exist_ok=True)  # Crea el directorio \"cache\" si no existe para almacenar respuestas cacheadas.\n",
    "    \n",
    "    # Mensaje de bienvenida e instrucciones para el usuario\n",
    "    print(\"Bienvenido al chatbot de ensayos clínicos\")\n",
    "    print(\"Conversemos sobre Ensayos Clínicos\\n de las siguientes enfermedades neuromusculares: Distrofia Muscular de Duchenne o de Becker, Enfermedad de Pompe, Distrofia Miotónica o Enfermedad de almacenamiento de glucógeno\")\n",
    "    print(\"Escribe tu pregunta, dejando claramente expresada la enfermedad sobre la que quieres conocer información de ensayos médicos. Escribe 'salir' para terminar.\")\n",
    "    \n",
    "    while True:\n",
    "        # Solicitar la pregunta al usuario\n",
    "        pregunta = input(\"Tu pregunta: \").strip()  # Recoge la entrada del usuario y elimina espacios innecesarios.\n",
    "\n",
    "        # Verificar si el usuario desea salir\n",
    "        if pregunta.lower() in ['salir', 'chau', 'exit', 'quit']:  # Frases que permiten terminar la sesión.\n",
    "            print(\"¡Chau!\")  # Mensaje de despedida.\n",
    "            logging.info(\"El usuario ha finalizado la sesión.\")  # Registra la finalización de la sesión en los logs.\n",
    "            break  # Sale del bucle principal.\n",
    "\n",
    "        # Verificar si la entrada es un saludo\n",
    "        if es_saludo(pregunta):\n",
    "            respuesta_saludo = responder_saludo()\n",
    "            print(respuesta_saludo)\n",
    "            logging.info(\"Se detectó un saludo del usuario.\")\n",
    "            continue  # Salta al siguiente ciclo del bucle.\n",
    "\n",
    "        # Normalizar la pregunta\n",
    "        pregunta_normalizada = normalizar_texto(pregunta)\n",
    "        logging.info(f\"Pregunta procesada: {pregunta_normalizada}\")\n",
    "        logging.debug(f\"Pregunta original: {pregunta}\")\n",
    "\n",
    "        # Identificar la enfermedad relacionada con la pregunta\n",
    "        idn = doc_enfermedad(pregunta_normalizada)\n",
    "        if idn is None:\n",
    "            logging.warning(f\"No se pudo identificar la enfermedad para la pregunta: '{pregunta}'\")\n",
    "            respuesta = \"Lo siento, no pude identificar la enfermedad relacionada con tu pregunta.\"\n",
    "            print(f\"Respuesta: {respuesta}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            index = index_archivos[idn]\n",
    "            trozos = trozos_archivos[idn]\n",
    "\n",
    "            # Generar la respuesta para la pregunta del usuario\n",
    "            respuesta = responder_pregunta(pregunta_normalizada, index, trozos)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al generar la respuesta para la pregunta '{pregunta}': {e}\")\n",
    "            respuesta = \"Lo siento, ocurrió un error al generar la respuesta.\"\n",
    "\n",
    "        print(f\"Respuesta: {respuesta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anexo: Generar respuestas desde una lista de preguntas y guardar en un archivo TXT**\n",
    "Este paso procesa una lista de preguntas predefinidas, genera respuestas y las guarda en un archivo de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import logging\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.DEBUG)  # Asegúrate de que el nivel de logging esté configurado para capturar DEBUG\n",
    "\n",
    "# Función para normalizar texto\n",
    "def normalizar_texto(texto):\n",
    "    \"\"\"\n",
    "    Normaliza el texto eliminando tildes, caracteres especiales, y convirtiendo a minúsculas.\n",
    "    \"\"\"\n",
    "    texto = texto.lower()  # Convertir a minúsculas.\n",
    "    # Eliminar tildes y normalizar caracteres unicode.\n",
    "    texto = unicodedata.normalize('NFD', texto).encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Eliminar caracteres que no sean alfanuméricos o espacios.\n",
    "    texto = ''.join(char for char in texto if char.isalnum() or char.isspace())\n",
    "    return texto.strip()  # Eliminar espacios extra.\n",
    "\n",
    "# Lista de sustituciones para problemas comunes\n",
    "sustituciones = {\n",
    "    \"ensayos\": \"estudios\",\n",
    "    \"ó\": \"o\",\n",
    "    \"á\": \"a\",\n",
    "    \"í\": \"i\",\n",
    "    \"ú\": \"u\",\n",
    "    \"é\": \"e\",\n",
    "    \"Distrofia Muscular de Duchenne\": \"Duchenne\",\n",
    "    \"Distrofia Muscular de Becker\": \"Becker\",\n",
    "    \"Enfermedad de Pompe\": \"Pompe\",\n",
    "    \"Distrofia Miotónica\": \"Miotónica\",\n",
    "    \"Enfermedad de almacenamiento de glucógeno\": \"Glucógeno\"\n",
    "}\n",
    "\n",
    "def corregir_pregunta(pregunta):\n",
    "    \"\"\"\n",
    "    Realiza sustituciones en la pregunta para evitar problemas en la búsqueda.\n",
    "    \"\"\"\n",
    "    for palabra, reemplazo in sustituciones.items():\n",
    "        pregunta = pregunta.replace(palabra, reemplazo)\n",
    "    return pregunta\n",
    "\n",
    "# Lista de preguntas hardcodeadas\n",
    "preguntas = [\n",
    "    \"¿Cuántos ensayos clínicos están activos actualmente para la Distrofia Muscular de Duchenne?\",\n",
    "    \"¿Qué tratamientos están siendo investigados para la Enfermedad de Pompe?\",\n",
    "    \"¿Cuáles son las fases de los ensayos clínicos disponibles para la Distrofia Miotónica?\",\n",
    "    \"¿Existen estudios completados sobre la Enfermedad de almacenamiento de glucógeno? Si es así, ¿qué resultados destacaron?\",\n",
    "    \"¿Qué países participan en los ensayos clínicos relacionados con la Distrofia Muscular de Becker?\",\n",
    "    \"¿Se está investigando alguna terapia génica para la Distrofia Muscular de Duchenne?\",\n",
    "    \"¿Qué medicamentos están siendo evaluados en ensayos clínicos para mejorar la fuerza muscular en la Enfermedad de Pompe?\",\n",
    "    \"¿Existen ensayos clínicos que investiguen Litifilimab (BIIB059) en la Distrofia Miotónica?\",\n",
    "    \"¿Qué tipo de intervenciones se están probando en los estudios para la Enfermedad de almacenamiento de glucógeno?\",\n",
    "    \"¿Hay estudios que comparen la eficacia de diferentes tratamientos en la Distrofia Muscular de Becker?\",\n",
    "    \"¿Cuáles son los criterios de inclusión para los ensayos clínicos sobre la Distrofia Muscular de Duchenne?\",\n",
    "    \"¿Hay ensayos clínicos diseñados específicamente para niños con Enfermedad de Pompe?\",\n",
    "    \"¿Qué estudios incluyen pacientes adultos con Distrofia Miotónica?\",\n",
    "    \"¿Hay ensayos clínicos que evalúen tratamientos preventivos para la Enfermedad de almacenamiento de glucógeno?\",\n",
    "    \"¿Existen ensayos multicéntricos en América Latina para estas enfermedades?\",\n",
    "    \"¿Cuántos ensayos clínicos sobre la Enfermedad de Pompe se están llevando a cabo en Argentina?\",\n",
    "    \"¿Qué estudios tienen una duración estimada de más de 5 años en la Distrofia Muscular de Duchenne?\",\n",
    "    \"¿Qué ciudades participan en los ensayos clínicos para la Distrofia Muscular de Becker en Europa?\",\n",
    "    \"¿Hay ensayos clínicos de la Enfermedad de almacenamiento de glucógeno en Canadá?\",\n",
    "    \"¿Cuáles son las instituciones que lideran ensayos clínicos en Estados Unidos para la Distrofia Miotónica?\",\n",
    "]\n",
    "\n",
    "# Nombre del archivo de salida\n",
    "archivo_salida = \"respuestas.txt\"\n",
    "\n",
    "# Abrir el archivo en modo de escritura\n",
    "with open(archivo_salida, \"w\", encoding=\"utf-8\") as file:\n",
    "    for pregunta in preguntas:\n",
    "        # Paso 1: Corregir y normalizar la pregunta\n",
    "        pregunta_corregida = corregir_pregunta(pregunta)\n",
    "        pregunta_normalizada = normalizar_texto(pregunta_corregida)\n",
    "        \n",
    "        # Log para depuración\n",
    "        logging.info(f\"Pregunta procesada: {pregunta_normalizada}\")\n",
    "        logging.debug(f\"Pregunta original: {pregunta}\")\n",
    "        \n",
    "        # Paso 2: Determinar si es un saludo\n",
    "        if es_saludo(pregunta_normalizada):\n",
    "            respuesta = responder_saludo()\n",
    "        else:\n",
    "            # Identificar el índice del documento\n",
    "            try:\n",
    "                idn = doc_enfermedad(pregunta_normalizada)\n",
    "                if idn is None:\n",
    "                    raise ValueError(\"No se pudo identificar la enfermedad relacionada con la pregunta.\")\n",
    "                index = index_archivos[idn]\n",
    "                trozos = trozos_archivos[idn]\n",
    "\n",
    "                # Generar respuesta\n",
    "                respuesta = responder_pregunta(pregunta_normalizada, index, trozos)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error al procesar la pregunta '{pregunta_normalizada}': {e}\")\n",
    "                respuesta = \"Lo siento, no pude procesar tu pregunta.\"\n",
    "\n",
    "        # Escribir pregunta y respuesta en el archivo\n",
    "        file.write(f\"Pregunta: {pregunta}\\n\")\n",
    "        file.write(f\"Respuesta: {respuesta}\\n\\n\")\n",
    "\n",
    "# Confirmar que se han guardado las respuestas\n",
    "logging.info(f\"Respuestas guardadas en el archivo '{archivo_salida}'.\")\n",
    "print(f\"Respuestas guardadas en el archivo '{archivo_salida}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
